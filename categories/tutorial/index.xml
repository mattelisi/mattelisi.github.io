<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tutorial | Matteo Lisi</title>
    <link>http://mlisi.xyz/categories/tutorial/</link>
      <atom:link href="http://mlisi.xyz/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    <description>tutorial</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Matteo Lisi</copyright><lastBuildDate>Fri, 25 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://mlisi.xyz/img/head.png</url>
      <title>tutorial</title>
      <link>http://mlisi.xyz/categories/tutorial/</link>
    </image>
    
    <item>
      <title>Bayesian model selection at the group level</title>
      <link>http://mlisi.xyz/post/bms/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      <guid>http://mlisi.xyz/post/bms/</guid>
      <description>




&lt;p&gt;In experimental psychology and neuroscience the classical approach when comparing different models that make quantitative predictions about the behavior of participants is to aggregate the predictive ability of the model (e.g. as quantified by Akaike Information criterion) across participants, and then see which one provide on average the best performance. Although correct, this approach neglect the possibility that different participants might use different strategies that are best described by alternative, competing models. To account for this, Stephan et al. &lt;span class=&#34;citation&#34;&gt;(Stephan et al. 2009)&lt;/span&gt; proposed a more conservative approach where models are treated as random effects that could differ between subjects and have a fixed (unknown) distribution in the population. The relevant statistical quantity is the frequency with which any model prevails in the population. Note that this is different from the definition of random-effects in classical statistic where random effects models have multiple sources of variation, e.g. within- and between- subject variance. An useful and popular way to summarize the results of this analysis is by reporting the model’s &lt;em&gt;exceedance probabilities&lt;/em&gt;, which measures how likely it is that any given model is more frequent than all other models in the set. The following exposition is largerly based on Stephan et al’s paper &lt;span class=&#34;citation&#34;&gt;(Stephan et al. 2009)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;model-evidence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model evidence&lt;/h1&gt;
&lt;p&gt;Let’s say we have an experiment with &lt;span class=&#34;math inline&#34;&gt;\(\left(1,\dots,N\right)\)&lt;/span&gt; participants. Their performance is quantitatively predicted by a set &lt;span class=&#34;math inline&#34;&gt;\(\left(1,\dots,K\right)\)&lt;/span&gt; competing models. The behaviour of any subject &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; can be fit by the model &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; by finding the value(s) of the parameter(s) &lt;span class=&#34;math inline&#34;&gt;\(\theta_k\)&lt;/span&gt; that maximize the likelihood of the data &lt;span class=&#34;math inline&#34;&gt;\(y_n\)&lt;/span&gt; under the model. In a fully Bayesian setting each unknown parameter would have a prior probability distribution, and the quantity of choice ofr comparing the goodness of fit of the model is the marginal likelihood, that is
&lt;span class=&#34;math display&#34;&gt;\[
  p \left(y_n \mid k \right) = \int p\left(y_n \mid k, \theta_k \right) \, p\left(\theta_k \right) d\theta.
\]&lt;/span&gt;
By integrating over the prior probability of parameters the marginal likelihood provide a measure of the evidence in favour of a specific model while taking into account the complexity of the model. We might also do something simpler and approximate the model evidence using e.g. the Akaike information criterion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;models-as-random-effects&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models as random effects&lt;/h1&gt;
&lt;p&gt;We are interested in finding which model does better at predicting behavior, however we allow for different participants to use different strategies which can be represented by different models. To achieve that we treat the model as random effects and we assume that the frequency or probability of models in the population, &lt;span class=&#34;math inline&#34;&gt;\((r_1, \dots, r_K)\)&lt;/span&gt;, is described by a Dirichlet distribution with parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha } = \alpha_1, \dots, \alpha_k\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
p\left(r \mid  \boldsymbol{\alpha } \right) &amp;amp; = \text{Dir} \left(r, \boldsymbol{ \alpha } \right) \\
&amp;amp; = \frac{1}{\mathbf{B} \left(\boldsymbol{ \alpha }  \right)} \prod_{i=1}^K r_i^{\alpha_i -1} \nonumber
\end{align}.
\]&lt;/span&gt;
Where the normalizing constant &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B} \left(\boldsymbol{ \alpha } \right)\)&lt;/span&gt; is the multivariate Beta function. The probabilities &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; generates ‘switches’ or indicator variables &lt;span class=&#34;math inline&#34;&gt;\(m_n = m_1, \dots, m_N\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(m \in \left \{ 0, 1\right \}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum_1^K m_{nk}=1\)&lt;/span&gt;. These indicator variables prescribe the model for the subjects &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, $ p(m_{nk}=1)=r_k$.
Given the probabilities &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, the indicator variables have thus a multinomial distribution, that is
&lt;span class=&#34;math display&#34;&gt;\[
p\left(m_n \mid  \mathbf{r} \right) =  \prod_{k=1}^K r_k^{m_{nk}}.
\]&lt;/span&gt;
The graphical model that summarizes these dependencies is shown the following graph:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://mlisi.xyz/img/bms.png&#34; alt=&#34;&#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-bayesian-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variational Bayesian approach&lt;/h1&gt;
&lt;p&gt;The goal is to estimate the parameters &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt; that define the posterior distribution of model frequencies given the data, $ p ( r | y)$. To do so we need an estimate of the model evidence &lt;span class=&#34;math inline&#34;&gt;\(p \left(m_{nk}=1 \mid y_n \right)\)&lt;/span&gt;, that is the belief that the model &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; generated data from subject &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;. There are many possible approach that can be used to estimate the model evidence, either exactly or approximately. Importantly, these would need to be normalized so that they sum to one across models, so that is one were using the Akaike Information criterion, this should be transformed into Akaike weights &lt;span class=&#34;citation&#34;&gt;(Burnham and Anderson 2002)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;generative-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generative model&lt;/h2&gt;
&lt;p&gt;Given the graphical model illustrated above, the joint probability of parameters and data can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
p \left( y, r, m \right) &amp;amp; = p \left( y \mid m \right) \, p \left( m \mid r \right) \, p \left( r \mid \boldsymbol{\alpha} \right) \\
&amp;amp; = p \left( r \mid \boldsymbol{\alpha} \right) \left[ \prod_{n=1}^N p \left( y_n \mid m_n \right) \, p\left(m_n \mid r \right) \right] \nonumber \\
&amp;amp; = \frac{1}{\mathbf{B} \left(\boldsymbol{ \alpha }  \right)} \left[ \prod_{k=1}^K r_k^{\alpha_k -1} \right] \left[ \prod_{n=1}^N p \left( y_n \mid m_n\right) \, \prod_{k=1}^K r_k^{m_{nk}} \right] \nonumber \\
&amp;amp; = \frac{1}{\mathbf{B} \left(\boldsymbol{ \alpha }  \right)} \prod_{n=1}^N \left[ \prod_{k=1}^K \left[ p \left( y_n \mid m_{nk} \right) \, r_k \right]^{m_{nk}} \, r_k^{\alpha_k -1} \right]. \nonumber
\end{align}
\]&lt;/span&gt;
And the log probability is
&lt;span class=&#34;math display&#34;&gt;\[
\log p \left( y, r, m \right)  = - \log \mathbf{B} \left(\boldsymbol{ \alpha }  \right)
+ \sum_{n=1}^N \sum_{k=1}^K \left[ \left(\alpha_k -1 \right) \log r_k
+ m_{nk} \left( p \left( \log y_n \mid m_{nk} \right) + \log r_k\right)\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-approximation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variational approximation&lt;/h2&gt;
&lt;p&gt;In order to fit this hierarchical model following the variational approach one needs to define an approximate posterior distribution over model frequencies and assignments, &lt;span class=&#34;math inline&#34;&gt;\(q\left(r,m\right)\)&lt;/span&gt;, which is assumed to be adequately described by a mean-field factorisation, that is &lt;span class=&#34;math inline&#34;&gt;\(q\left(r,m\right) = q\left(r\right) \, q\left(m\right)\)&lt;/span&gt;. The two densities are proportional to the exponentiated &lt;em&gt;variational energies&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(I(m), I(r)\)&lt;/span&gt;, which are essentially the un-normalized approximated log-posterior densities, that is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
q\left(r\right) &amp;amp; \propto e^{I(r)}, \, q\left(m\right)\propto e^{I(m)} \\
I(r) &amp;amp; = \left&amp;lt; \log p \left( y, r, m \right) \right&amp;gt;_{q(r)} \\
I(m) &amp;amp; = \left&amp;lt; \log p \left( y, r, m \right) \right&amp;gt;_{q(m)}
\end{align}
\]&lt;/span&gt;
For the approximate posterior over model assignment &lt;span class=&#34;math inline&#34;&gt;\(q(m)\)&lt;/span&gt; we first compute &lt;span class=&#34;math inline&#34;&gt;\(I(m)\)&lt;/span&gt; and then an appropriate normalization constant. From the expression above of the joint log-probability, and removing all the terms that do not depend on &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; we have that the un-normalized approximate log-posterior (the variational energy) can be expressed as
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
I(m) &amp;amp; = \int p \left( y, r, m \right) \, q(r) \, dr \\
&amp;amp; = \sum_{n=1}^N \sum_{k=1}^K m_{nk} \left[ p \left( \log y_n \mid m_{nk} \right) + \int q(r_k) \log r_k \, d r_k \right] \nonumber \\
&amp;amp; = \sum_{n=1}^N \sum_{k=1}^K m_{nk} \left[ p \left( \log y_n \mid m_{nk} \right) + \psi (\alpha_k) -\psi \left(  \alpha_S \right) \right] \nonumber
\end{align}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha_S = \sum_{k=1}^K \alpha_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is the digamma function. If you wonder (as I did when reading this the first time) where the hell does the digamma function comes from here: well it is here due to a property of the Dirichlet distribution, which says that the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\log r_k\)&lt;/span&gt; can be computed as
&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E} \left[\log r_k \right] = \int p(r_k) \log r_k \, d r_k = \psi (\alpha_k) -\psi \left( \sum_{k=1}^K \alpha_k \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From this, we have that the un-normalized posterior belief that model &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; generated data from subject &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
u_{nk} =  \exp {\left[ p \left( \log y_n \mid m_{nk} \right) + \psi (\alpha_k) -\psi \left(  \alpha_S \right) \right]}
\]&lt;/span&gt;
and the normalized belief is
&lt;span class=&#34;math display&#34;&gt;\[
g_{nk} = \frac{u_{nk}}{\sum_{k=1}^K u_{nk}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We need also to compute the approximate posterior density &lt;span class=&#34;math inline&#34;&gt;\(q(r)\)&lt;/span&gt;, and we begin as above by computing the un-normalized, approximate log-posterior or variational energy
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
I(r) &amp;amp; = \int p \left( y, r, m \right) \, q(m) \, dm \\
&amp;amp; = \sum_{k=1}^K \left[\log r_k \left(\alpha_{0k} -1 \right) +  \sum_{n=1}^N g_{nk} \log r_k \right]
\end{align}
\]&lt;/span&gt;
The logarithm of a Dirichlet density is &lt;span class=&#34;math inline&#34;&gt;\(\log \text{Dir} (r , \boldsymbol{\alpha}) = \sum_{k=1}^K \log r_k \left(\alpha_{0k} -1 \right) + \dots\)&lt;/span&gt;, therefore the parameters of the approximate posterior are
&lt;span class=&#34;math display&#34;&gt;\[
  \boldsymbol{\alpha} = \boldsymbol{\alpha}_0 + \sum_{n=1}^N g_{nk}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iterative-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iterative algorithm}&lt;/h2&gt;
&lt;p&gt;The algorithm &lt;span class=&#34;citation&#34;&gt;(Stephan et al. 2009)&lt;/span&gt; proceeds by estimating iteratively the posterior belief that a given model generated the data from a certain subject, by integrating out the prior probabilities of the models (the &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt; predicted by the Dirichlet distribution that describes the frequency of models in the population) in log-space as described above. Next the parameters of the approximate Dirichlet posterior are updated, which gives new priors to integrate out from the model evidence, and so on until convergence.Convergence is assessed by keeping track of how much the vector $ $ change from one iteration to the next, i.e. is common to consider that the procedure has converged when &lt;span class=&#34;math inline&#34;&gt;\(\left\Vert \boldsymbol{\alpha}_{t-1} \cdot \boldsymbol{\alpha}_t \right\Vert &amp;lt; 10^{-4}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt; is the dot product).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exceedance-probabilities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exceedance probabilities&lt;/h2&gt;
&lt;p&gt;After having found the optimised values of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\alpha}\)&lt;/span&gt;, one popular way to report the results and rank the models is by their exceedance probability, which is defined as the (second order) probability that participants were more likely to choose a certain model to generate behavior rather than any other alternative model, that is
&lt;span class=&#34;math display&#34;&gt;\[
\forall j \in \left\{1, \dots, K, j \ne k \right\}, \,\,\, \varphi_k = p \left(r_k &amp;gt; r_j \mid y, \boldsymbol{\alpha} \right).
\]&lt;/span&gt;
In the case of &lt;span class=&#34;math inline&#34;&gt;\(K&amp;gt;2\)&lt;/span&gt; models, the exceedance probabilities &lt;span class=&#34;math inline&#34;&gt;\(\varphi_k\)&lt;/span&gt; are computed by generating random samples from univariate Gamma densities and then normalizing. Specifically, each multivariate Dirichlet sample is composed of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; independent random samples &lt;span class=&#34;math inline&#34;&gt;\((x_1, \dots, x_K)\)&lt;/span&gt; distributed according to the density &lt;span class=&#34;math inline&#34;&gt;\(\text{Gamma}\left(\alpha_i, 1\right) = \frac{x_i^{\alpha_i-1} e^{-x_i}}{\Gamma(\alpha_i)}\)&lt;/span&gt;, and then set normalize them by taking &lt;span class=&#34;math inline&#34;&gt;\(z_i = \frac{x_i}{ \sum_{i=1}^K x_i}\)&lt;/span&gt;. The exceedance probability &lt;span class=&#34;math inline&#34;&gt;\(\varphi_k\)&lt;/span&gt; for each model &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is then computed as
&lt;span class=&#34;math display&#34;&gt;\[
\varphi_k = \frac{\sum \mathop{\bf{1}}_{z_k&amp;gt;z_j, \forall j \in \left\{1, \dots, K, j \ne k \right\} }}{ \text{n. of samples}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\bf{1}}_{\dots}\)&lt;/span&gt; is the indicator function (&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\bf{1}}_{x&amp;gt;0} = 1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(x&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise), summed over the total number of multivariate samples drawn.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Code!&lt;/h1&gt;
&lt;p&gt;All this is already implementd in Matlab code in &lt;a href=&#34;https://www.fil.ion.ucl.ac.uk/spm/software/spm12/&#34;&gt;SPM 12&lt;/a&gt;. However, if you don’t like Matlab, I have translated it into R, and put it into a &lt;a href=&#34;https://github.com/mattelisi/bmsR&#34;&gt;package on Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Burnham2002&#34;&gt;
&lt;p&gt;Burnham, Kenneth P., and David R. Anderson. 2002. &lt;em&gt;Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach&lt;/em&gt;. 2nd editio. New York, US: Springer New York. &lt;a href=&#34;https://doi.org/10.1007/b97636&#34;&gt;https://doi.org/10.1007/b97636&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Stephan2009&#34;&gt;
&lt;p&gt;Stephan, Klaas Enno, Will D. Penny, Jean Daunizeau, Rosalyn J. Moran, and Karl J. Friston. 2009. “Bayesian model selection for group studies.” &lt;em&gt;NeuroImage&lt;/em&gt; 46 (4). Elsevier Inc.: 1004–17. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2009.03.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2009.03.025&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian multilevel models using R and Stan (part 1)</title>
      <link>http://mlisi.xyz/post/bayesian-multilevel-models-r-stan/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>http://mlisi.xyz/post/bayesian-multilevel-models-r-stan/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://mlisi.xyz/img/turtlepile.jpg&#34; alt=&#34;Photo ©Roxie and Lee Carroll, www.akidsphoto.com.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo ©Roxie and Lee Carroll, www.akidsphoto.com.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In my previous lab I was known for promoting the use of multilevel, or mixed-effects model among my colleagues. (The slides on the &lt;a href=&#34;https://mattelisi.github.io/#notes&#34;&gt;/misc&lt;/a&gt; section of this website are part of this effort.) Multilevel models should be the standard approach in fields like experimental psychology and neuroscience, where the data is naturally grouped according to “observational units”, i.e. individual participants. I agree with Richard McElreath when he writes that &lt;em&gt;“multilevel regression deserves to be the default form of regression”&lt;/em&gt; (see &lt;a href=&#34;http://xcelab.net/rmpubs/rethinking/Statistical_Rethinking_sample.pdf&#34;&gt;here&lt;/a&gt;, section 1.3.2) and that, at least in our fields, studies not using a multilevel approach should justify the choice of not using it.&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;math inline&#34;&gt;\(\textsf{R}\)&lt;/span&gt;, the easiest way to fit multilevel linear and generalized-linear models is provided by the &lt;code&gt;lme4&lt;/code&gt; library &lt;span class=&#34;citation&#34;&gt;(Bates et al. 2014)&lt;/span&gt;. &lt;code&gt;lme4&lt;/code&gt; is a great package, which allows users to test different models very easily and painlessly. However it has also some limitations: it can be used to fit only classical forms of linear and generalized linear models, and can’t, for example, use to fit psychometric functions that take attention lapses into account (see &lt;a href=&#34;https://mattelisi.github.io/post/model-averaging/&#34;&gt;here&lt;/a&gt;). Also, &lt;code&gt;lme4&lt;/code&gt; allows to fit multilevel models from a frequentist approach, and thus do not allow to incorporate prior knowledge into the model, or to use regularizing priors to reduce the risk of overfitting. For this reason, I have recently started using &lt;a href=&#34;http://mc-stan.org&#34;&gt;Stan&lt;/a&gt;, through its &lt;a href=&#34;http://mc-stan.org/users/interfaces/rstan.html&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textsf{R}\)&lt;/span&gt;Stan&lt;/a&gt; interface, to fit multilevel models in a Bayesian settings, and I find it great! It certainly requires more effort to define the models, however I think that the flexibility offered by a software like Stan is well worth the time spent to learning how to use it.&lt;/p&gt;
&lt;p&gt;For people like me, used to work with &lt;code&gt;lme4&lt;/code&gt;, Stan can be a bit discouraing at first. The approach to write the model is quite different, and it requires specifying explicitly all the distributional assumptions. Also, implementing models with correlated random effects requires some specific notions of algebra. So I prepared a first tutorial showing how to analyse in Stan one of the most common introductory examples to mixed-effects models, the &lt;code&gt;sleepstudy&lt;/code&gt; dataset (contained in the &lt;code&gt;lme4&lt;/code&gt; package). This will be followed by another tutorial showing how to use this approach to fit dataset where the dependent variable is a binary outcome, as it is the case for most psychophysical data.&lt;/p&gt;
&lt;div id=&#34;the-sleepstudy-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The &lt;code&gt;sleepstudy&lt;/code&gt; example&lt;/h1&gt;
&lt;p&gt;This dataset contains part of the data from a published study &lt;span class=&#34;citation&#34;&gt;(Belenky et al. 2003)&lt;/span&gt; that examined the effect of sleep deprivation on reaction times. (This is a sensible topic: think for example to long-distance truck drivers.) The dataset contains the average reaction times for the 18 subjects of the sleep-deprived group, for the first 10 days of the study, up to the recovery period.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
Loading required package: Matrix
str(sleepstudy)
&amp;#39;data.frame&amp;#39;:   180 obs. of  3 variables:
 $ Reaction: num  250 259 251 321 357 ...
 $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...
 $ Subject : Factor w/ 18 levels &amp;quot;308&amp;quot;,&amp;quot;309&amp;quot;,&amp;quot;310&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model I want to fit to the data will contain both random intercepts and slopes; in addition the correlation between the random effects should also be estimated. Using &lt;code&gt;lme4&lt;/code&gt;, this model could be estimated by using&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmer(Reaction ~ Days + (Days | Subject), sleepstudy)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model could be formally notated as
&lt;span class=&#34;math display&#34;&gt;\[
y_{ij} = \beta_0 + u_{0j} + \left( \beta_1 + u_{1j} \right) \cdot {\rm{Days}} + e_i
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; are the fixed effects parameters (intercept and slope), &lt;span class=&#34;math inline&#34;&gt;\(u_{0j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_{1j}\)&lt;/span&gt; are the subject specific random intercept and slope (the index &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; denotes the subject), and &lt;span class=&#34;math inline&#34;&gt;\(e \sim\cal N \left( 0,\sigma_e^2 \right)\)&lt;/span&gt; is the (normally distributed) residual error. The random effects &lt;span class=&#34;math inline&#34;&gt;\(u_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(u_1\)&lt;/span&gt; have a multivariate normal distribution, with mean 0 and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\left[ {\begin{array}{*{20}{c}}
{{u_0}}\\
{{u_1}}
\end{array}} \right] \sim\cal N \left( {\left[ {\begin{array}{*{20}{c}}
0\\
0
\end{array}} \right],\Omega  = \left[ {\begin{array}{*{20}{c}}
{\sigma _0^2}&amp;amp;{{\mathop{\rm cov}} \left( {{u_0},{u_1}} \right)}\\
{{\mathop{\rm cov}} \left( {{u_0},{u_1}} \right)}&amp;amp;{\sigma _1^2}
\end{array}} \right]} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In Stan, fitting this model requires preparing a separate text file (usually saved with the ‘.stan’ extension), containing several “blocks”. The 3 main types of blocks in Stan are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;data&lt;/code&gt;&lt;/strong&gt; all the dependent and independent variables needs to be declared in this blocks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;parameters&lt;/code&gt;&lt;/strong&gt; here one should declare the free parameters of the model; what Stan do is essentially use a MCMC algorithm to draw samples from the posterior distribution of the parameters given the dataset&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;model&lt;/code&gt;&lt;/strong&gt; here one should define the likelihood function and, if used, the priors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, we will use two other types of blocks, &lt;strong&gt;&lt;code&gt;transformed parameters&lt;/code&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;code&gt;generated quantities&lt;/code&gt;&lt;/strong&gt;. The first is necessary because we are estimating also the full correlation matrix of the random effects. We will parametrize the covariance matrix as the Cholesky factor of the correlation matrix (see &lt;a href=&#34;https://mattelisi.github.io/post/simulating-correlated-variables-with-the-cholesky-factorization/&#34;&gt;my post on the Cholesky factorization&lt;/a&gt;), and in the &lt;code&gt;transformed parameters&lt;/code&gt; block we will multiply the random effects with the Choleki factor, to transform them so that they have the intended correlation matrix. The &lt;code&gt;generated quantities&lt;/code&gt; block can be used to compute any additional quantities we may want to compute once for each sample; I will use it to transform the Cholesky factor into the correlation matrix (this step is not essential but makes the examination of the model easier).&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;RStan requires the data to be organized in a list object. It can be done with the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_stan &amp;lt;- list(Subject = as.numeric(factor(sleepstudy$Subject, 
    labels = 1:length(unique(sleepstudy$Subject)))), Days = sleepstudy$Days, 
    RT = sleepstudy$Reaction/1000, N = nrow(sleepstudy), J = length(unique(sleepstudy$Subject)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I also included two scalar variables, &lt;code&gt;N&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;, indicating respectively the number of observation and the number of subjects. &lt;code&gt;Subject&lt;/code&gt; was a categorical factor, but to input it in Stan I transformed it into an integer index. I also rescaled the reaction times, so that they are in seconds instead of milliseconds.&lt;/p&gt;
&lt;p&gt;These variables can be declared in Stan with the following block. We need to declare the variable type (e.g. real or integer, similarly to programming languages as C++) and for vectors we need to declare the length of the vectors (hence the need of the two scalar variables &lt;code&gt;N&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;). Note that variables can be given lower and upper bounds. See the Stan reference manual for more information of the variable types.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int&amp;lt;lower=1&amp;gt; N;            //number of observations
  real RT[N];                //reaction times

  int&amp;lt;lower=0,upper=9&amp;gt; Days[N];   //predictor (days of sleep deprivation)

  // grouping factor
  int&amp;lt;lower=1&amp;gt; J;                   //number of subjects
  int&amp;lt;lower=1,upper=J&amp;gt; Subject[N];  //subject id
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameters&lt;/h2&gt;
&lt;p&gt;Here is the parameter block. Stan will draw samples from the posterior distribution of all the parameters listed here. Note that for parameters representing standard deviations is necessary to set the lower bound to 0 (variances and standard deviations cannot be negative). This is equivalent to estimating the logarithm of the standard deviation (which can be both positive or negative) and exponentiating before computing the likelihood (because &lt;span class=&#34;math inline&#34;&gt;\(e^x&amp;gt;0\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;). Note that we have also one parameter for the standard deviation of the residual errors (which was implicit in &lt;code&gt;lme4&lt;/code&gt;).
The random effects are parametrixed by a 2 x &lt;code&gt;J&lt;/code&gt; random effect matrix &lt;code&gt;z_u&lt;/code&gt;, and by the Cholesky factor of the correlation matrix &lt;code&gt;L_u&lt;/code&gt;. I have added also the transformed parameters block, where the Cholesky factor is first multipled by the diagonal matrix formed by the vector of the random effect variances &lt;code&gt;sigma_u&lt;/code&gt;, and then is multiplied with the random effect matrix, to obtain a random effects matrix with the intended correlations, which will be used in the model block below to compute the likelihood of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;parameters {
  vector[2] beta;                   // fixed-effects parameters
  real&amp;lt;lower=0&amp;gt; sigma_e;            // residual std
  vector&amp;lt;lower=0&amp;gt;[2] sigma_u;       // random effects standard deviations

  // declare L_u to be the Choleski factor of a 2x2 correlation matrix
  cholesky_factor_corr[2] L_u;

  matrix[2,J] z_u;                  // random effect matrix
}

transformed parameters {
  // this transform random effects so that they have the correlation
  // matrix specified by the correlation matrix above
  matrix[2,J] u;
  u = diag_pre_multiply(sigma_u, L_u) * z_u;

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;Finally the model block. Here we can define priors for the parameters, and then write the likelihood of the data given the parameters. The likelihood function corresponds to the model equation we saw before.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model {
  real mu; // conditional mean of the dependent variable

  //priors
  L_u ~ lkj_corr_cholesky(1.5); // LKJ prior for the correlation matrix
  to_vector(z_u) ~ normal(0,2);
  sigma_e ~ normal(0, 5);       // prior for residual standard deviation
  beta[1] ~ normal(0.3, 0.5);   // prior for fixed-effect intercept
  beta[2] ~ normal(0.2, 2);     // prior for fixed-effect slope

  //likelihood
  for (i in 1:N){
    mu = beta[1] + u[1,Subject[i]] + (beta[2] + u[2,Subject[i]])*Days[i];
    RT[i] ~ normal(mu, sigma_e);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the correlation matrix, Stan manual suggest to use a LKJ prior&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. This prior has one single shape parameters, &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;: if you set &lt;span class=&#34;math inline&#34;&gt;\(\eta=1\)&lt;/span&gt; then you have effectively a uniform prior distribution over any (Cholesky factor of) 2x2 correlation matrices. For values &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;gt;1\)&lt;/span&gt; instead you get a more conservative prior, with a mode in the identity matrix (where the correlations are 0). For more information about the LKJ prior see page 556 of Stan reference manual, version 2.17.0, and also &lt;a href=&#34;http://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;this page&lt;/a&gt; for an intuitive demonstration.&lt;/p&gt;
&lt;p&gt;Importantly, I have used (weakly) informative priors for the fixed effect estimates. We know from the literature that simple reaction times are around 300ms, hence the prior for the intercept, which represents the avearage reaction times at Day 0, i.e. before the sleep deprivation. We expect the reaction times to increase with sleep deprivation, so I have used for the slope a Gaussian prior centered at a small positive value (0.2 seconds), which would represents the increase in reaction times with each day of sleep deprivation, however using a very broad standard deviation (2 seconds), which could accomodate also negative or very different slope values if needed. It may be useful to visualize with a plot the priors.
&lt;img src=&#34;http://mlisi.xyz/post/2018-03-4-Bayesian-multilevel-models-R-Stan_files/figure-html/fig1-1.png&#34; width=&#34;624&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generated-quantities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generated quantities&lt;/h2&gt;
&lt;p&gt;Finally, we can add one last block to the model file, to store for each sampling iteration the correlation matrix of the random effect, which can be computed multyplying the Cholesky factor with its transpose.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;generated quantities {
  matrix[2, 2] Omega;
  Omega = L_u * L_u&amp;#39;; // so that it return the correlation matrix
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating the model&lt;/h1&gt;
&lt;p&gt;Having written all the above blocks in a separate text file (I called it “sleep_model.stan”), we can call Stan from R with following commands. I run 4 independent chains (each chain is a stochastic process which sequentially generate random values; they are called &lt;em&gt;chain&lt;/em&gt; because each sample depends on the previous one), each for 2000 samples. The first 1000 samples are the &lt;em&gt;warmup&lt;/em&gt; (or sometimes called &lt;em&gt;burn-in&lt;/em&gt;), which are intended to allow the sampling process to settle into the posterior distribution; these samples will not be used for inference. Each chain is independent from the others, therefore having multiple chains is also useful to check the convergence (i.e. by looking if all chains converged to the same regions of the parameter space). Additionally, having multiple chain allows to compute a statistic which is also used to check convergence: this is called &lt;span class=&#34;math inline&#34;&gt;\(\hat R\)&lt;/span&gt; and it corresponds to the ratio of the between-chain variance and the within-chain variance. If the sampling has converged then &lt;span class=&#34;math inline&#34;&gt;\({\hat R} \approx 1 \pm 0.01\)&lt;/span&gt;.
When we call function &lt;code&gt;stan&lt;/code&gt;, it will compile a C++ program which produces samples from the joint posterior of the parameter using a powerful variant of MCMC sampling, called &lt;em&gt;Hamiltomian Monte Carlo&lt;/em&gt; (see &lt;a href=&#34;http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/&#34;&gt;here&lt;/a&gt; for an intuitive explanation of the sampling algorithm).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)
options(mc.cores = parallel::detectCores())  # indicate stan to use multiple cores if available
sleep_model &amp;lt;- stan(file = &amp;quot;sleep_model.stan&amp;quot;, data = d_stan, 
    iter = 2000, chains = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One way to check the convergence of the model is to plot the chain of samples. They should look like a &lt;em&gt;“fat, hairy caterpillar which does not bend”&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Sorensen, Hohenstein, and Vasishth 2016)&lt;/span&gt;, suggesting that the sampling was stable at the posterior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(sleep_model, pars = c(&amp;quot;beta&amp;quot;), inc_warmup = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://mlisi.xyz/post/2018-03-4-Bayesian-multilevel-models-R-Stan_files/figure-html/fig2-1.png&#34; width=&#34;480&#34; /&gt;
There is a &lt;code&gt;print()&lt;/code&gt; method for visualising the estimates of the parameters. The values of the &lt;span class=&#34;math inline&#34;&gt;\({\hat R}\)&lt;/span&gt; (&lt;code&gt;Rhat&lt;/code&gt;) statistics also confirm that the chains converged. The method automatically report credible intervals for the parameters (computed with the percentile method from the samples of the posterior distribution).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(sleep_model, pars = c(&amp;quot;beta&amp;quot;), probs = c(0.025, 0.975), 
    digits = 3)
Inference for Stan model: sleep_model_v1.
5 chains, each with iter=6000; warmup=3000; thin=1; 
post-warmup draws per chain=3000, total post-warmup draws=15000.

         mean se_mean    sd  2.5% 97.5% n_eff Rhat
beta[1] 0.255       0 0.006 0.243 0.268  6826    1
beta[2] 0.011       0 0.001 0.008 0.013  7830    1

Samples were drawn using NUTS(diag_e) at Sat Sep 22 17:15:42 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can visualze the posterior distribution as histograms (here for the fixed effects parameters and the standard deviations of the corresponding random effects).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sleep_model, plotfun = &amp;quot;hist&amp;quot;, pars = c(&amp;quot;beta&amp;quot;, &amp;quot;sigma_u&amp;quot;))
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://mlisi.xyz/post/2018-03-4-Bayesian-multilevel-models-R-Stan_files/figure-html/fig3-1.png&#34; width=&#34;384&#34; /&gt;
Finally, we can also examine the correlation matrix of random-effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(sleep_model, pars = c(&amp;quot;Omega&amp;quot;), digits = 3)
Inference for Stan model: sleep_model_v1.
5 chains, each with iter=6000; warmup=3000; thin=1; 
post-warmup draws per chain=3000, total post-warmup draws=15000.

            mean se_mean    sd   2.5%   25%   50%   75% 97.5% n_eff  Rhat
Omega[1,1] 1.000     NaN 0.000  1.000 1.000 1.000 1.000 1.000   NaN   NaN
Omega[1,2] 0.221   0.007 0.344 -0.546 0.011 0.251 0.467 0.807  2228 1.001
Omega[2,1] 0.221   0.007 0.344 -0.546 0.011 0.251 0.467 0.807  2228 1.001
Omega[2,2] 1.000   0.000 0.000  1.000 1.000 1.000 1.000 1.000   160 1.000

Samples were drawn using NUTS(diag_e) at Sat Sep 22 17:15:42 2018.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Rhat&lt;/code&gt; values for the first entry of the correlation matrix is NaN. This is expected for variables that remain constant during samples. We can check that this variable resulted in a series of identical values during sampling with the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(unlist(extract(sleep_model, pars = &amp;quot;Omega[1,1]&amp;quot;)) == 1)  # all values are =1 ?
[1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all! You can check by yourself that the values are the sufficiently similar to what we would obtain using &lt;code&gt;lmer&lt;/code&gt;, and eventually experiment by yourself how the estimates changes when more informative priors are used. For more examples on how to fit linear mixed-effects models using Stan I recommend the article by Sorensen &lt;span class=&#34;citation&#34;&gt;(Sorensen, Hohenstein, and Vasishth 2016)&lt;/span&gt;, which also show how to implement &lt;em&gt;crossed&lt;/em&gt; random effects of subjects and item (words), as it is conventional in linguistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Bates2014&#34;&gt;
&lt;p&gt;Bates, D, M Maechler, B Bolker, and S Walker. 2014. “lme4: Linear mixed-effects models using Eigen and S4.” R package version 1.1-7. &lt;a href=&#34;http://cran.r-project.org/package=lme4&#34;&gt;http://cran.r-project.org/package=lme4&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Belenky2003&#34;&gt;
&lt;p&gt;Belenky, Gregory, Nancy J Wesensten, David R Thorne, Maria L Thomas, Helen C Sing, Daniel P Redmond, Michael B Russo, and J Balkin, Thomas. 2003. “Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study.” &lt;em&gt;Journal of Sleep Research&lt;/em&gt; 12 (1): 1–12. &lt;a href=&#34;https://doi.org/10.1046/j.1365-2869.2003.00337.x&#34;&gt;https://doi.org/10.1046/j.1365-2869.2003.00337.x&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Sorensen2016&#34;&gt;
&lt;p&gt;Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. “Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.” &lt;em&gt;The Quantitative Methods for Psychology&lt;/em&gt; 12 (3): 175–200. &lt;a href=&#34;https://doi.org/10.20982/tqmp.12.3.p175&#34;&gt;https://doi.org/10.20982/tqmp.12.3.p175&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The LKJ prior is named after the authors, see: Lewandowski, D., Kurowicka, D., and Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, 100:1989–2001&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Listing&#39;s law, and the mathematics of the eyes</title>
      <link>http://mlisi.xyz/post/listing-s-law-and-the-mathematics-of-the-eyes/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      <guid>http://mlisi.xyz/post/listing-s-law-and-the-mathematics-of-the-eyes/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Brief intro to the mathematical formalism used to describe rotations of the eyes in 3D (including the torsional component).&lt;/em&gt;
&lt;img src=&#34;http://mlisi.xyz/img/3Deyecoord_Haslwanter1995.png&#34; alt=&#34;3D eye coordinate systems in the primary reference position, left panel, and after a leftward rotation, right panel (Haslwanter 1995). &#34; /&gt;&lt;/p&gt;
&lt;p&gt;The shape of the human eye is approximately a sphere with a diameter of 23 mm, and mechanically it behaves like a ball in a ball and socket joint. Because there is a functional distinguished axis - the visual axis, that is the line of gaze or more precisely the imaginary straight line passing through both the center of the pupil and the center of the fovea - the movements of the eyes are usually divided in &lt;em&gt;gaze direction&lt;/em&gt; and &lt;em&gt;cyclotorsion&lt;/em&gt; (or simply &lt;em&gt;torsion&lt;/em&gt;): while gaze direction refers to the direction of the visual axis, the torsion indicates the rotation of the eyeball about the visual axis. While modern video-based eyetrackers allow to record movements of the visual axis, they do not provide data about torsion. It turns out that there is a nice mathematical relationship that constrains the torsion of the eye in every direction of the gaze.
This relationship is known as Listing’s law, and was named after the german mathematician &lt;a href=&#34;https://en.wikipedia.org/wiki/Johann_Benedict_Listing&#34;&gt;Johann Benedict Listing (1808-1882)&lt;/a&gt;. Listing’s law can be better understood by looking at how the 3D orientation of the eye can be formally described.&lt;/p&gt;
&lt;div id=&#34;mathematics-of-3d-eye-movements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mathematics of 3D eye movements&lt;/h1&gt;
&lt;p&gt;3D eye position can be specified by characterising the 3D rotation that brings the eye to the current eye position from an arbitrary reference or &lt;em&gt;primary&lt;/em&gt; position&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, which typically is defined as the position that the eye assumes when looking straight ahead with the head in normal, upright position. This rotation can be described by the 3-by-3 rotation matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt;. More specifically the matrix can be used to describe the rotation of three-dimensional coordinates by a certain angle about a certain axis. To formally define this matrix, consider the coordinate system &lt;span class=&#34;math inline&#34;&gt;\(\{ \vec{h}_1,\vec{h}_2,\vec{h}_3 \}\)&lt;/span&gt; (a coordinate system is defined by a set of linearly independent vectors; e.g. here &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1 = (1,0,0)\)&lt;/span&gt;, corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; axis) as the &lt;em&gt;head-centered&lt;/em&gt; coordinate system where the axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1\)&lt;/span&gt; correspond to the visual axis when the eye is in the reference position, and &lt;span class=&#34;math inline&#34;&gt;\(\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}\)&lt;/span&gt; is an &lt;em&gt;eye-centered&lt;/em&gt; coordinate system where &lt;span class=&#34;math inline&#34;&gt;\(\vec{e}_1\)&lt;/span&gt; always correspond to the visual axis, regardless of the orientation of the eye (see the figure on top of this page). Any orientation of the eye can be described by a matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[
{{\vec{e}}_i} = {\bf{R}} {{\vec{h}}_i}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(i=1,2,3\)&lt;/span&gt;. This rotation matrix is straightforward for 1D rotations. For example, a purely horizontal rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; around the axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_3\)&lt;/span&gt; is formulated as
&lt;span class=&#34;math display&#34;&gt;\[
\bf{R}_3 \left( \theta  \right) = \left( {\begin{array}{*{20}{c}}
{\cos \theta }&amp;amp;{ - \sin \theta }&amp;amp;0\\
{\sin \theta }&amp;amp;{\cos \theta }&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{array}} \right)
\]&lt;/span&gt;
The first two columns of the matrix indicates the new coordinates of the first (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1\)&lt;/span&gt;) and of the second basis (&lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_2\)&lt;/span&gt;) of the new eye-centerd coordinate system after the rotation, expressed according to the initial head-centered coordinate system. The third basis, &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_3\)&lt;/span&gt; is the axis of rotation, and does not change. It becomes more complicated for 3D rotations, i.e. rotations of the fixed eye-centered coordinate system to any new orientation. They can be obtained by calculating a sequence of 3 different rotations about the three fixed axis, and multiplying the corresponding matrices: &lt;span class=&#34;math inline&#34;&gt;\(\bf{R} = \bf{R}_3 \left( \theta \right) \bf{R}_2 \left( \phi \right) \bf{R}_1 \left( \psi \right)\)&lt;/span&gt;. Although the first two rotations are sufficient to specity the orientation of the visual axis, the third is necessary to specify the torsion component and fully specify the 3D orientation of the eye. Importantly, the order of the three rotations is relevant - rotations are not commutative, so if you put them in different order you end up with a different result - and needs to be arbitrarily specified (when it is specified in this order is referred to as &lt;em&gt;Flick sequence&lt;/em&gt;). This representation of 3D orientations is not very efficient (9 values, while only 3 are necessary), or practical for computations; additionally one needs to define arbitrarily the order of the rotations of the sequence.&lt;/p&gt;
&lt;div id=&#34;quaternions-and-rotation-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quaternions and rotation vectors&lt;/h2&gt;
&lt;p&gt;An alternative way to describe rotations is with &lt;em&gt;quaternions&lt;/em&gt;. Quaternions can be looked upon as four-dimensional vectors, although they are more commonly split in a real scalar part and an imaginary vector part; they are in fact an extension of the complex numbers. They have the form
&lt;span class=&#34;math display&#34;&gt;\[
q_0 + q_1i + q_2j + q_3k = \left( q_0,\vec{q} \cdot \vec{I} \right) = \left( r,\vec{v} \right)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
\vec{q} = \left( \begin{array}{*{20}{c}}
{q_1}\\
{q_2}\\
{q_3}
\end{array} \right)
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\vec{I} = \left( \begin{array}{*{20}{c}}
{i}\\
{j}\\
{k}
\end{array} \right)
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(i,j,k\)&lt;/span&gt; are the quaternion units. These can be multiplied according to the following formula, discovered by &lt;a href=&#34;https://en.wikipedia.org/wiki/William_Rowan_Hamilton&#34;&gt;Hamilton&lt;/a&gt; in 1843
&lt;span class=&#34;math display&#34;&gt;\[
i^2 = j^2 = k^2 = ijk =  - 1
\]&lt;/span&gt;
This formula may seems strange but it determines all the possible products of &lt;span class=&#34;math inline&#34;&gt;\(i,j,k\)&lt;/span&gt;, such as &lt;span class=&#34;math inline&#34;&gt;\(ij=k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(ji=-k\)&lt;/span&gt;. Note that the product of the basis are not commutative. There is a visual trick to remember the multiplication rules, based on the following diagram:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://mlisi.xyz/img/quatrule.png&#34; alt=&#34;Multiplying quaternions. Multiplying two elements in the clockwise direction gives the next element along the same direction (e.g. jk=i). The same is for counter-clockwise directions, except that the result is negative (e.g. kj=-i). &#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Multiplying quaternions. Multiplying two elements in the clockwise direction gives the next element along the same direction (e.g. &lt;span class=&#34;math inline&#34;&gt;\(jk=i\)&lt;/span&gt;). The same is for counter-clockwise directions, except that the result is negative (e.g. &lt;span class=&#34;math inline&#34;&gt;\(kj=-i\)&lt;/span&gt;). &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Quaternions can be used to represent rotations. For example, a rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; around the axis define by the unit vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{u} = (u_1, u_2,u_3) = u_1i + u_2j + u_3k\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; can be described by the following quaternion
&lt;span class=&#34;math display&#34;&gt;\[
\cos \frac{\theta}{2} + \sin \frac{\theta}{2}\left( u_1i + u_2j + u_3k \right)
\]&lt;/span&gt;
The direction of the rotation is given by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Right-hand_rule#A_rotating_body&#34;&gt;right-hand rule&lt;/a&gt;.
Successive rotations can combined using the formula for quaternion multiplication. The multiplication of quaternions can be computed by the products of their elements element as if they were two polynomials, but keeping track of the ordering of the basis, as their multiplication is not commutative. This is a desired property if we want to specify rotations, which as seen earlier are also not commutative. Quaternion multiplication can be also expressed in the modern language of vector and cross product
&lt;span class=&#34;math display&#34;&gt;\[
\left( r_1,\vec{v_1} \right) \left( r_2,\vec{v_2} \right) = 
\left( r_1 r_2 - \vec{v_1} \cdot \vec{v_2},\;\; r_1\vec{v_2} + r_2\vec{v_1} +\vec{v_1} \times \vec{v_2} \right)
\]&lt;/span&gt;
where “&lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt;” is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_product&#34;&gt;dot product&lt;/a&gt; and “&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;” is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_product&#34;&gt;cross product&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In sum, quaternions are pretty useful to compute transformations in 3D. One can use quaternions to combine 3D any sequence of rotations about arbitrary axis (using quaternion multiplications), as well as to rotate any 3D Euclidean vector about any arbitrary axis. A quaternion can also be transformed into a 3D rotation matrix (formula &lt;a href=&#34;https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Quaternion-derived_rotation_matrix&#34;&gt;here&lt;/a&gt;), which then may be used in 3D graphics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rotation vectors&lt;/strong&gt; are an even more succint representation of rotations. Indeed, the scalar component of the quaterion (&lt;span class=&#34;math inline&#34;&gt;\(q_0\)&lt;/span&gt;) does not add any information that is not alredy in the vector part, so a rotation could be effectively described by just 3 numbers. The rotation vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{r}\)&lt;/span&gt;, which correspond to a rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; about an axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{n}\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[
\vec{r} = \tan \left( \frac{\theta}{2} \right) \vec{n}
\]&lt;/span&gt;
which can be defined also with respect to the equivalent quaternion &lt;span class=&#34;math inline&#34;&gt;\(\textbf{q}\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\textbf{q}=\left( q_0, \vec{q} \right) = \left( \cos \left(\frac{\theta}{2}\right), \sin \left(\frac{\theta}{2}\right)\vec{n} \right)
\]&lt;/span&gt;
as
&lt;span class=&#34;math display&#34;&gt;\[ \vec{r} = \frac{\vec{q}}{q_0} \]&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;donders-law-and-listings-law&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Donder’s law and Listing’s law&lt;/h1&gt;
&lt;p&gt;Donder’s law (1848) states that the eye use only two degrees of freedom while fixating, although mechanically it has three. In othere words this means that the torsion component of the eye movement is not arbitrary but it is uniquely determined by the direction of the visual axis and is independent of the previous eye movements. From the material review above it should be clear how any 3D eye orientation can be fully described as a rotation abour a given axis from a primary reference position. This allows also to formulate Donder’s law more specifically,according to what is known as Listing’s law &lt;span class=&#34;citation&#34;&gt;(Helmholtz et al. 1910,&lt;span class=&#34;citation&#34;&gt;@Haustein1989&lt;/span&gt;)&lt;/span&gt; “&lt;em&gt;There exists a certain eye position from which the eye may reach any other position of fixation by a rotation around an axis perpendicular to the visual axis. This particular position is called primary position&lt;/em&gt;”. This means that &lt;em&gt;all possible eye positions&lt;/em&gt; can be reached from the primary position by a single rotation about an axis perpendicular to the visual axis. Since they are all perpendicular to the visual axis, all rotation axis that satisfy Listing’s law are on the same plane (&lt;em&gt;Listing’s plane&lt;/em&gt;). The law can be tested with eyetracking equipments that allows measuring also the torsional components (such as scleral coils): results have shown that the standard deviation from Listing’s plane of empirically measured rotation vectors is only about 0.5-1 deg &lt;span class=&#34;citation&#34;&gt;(Haslwanter 1995)&lt;/span&gt;. Formally it can be written that for any orientation of the visual axis, defined by the rotation vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{a}\)&lt;/span&gt; and measured from the primary position &lt;span class=&#34;math inline&#34;&gt;\(\vec{h_1}=(1,0,0)\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\vec{h_1} \cdot \vec{a} = 0
\]&lt;/span&gt;
This indicates simply that the rotation about the visual axis is 0, and that as a consequence all the rotation axes lies in a frontal plane.&lt;/p&gt;
&lt;p&gt;Going back to the beginning, knowing the coordinates os Listing’s plane one can compute the rotation vector that correspond to the current eye position from the recording of the 2D gaze location on a screen. In the simplest case, we assume that the primary position corresponds to when the observer fixates the center of the screen, &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt;. What is the rotation vector that describes the 3D eye orientation when the observer fixates the location &lt;span class=&#34;math inline&#34;&gt;\((s_x, s_y)\)&lt;/span&gt; ? Let’s say the position on screen is defined in cm, and we know that the distance of the eye from the screen is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; cm. The rotation angle can be computed as &lt;span class=&#34;math inline&#34;&gt;\(\theta = \rm{atan} \frac{\sqrt{s_x^2+s_y^2}}{L}\)&lt;/span&gt;, while the angle that defines the orientation of the rotation axis within Listing’s plane is &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \rm{atan2}(s_y,s_x)\)&lt;/span&gt;. The complete rotation vector is then
&lt;span class=&#34;math display&#34;&gt;\[
\vec{r} = \tan \left( \frac{\theta}{2}\right) \cdot \left( {\begin{array}{*{20}{c}}
0\\
{\cos \alpha }\\
{ - \sin \alpha }
\end{array}} \right)
\]&lt;/span&gt;
This vector describe aparticular eye position as a rotation from the reference position, and does not have a torsional component (that is a component along &lt;span class=&#34;math inline&#34;&gt;\(\vec{h_1}\)&lt;/span&gt;). Indeed, Listing’s law implies that all possible eye positions can be reached from the primary reference position without a torsional component. However, vectors describing rotations from and to positions different than the primary one do &lt;em&gt;not&lt;/em&gt;, in general, lie in Listing’s plane. For Listing law to hold such vectors must lie in a plane whose orientation depends on the current eye position, and more specifically is such that the vector perpendicular to the plane is exactly halfway between the current and the primary eye position &lt;span class=&#34;citation&#34;&gt;(Tweed and Vilis 1990)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Haslwanter1995&#34;&gt;
&lt;p&gt;Haslwanter, Thomas. 1995. “Mathematics of three-dimensional eye rotations.” &lt;em&gt;Vision Research&lt;/em&gt; 35 (12): 1727–39. &lt;a href=&#34;https://doi.org/10.1016/0042-6989(94)00257-M&#34;&gt;https://doi.org/10.1016/0042-6989(94)00257-M&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Haustein1989&#34;&gt;
&lt;p&gt;Haustein, Werner. 1989. “Considerations on Listing’s Law and the primary position by means of a matrix description of eye position control.” &lt;em&gt;Biological Cybernetics&lt;/em&gt; 60 (6): 411–20. &lt;a href=&#34;https://doi.org/10.1007/BF00204696&#34;&gt;https://doi.org/10.1007/BF00204696&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Helmholtz1910&#34;&gt;
&lt;p&gt;Helmholtz, Hermann von, Hermann von Helmholtz, Hermann von Helmholtz, and Hermann von Helmholtz. 1910. &lt;em&gt;Handbuch der Physiologischen Optik&lt;/em&gt;. Hamburg: Voss.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Tweed1990&#34;&gt;
&lt;p&gt;Tweed, Douglas, and Tutis Vilis. 1990. “Geometric relations of eye position and velocity vectors during saccades.” &lt;em&gt;Vision Research&lt;/em&gt; 30 (1): 111–27. &lt;a href=&#34;https://doi.org/10.1016/0042-6989(90)90131-4&#34;&gt;https://doi.org/10.1016/0042-6989(90)90131-4&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Euler%27s_rotation_theorem&#34;&gt;Euler’s theorem&lt;/a&gt; guarantee that a rigid body can always move from one orientation to any different one through a single rotation about a fixed axis.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Saying that &lt;span class=&#34;math inline&#34;&gt;\(\vec(u)\)&lt;/span&gt; is a unit vector indicates that it has length 1, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\left| \vec{u} \right| = \sqrt{u_1^2 + u_2^2 + u_3^2} = 1\)&lt;/span&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
