<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>psychophysics | Matteo Lisi</title>
    <link>http://mlisi.xyz/tags/psychophysics/</link>
      <atom:link href="http://mlisi.xyz/tags/psychophysics/index.xml" rel="self" type="application/rss+xml" />
    <description>psychophysics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Matteo Lisi</copyright><lastBuildDate>Fri, 08 Dec 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://mlisi.xyz/img/me_off.JPG</url>
      <title>psychophysics</title>
      <link>http://mlisi.xyz/tags/psychophysics/</link>
    </image>
    
    <item>
      <title>Multi-model estimation of psychophysical parameters</title>
      <link>http://mlisi.xyz/post/model-averaging/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      <guid>http://mlisi.xyz/post/model-averaging/</guid>
      <description>


&lt;p&gt;In the study of human perception we often need to measure how sensitive is an observer to a stimulus variation, and how her/his sensitivity changes due to changes in the context or experimental manipulations. In many applications this can be done by estimating the slope of the psychometric function&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a parameter that relates to the precision with which the observer can make judgements about the stimulus. A psychometric function is generally characterized by 2-3 parameters: the slope, the threshold (or criterion), and an optional lapse parameter, which indicate the rate at which attention lapses (i.e. &lt;em&gt;stimulus-independent&lt;/em&gt; errors) occur.&lt;/p&gt;
&lt;p&gt;As an example, consider the situation where an observer is asked to judge whether a signal (can be anything, from the orientation angle of a line on a screen, or the pitch of a tone, to the speed of a car or the approximate number of people in a crowd, etc.) is above or below a given reference value, call it zero. The experimenter presents the observers with many signals of different intensities, and the observer is asked to respond by making a binary choice (larger/smaller than the reference), under two different contextual conditions (before/after having a pint, with different headphones, etc.). These two conditions are expected to results in different sensitivity, and the experimenter is interested in estimating as precisely as possible the difference in sensitivity&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The psychometric function for one observer in the two conditions might look like this (figure below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://mlisi.xyz/img/psyfun.png&#34; alt=&#34;Psychometric functions. Each points is a response (0 or 1 ; some vertical jitter is added for clarity), and the lines represent the fitted psychometric model (here a cumulative Gaussian psychometric function). The two facets of the plots represent the two different conditions. It can be seen that the precision seems to be different across conditions: judgements made under condition ‘2’ are more variable, indicating reduced sensitivity. &#34; style=&#34;width:70.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Psychometric functions. Each points is a response (0 or 1 ; some vertical jitter is added for clarity), and the lines represent the fitted psychometric model (here a cumulative Gaussian psychometric function). The two facets of the plots represent the two different conditions. It can be seen that the precision seems to be different across conditions: judgements made under condition ‘2’ are more variable, indicating reduced sensitivity. &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Our focus is on the psychometric slope, and we are not really interested in measuring the lapse rate; however it is still important to take lapses into account: it has been shown that not accounting for lapses can have a large influence on the estimates of the slope &lt;span class=&#34;citation&#34;&gt;(Wichmann and Hill 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-problem-with-lapses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The problem with lapses&lt;/h3&gt;
&lt;p&gt;Different observer may lapse at quite different rates, and for some of them the lapse rate is probably so small that can be considered negligible. Also, we usually don’t have hypothesis about lapses, and about whether they should or should not vary across conditions.
We can base our analysis on different assumptions about when the observers may have attention lapses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;they may never lapse (or they do so with a small, negligible frequency);&lt;/li&gt;
&lt;li&gt;they may lapse at a fairly large rate, but the rate is assumed constant across conditions (reasonable, especially if conditions are randomly interleaved);&lt;/li&gt;
&lt;li&gt;they may lapse with variable rate across conditions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These assumptions will lead to three different psychometric models. The number can increase if we consider also different functional forms of the relationship between stimulus and choice; here for simplicity I will consider only psychometric models based on the cumulative Gaussian function (equivalent to a &lt;em&gt;probit&lt;/em&gt; analysis),
&lt;span class=&#34;math inline&#34;&gt;\(\Phi (\frac{x-\mu}{\sigma}) = \frac{1}{2}\left[ {1 + {\rm{erf}}\left( {\frac{{x - \mu }}{{\sigma \sqrt 2 }}} \right)} \right]\)&lt;/span&gt;,
where the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; woud correspond to the threshold parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to the slope, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the stimulus intensity.
In our case the first assumption (&lt;em&gt;zero lapses&lt;/em&gt;) would lead to the simplest psychometric model
&lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i)= \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt;
where the subscript &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; indicates that the values of both mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; are specific to the condition &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.
The second assumption (&lt;em&gt;fixed lapse rate&lt;/em&gt;) could correspond to the model
&lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i, \lambda)= \lambda + (1-2\lambda) \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt;
where the parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; correspond to the probability of the observer making a random error. Note that this is assumed to be fixed with respect to the condition (no subscript).
Finally the last assumption (&lt;em&gt;variable lapse rate&lt;/em&gt;) would suggests the model
&lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i, \lambda_i)= \lambda_i + (1-2\lambda_i) \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt;
where all the parameters are allowed to vary between conditions.&lt;/p&gt;
&lt;p&gt;We have thus three different models, but we haven’t any prior information to decide which model is more likely to be correct in our case. Also, we acknowledge the fact that there are individual differences and each observer in our sample may conform to one of the three assumptions with equal probability. Hence, ideally, we would like to find a way to deal with lapses - and find the best estimates of the slope values &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; without committing to one of the three models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-model-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-model inference&lt;/h1&gt;
&lt;p&gt;One possible solution to this problem is provided by a &lt;em&gt;multi-model&lt;/em&gt;, or model averaging, approach &lt;span class=&#34;citation&#34;&gt;(Burnham and Anderson 2002)&lt;/span&gt;. This requires calculating the &lt;a href=&#34;https://en.wikipedia.org/wiki/Akaike_information_criterion&#34;&gt;AIC (Akaike Information Criterion)&lt;/a&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; for each model and subjects, and then combine the estimates according to the Akaike weights of each model. To compute the Akaike weights one typically proceed by first transforming them into differences with respect to the AIC of the best candidate model (i.e. the one with lower AIC)
&lt;span class=&#34;math display&#34;&gt;\[
{\Delta _m} = {\rm{AI}}{{\rm{C}}_m} - \min {\rm{AIC}}
\]&lt;/span&gt;
From the differences in AIC, we can obtain an estimate of the relative likelihood of the model &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; given the data
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L} \left( {m|{\rm{data}}} \right) \propto \exp \left( { - \frac{1}{2}{\Delta _m}} \right)
\]&lt;/span&gt;
Then, to obtain the Akaike weight &lt;span class=&#34;math inline&#34;&gt;\(w_m\)&lt;/span&gt; of the model &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, the relative likelihoods are normalized (divided by their sum)
&lt;span class=&#34;math display&#34;&gt;\[
{w_m} = \frac{{\exp \left( { - \frac{1}{2}{\Delta _m}} \right)}}{{\mathop \sum \limits_{k = 1}^K \exp \left( { - \frac{1}{2}{\Delta _k}} \right)}}
\]&lt;/span&gt;
Finally, one can compute the model-averaged estimate of the parameter&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat {\bar \sigma}\)&lt;/span&gt;, by combining the estimate of each model according to their Akaike weight
&lt;span class=&#34;math display&#34;&gt;\[
\hat {\bar \sigma} = \sum\limits_{k = 1}^K {{w_k}\hat \sigma_k } 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation results&lt;/h1&gt;
&lt;p&gt;Model averaging seems a sensitive approach to deal with the uncertainty about which form of the model is best suited to our data. To see whether it is worth doing the extra work of fitting 3 models instead of just one, I run a simulation, where I repeatedly fit and compare the estimates of the three models, with the model-averaged estimate, for different values of sample sizes. In all the simulations, each observer is generated by randomly drawing parameters from a Gaussian distribution which summarize the distribution of the parameters in the population. Hence, I know the &lt;em&gt;true&lt;/em&gt; difference in sensitivity in the population, and by simulating and fitting the models I can test which estimating procedure is more &lt;em&gt;efficient&lt;/em&gt;. In statistics a procedure or an estimator is said to be more efficient than another one when it provides a better estimate with the same number or fewer observations. The notion of “better” clearly relies on the choice of a cost function, which for example can be the mean squared error (it is here).&lt;/p&gt;
&lt;p&gt;Additionally, in my simulations each simulated observer could, &lt;em&gt;with equal probability&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{3}\)&lt;/span&gt;, either never lapse, lapse with a constant rate across conditions, or lapse at a higher rate in the more difficult condition (condition ‘2’ where the judgements are less precise). The lapse rates were draw uniformly from the interval [0.01, 0.1], and could get as high as 0.15 in condition ‘2’. Each simulated observer ran 250 trials per condition (similar to the figure at the top of this page). I simulated dataset from &lt;span class=&#34;math inline&#34;&gt;\(n=5\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(n=50\)&lt;/span&gt;, using 100 iterations for each sample size (only 85 in the case of &lt;span class=&#34;math inline&#34;&gt;\(n=50\)&lt;/span&gt; because the simulation was taking too long and I needed my laptop for other stuff). For simplicity I assumed that the different parameters were not correlated across observers&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. I also had my simulated observer using the same criterion across the two conditions, although this may not necessarily be true.
The quantity of interest here is the difference in slope between the two condition, that is &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2 - \sigma_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I examined the mean squared error of each of the models’ estimates, and of the model-averaged estimate. This is the average squared difference between the estimate and the true value.
&lt;img src=&#34;http://mlisi.xyz/img/mse.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt;
The results shows (unless my color blindness fooled me) that the model-averaged estimate attains always the smaller error. Note also that the error tend to decrease exponentially with the sample size. Interestingly, the worst model seems to be the one that allow for the lapses to vary across conditions. This may be because the change in the lapse rate across condition was - when present - relatively small, but also because this model has a larger number of parameters, and thus produces more variable estimates (that is with higher standard errors) than smaller model. Indeed, given that I know the ‘true’ value of the parameres in this simulation settings, I can divide the error into the two subcomponents of variance and bias (see &lt;a href=&#34;http://scott.fortmann-roe.com/docs/BiasVariance.html&#34;&gt;this page&lt;/a&gt; for a nice introduction to the bias-variance tradeoff). The bias is the difference between the expected estimate (averaged over many repetitions/iterations) of the same model and the true quantity that we want to estimate. The variance is simply the variability of the model estimates, i.e. how much they oscillate around the expected estimate.&lt;/p&gt;
&lt;p&gt;Here is a plot of the variance. Indeed it can be seen that the variable-lapse model, which has more parameters, is the one that produces more variable estimates. There is however little difference between the other two models’ and the multi-model estimates
&lt;img src=&#34;http://mlisi.xyz/img/variance.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is the bias. This is very satisfactory, as it shows that while all individual models produced biased estimates, the bias of the model-averaged estimates is zero, or very close to zero.
&lt;img src=&#34;http://mlisi.xyz/img/bias.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt;
In sum, by averaging models of different levels of complexity according to their relative likelihood, I was able to simultaneously minimize the variance and decrease the bias of my estimates, and achieve a greater efficiency. Model averaging seems to be the ideal procedure in this specific settings where the observer would belong to one of the three categories (i.e., she/he would conform to one of the three assumptions) with equal probability. However I think (although I haven’t checked) that it would perform well even in cases where a single “type” of observers is largely predominant over the other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;The (clumsy written) code for the simulations is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This load some handy functions that are required below
library(RCurl)
script &amp;lt;- getURL(&amp;quot;https://raw.githubusercontent.com/mattelisi/miscR/master/miscFunctions.R&amp;quot;, ssl.verifypeer = FALSE)
eval(parse(text = script))

set.seed(1)

# sim parameters
n_sim &amp;lt;- 100
sample_sizes &amp;lt;- seq(5, 100, 5)

# parameters
R &amp;lt;- 3 # range of signal levels (-R, R)
n_trial &amp;lt;- 500
mu_par &amp;lt;- c(0, 0.25) # population (mean, std.)
sigma_par &amp;lt;- c(1, 0.25)
sigmaDiff_par &amp;lt;- c(1, 0.5)
lapse_range &amp;lt;- c(0.01, 0.1)

# start
res &amp;lt;- {}
for(n_subjects in sample_sizes){
for(iteration in 1:n_sim){  
    
    # make dataset
    d &amp;lt;- {}
    for(i in 1:n_subjects){
        d_ &amp;lt;- data.frame(x=runif(n_trial)*2*R-R, 
                    condition=as.factor(rep(1:2,n_trial/2)), 
                    id=i, r=NA)

        r_i &amp;lt;- runif(1) # draw observer type (wrt lapses)

        if(r_i&amp;lt;1/3){
            # no lapses
            par1 &amp;lt;- c(rnorm(1,mu_par[1],mu_par[2]), 
                abs(rnorm(1,sigma_par[1],sigma_par[2])),
                0)
            par2 &amp;lt;- c(par1[1], 
                par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])),
                0) 

        }else if(r_i&amp;gt;=1/3 &amp;amp; r_i&amp;lt;2/3){
            # fixed lapses
            l_i &amp;lt;- runif(1)*diff(lapse_range) + lapse_range[1]
            par1 &amp;lt;- c(rnorm(1,mu_par[1],mu_par[2]), 
                abs(rnorm(1,sigma_par[1],sigma_par[2])),
                l_i)
            par2 &amp;lt;- c(par1[1], 
                par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])), 
                l_i) 

        }else{
            # varying lapses
            l_i_1 &amp;lt;- runif(1)*diff(lapse_range) + lapse_range[1]
            l_i_2 &amp;lt;- l_i_1 + (runif(1)*diff(lapse_range) + lapse_range[1])/2
            par1 &amp;lt;- c(rnorm(1,mu_par[1],mu_par[2]), 
                abs(rnorm(1,sigma_par[1],sigma_par[2])),
                l_i_1)
            par2 &amp;lt;- c(par1[1], 
                par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])), 
                l_i_2) 
        }

        ## simulate observer
        for(i in 1:sum(d_$condition==&amp;quot;1&amp;quot;)){
            d_$r[d_$condition==&amp;quot;1&amp;quot;][i] &amp;lt;- rbinom(1,1,
                psy_3par(d_$x[d_$condition==&amp;quot;1&amp;quot;][i],par1[1],par1[2],par1[3]))
        }
        for(i in 1:sum(d_$condition==&amp;quot;2&amp;quot;)){
            d_$r[d_$condition==&amp;quot;2&amp;quot;][i] &amp;lt;- rbinom(1,1,
                psy_3par(d_$x[d_$condition==&amp;quot;2&amp;quot;][i],par2[1],par2[2],par2[3]))
        }
        d &amp;lt;- rbind(d,d_)
    }
    
    
    ## model fitting

    # lapse assumed to be 0
    fit0 &amp;lt;- {}
    for(j in unique(d$id)){
        m0 &amp;lt;- glm(r~x*condition, family=binomial(probit),d[d$id==j,])
        sigma_1 &amp;lt;- 1/coef(m0)[2]
        sigma_2 &amp;lt;- 1/(coef(m0)[2] + coef(m0)[4])
        fit0 &amp;lt;- rbind(fit0, data.frame(id=j, sigma_1, sigma_2, 
                loglik=logLik(m0), aic=AIC(m0), model=&amp;quot;zero_lapse&amp;quot;) )
    }
    
    # fix lapse rate 
    start_p &amp;lt;- c(rep(c(0,1),2), 0)
    l_b &amp;lt;- c(rep(c(-5, 0.05),2), 0)
    u_b &amp;lt;- c(rep(c(5, 20), 2), 0.5)
    fit1 &amp;lt;- {}

    for(j in unique(d$id)){
        ftm &amp;lt;- optimx::optimx(par = start_p, lnorm_3par_multi , 
                d=d[d$id==j,],  method=&amp;quot;bobyqa&amp;quot;, 
                lower =l_b, upper =u_b)
        
        negloglik &amp;lt;- ftm$value
        aic &amp;lt;- 2*5 + 2*negloglik
        # fitted parameters are the first n numbers of optimx output
        sigma_1&amp;lt;-unlist(ftm [1,2])
        sigma_2&amp;lt;-unlist(ftm [1,4])
        fit1 &amp;lt;- rbind(fit1, data.frame(id=j, sigma_1, sigma_2, 
                loglik=-negloglik, aic, model=&amp;quot;fix_lapse&amp;quot;)  )
    }
    
    
    # varying lapse rate
    start_p &amp;lt;- c(0,1, 0)
    l_b &amp;lt;- c(-5, 0.05, 0)
    u_b &amp;lt;- c(5, 20, 0.5)
    fit2 &amp;lt;- {}
    for(j in unique(d$id)){
        # fit condition 1
        ftm &amp;lt;- optimx::optimx(par = start_p, lnorm_3par , 
                d=d[d$id==j &amp;amp; d$condition==&amp;quot;1&amp;quot;,],  
                method=&amp;quot;bobyqa&amp;quot;, lower =l_b, upper =u_b) 
        negloglik_1 &amp;lt;- ftm$value; sigma_1 &amp;lt;- unlist(ftm [1,2])
        # fit condition 2
        ftm &amp;lt;- optimx::optimx(par = start_p, lnorm_3par , 
                d=d[d$id==j &amp;amp; d$condition==&amp;quot;2&amp;quot;,],  
                method=&amp;quot;bobyqa&amp;quot;, lower =l_b, upper =u_b) 
        negloglik_2 &amp;lt;- ftm$value; sigma_2 &amp;lt;- unlist(ftm [1,2])

        aic &amp;lt;- 2*6 + 2*(negloglik_1 + negloglik_2)
        fit2 &amp;lt;- rbind(fit2, data.frame(id=j, sigma_1, sigma_2, 
                loglik=-negloglik_1-negloglik_2, aic, model=&amp;quot;var_lapse&amp;quot;))
    }
    
    # compute estimates of the change in slope
    effect_0 &amp;lt;- mean((fit0$sigma_2-fit0$sigma_1))
    effect_1 &amp;lt;- mean((fit1$sigma_2-fit1$sigma_1))
    effect_2 &amp;lt;- mean((fit2$sigma_2-fit2$sigma_1))
    
    effect_av &amp;lt;- {}
    for(j in unique(fit0$id)){
        dj &amp;lt;- rbind(fit0[fit0$id==j,], fit1[fit1$id==j,], fit2[fit2$id==j,])
        min_aic &amp;lt;- min(dj$aic)
        dj$delta &amp;lt;- dj$aic - min_aic
        den &amp;lt;- sum(exp(-0.5*c(dj$delta)))
        dj$w &amp;lt;- exp(-0.5*dj$delta) / den
        effect_av &amp;lt;- c(effect_av, sum((dj$sigma_2-dj$sigma_1) * dj$w))
    }
    effect_av &amp;lt;- mean(effect_av)
    
    # store results
    res &amp;lt;- rbind(res, data.frame(effect_0, effect_1, effect_2, 
            effect_av, effect_true=sigmaDiff_par[1], 
            n_subjects, n_trial, iteration))

}
}

## PLOT RESULTS
library(ggplot2)
library(reshape2)

res$err0 &amp;lt;- (res$effect_0 -1)^2
res$err1 &amp;lt;- (res$effect_1 -1)^2
res$err2 &amp;lt;- (res$effect_2 -1)^2
res$errav &amp;lt;- (res$effect_av -1)^2

# plot MSE
ares &amp;lt;- aggregate(cbind(err0,err1,err2,errav)~n_subjects, res, mean)
ares &amp;lt;- melt(ares, id.vars=c(&amp;quot;n_subjects&amp;quot;))
levels(ares$variable) &amp;lt;- c(&amp;quot;no lapses&amp;quot;, &amp;quot;fixed lapse rate&amp;quot;, &amp;quot;variable lapse rate&amp;quot;, &amp;quot;model averaged&amp;quot;)
ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_line(size=1)+nice_theme+scale_color_brewer(palette=&amp;quot;Dark2&amp;quot;,name=&amp;quot;model&amp;quot;)+labs(x=&amp;quot;number of subjects&amp;quot;,y=&amp;quot;mean squared error&amp;quot;)+geom_hline(yintercept=0,lty=2,size=0.2)

# plot variance
ares &amp;lt;- aggregate(cbind(effect_0,effect_1,effect_2,effect_av)~n_subjects, res, var)
ares &amp;lt;- melt(ares, id.vars=c(&amp;quot;n_subjects&amp;quot;))
levels(ares$variable) &amp;lt;- c(&amp;quot;no lapses&amp;quot;, &amp;quot;fixed lapse rate&amp;quot;, &amp;quot;variable lapse rate&amp;quot;, &amp;quot;model averaged&amp;quot;)
ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_line(size=1)+nice_theme+scale_color_brewer(palette=&amp;quot;Dark2&amp;quot;,name=&amp;quot;model&amp;quot;)+labs(x=&amp;quot;number of subjects&amp;quot;,y=&amp;quot;variance&amp;quot;)

# plot bias
ares &amp;lt;- aggregate(cbind(effect_0,effect_1,effect_2,effect_av)~n_subjects, res, mean)
ares &amp;lt;- melt(ares, id.vars=c(&amp;quot;n_subjects&amp;quot;))
levels(ares$variable) &amp;lt;- c(&amp;quot;no lapses&amp;quot;, &amp;quot;fixed lapse rate&amp;quot;, &amp;quot;variable lapse rate&amp;quot;, &amp;quot;model averaged&amp;quot;)
ares$value &amp;lt;- ares$value -1
ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_hline(yintercept=0,lty=2,size=0.2)+geom_line(size=1)+nice_theme+scale_color_brewer(palette=&amp;quot;Dark2&amp;quot;,name=&amp;quot;model&amp;quot;)+labs(x=&amp;quot;number of subjects&amp;quot;,y=&amp;quot;bias&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Burnham2002&#34;&gt;
&lt;p&gt;Burnham, Kenneth P., and David R. Anderson. 2002. &lt;em&gt;Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach&lt;/em&gt;. 2nd editio. New York, US: Springer New York. &lt;a href=&#34;https://doi.org/10.1007/b97636&#34;&gt;https://doi.org/10.1007/b97636&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wichmann2001&#34;&gt;
&lt;p&gt;Wichmann, F a, and N J Hill. 2001. “The psychometric function: I. Fitting, sampling, and goodness of fit.” &lt;em&gt;Perception &amp;amp; Psychophysics&lt;/em&gt; 63 (8): 1293–1313. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/11800458&#34;&gt;http://www.ncbi.nlm.nih.gov/pubmed/11800458&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The psychometric function is a statistical model that predicts the probabilities of the observer response (e.g. “stimulus A has a larger/smaller instensity than stimulus B”), conditional to the stimulus and the experimental condition.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;A good experimenter should do that (estimate the size of the difference). A “bad” experimenter might just be interested in obtaining &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;.05\)&lt;/span&gt;. See &lt;a href=&#34;http://cerco.ups-tlse.fr/-Charte-statistique-?lang=fr&#34;&gt;this page&lt;/a&gt;, compiled by Jean-Michel Hupé, for some references and guidelines against &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-hacking and the misuse of statistical tools in neuroscience.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The AIC of a model is computed as &lt;span class=&#34;math inline&#34;&gt;\({\rm{AIC}} = 2k - 2\log \left( \mathcal{L} \right)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of free parameters, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}\)&lt;/span&gt; is the maximum value of the likelihood function of that model.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Here it is a parameter common to all models. See the book of Burnham &amp;amp; Andersen for methods to methods to deal with different situations &lt;span class=&#34;citation&#34;&gt;(Burnham and Anderson 2002)&lt;/span&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Such correlation when present can be modelled using a mixed-effect approach. See my tutorial on mized-effects model in the&lt;a href=&#34;http://mattelisi.github.io/#notes&#34;&gt;‘misc’&lt;/a&gt; section of this website.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
