[{"authors":["admin"],"categories":null,"content":"¬†Hi, this is my attempt at blogging. I will use this space to post ideas and my random thoughts about science, statistics, politics, and share notes about what I am learning (see blog and misc). All views my own, except those I have nicked from cleverer people.\nI am a Lecturer at the Department of Psychology of the University of Essex. I use psychophysics, eye-tracking, and computational modelling to investigate visual perception, broadly defined as the ability to assimilate information contained in visible light. I am also interested in decision-making and cognitive processes, both in adults and during development.\nFind my publications on Scholar or RG. I started sharing all my data and code on GitHub and OSF.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://mlisi.xyz/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, this is my attempt at blogging. I will use this space to post ideas and my random thoughts about science, statistics, politics, and share notes about what I am learning (see blog and misc). All views my own, except those I have nicked from cleverer people.\nI am a Lecturer at the Department of Psychology of the University of Essex. I use psychophysics, eye-tracking, and computational modelling to investigate visual perception, broadly defined as the ability to assimilate information contained in visible light.","tags":null,"title":"Matteo Lisi","type":"authors"},{"authors":["Matteo Lisi"],"categories":["probability theory","interview question"],"content":" This question was posed during an interview for a AI / data science position for a global financial firm:\n Consider a game of chance in which a player can roll up to 3 times a dice. They win an amount of money proportional to the outcome of the last dice roll (1, 2, 3, 4, 5, or 6 ¬£). They don‚Äôt need to do all the 3 throws and can stop before and collect their win if they want. You are the house in this game, what is the minimum amount of ¬£ that you can charge for playing this game such that you won‚Äôt take losses in the long term.\n It is a subtle question and it was fun working through it.\n   Edit: click to see correct answer. \nMy previous answer was not entirely correct. Oliver Perkins pointed out that this can be calculated working backwards from the last throw:\n¬£4.65? As the punter on roml 3, E(¬£) = 3.5, so we stick on 4+ on throw 2. Knowing this our EV for the final 2 throws is is (0.53.5)+(0.55)=4.25. Therefore we stick on 5+ on the first throw so E(¬£) = (0.6674.25)+(0.3335.5) = ~4.64 ‚Äî Oli Perkins üî•üåçüè≥Ô∏è üåà (@OliPerkins2) September 27, 2020   His solution makes perfect sense and differs from my previous one in that the player would accept a 4 at throw 2. This small difference in the strategy it‚Äôs rational since it increases the overall expected value of the game by \\(\\approx\\) 0.03.\n    Click to see previous answer. \n The solution depends also on the strategy of the player. However, as the house you need to worry only about the rational players, not the irrational ones, for example those that keep playing after they scored a \\(6\\) (given that \\(6\\) is the maximum, if they continue they can only either do worse or keep the current score).\nLet‚Äôs define with \\(\\mathcal{C}\\) the price that the player pay to play the game, and with \\(\\mathcal{W}\\) the amount they win. A rational player would keep playing until their winning are at least equal to the price, i.e.¬†until \\(\\mathcal{W} \\ge \\mathcal{C}\\) (and may also continue after that if continuing increases their expected win). The question is then to find the expected value of \\(\\mathcal{W}\\) assuming a rational player.\n If \\(\\mathcal{C}=5.5\\) In this case the rational player should continue until they get a 6 (keeping aside that it may not be rational at all to play if \\(\\mathcal{C}=5.5\\)). Let‚Äôs define \\(D_1, D_2, D_3\\) indicate the outcomes of throws 1, 2, and 3, respectively. The probability \\(p\\left(\\mathcal{W} \\ge \\mathcal{C} \\right)\\) is equal to the probability of obtaining at least a \\(6\\) in three independent throws, that is\n\\[ p\\left(\\mathcal{W} \\ge \\mathcal{C} \\mid \\mathcal{C}=5.5\\right) = \\frac{1}{6} + \\underbrace{\\left( 1 - \\frac{1}{6} \\right) \\times \\frac{1}{6}}_{p \\left(D_2=6 \\mid D_1\u0026lt;6\\right)} + \\underbrace{\\left( 1 - \\frac{1}{6} \\right)^2 \\times \\frac{1}{6}}_{p \\left(D_3=6 \\mid D_1\u0026lt;6 \\cup D_2\u0026lt;6\\right)} = \\frac{91}{216} \\approx 0.42 \\]\nThus if \\(\\mathcal{C}=5.5\\) the house will alway win the long term, since the player will win more than they put in to play only about \\(42\\)% of the times\n  If \\(\\mathcal{C}=4.5\\) Note that I am using non-integer values of \\(\\mathcal{C}\\) to avoid considering (for the moment) the case in which house and player end up even (\\(\\mathcal{C} = \\mathcal{W}\\)). Let‚Äôs assume for the moment that the player stops if they get at least \\(5\\), we have\n\\[ p\\left(\\mathcal{W} \\ge \\mathcal{C} \\mid \\mathcal{C}= 4.5 \\right) = \\frac{2}{6} + \\underbrace{\\left( 1 - \\frac{2}{6} \\right) \\times \\frac{2}{6}}_{p \\left(D_2 \\ge5 \\mid D_1\u0026lt;5\\right)} + \\underbrace{\\left( 1 - \\frac{2}{6} \\right)^2 \\times \\frac{2}{6}}_{p \\left(D_3 \\ge 5 \\mid D_1\u0026lt;5 \\cup D_2\u0026lt;5\\right)} = \\frac{19}{27} \\approx 0.70 \\]\nIn this case thus the player will win more than they paid \\(70\\)% of the times - not a good deal for the house.\n Assume the player obtain 5 at the first throw. Is it worth to continue?\nThe probability of getting at least \\(5\\) in the next two throws is \\(\\frac{2}{6} + \\left(1 - \\frac{2}{6} \\right)\\times \\frac{2}{6} = \\frac{20}{36} \\approx 0.55\\)\nThe probability of getting a \\(6\\) in the next two throws is \\(\\frac{1}{6} + \\left(1 - \\frac{1}{6} \\right)\\times \\frac{1}{6} = \\frac{11}{36} \\approx 0.30\\)\nIf the player reach the last throw without obtaining at least \\(5\\) the remaining outcomes, \\(1\\) to \\(4\\), are equally likely with probability \\(\\frac{1}{4}\\). Thus continuing after obtaining \\(5\\) at the first throw has an expected value of \\[\\frac{11}{36}\\times6 + \\left(\\frac{20}{36} - \\frac{11}{36}\\right)\\times 5 + \\left(1 - \\frac{20}{36}\\right)\\times \\sum_{i=1}^4 \\frac{1}{4}i \\approx 4.19\\]\nThe value of stopping after having obtained \\(5\\) is \\(5\\), thus the rational player should stop and not continue.\n  Expected value of the bet What if \\(\\mathcal{C}=5\\)? In this case we have that \\(p\\left(\\mathcal{W} \u0026gt; 5 \\right) = \\frac{91}{216} \\approx 0.42\\), and also that \\(p\\left(\\mathcal{W} \u0026gt; 4 \\right) = \\frac{19}{27} \\approx 0.70\\). Thus in the long run the player will win more than they paid about \\(42\\)% of the times, they will be even with the bank \\(28\\)% of the times, and they will loose money (obtaining any number from \\(1\\) to \\(4\\), with equal probability) about \\(30\\)% of the times.\nTo see if this is a good deal we calculate the expected value assuming the player continue until they either obtain a \\(5\\) or complete 3 throws. The probability of getting at least 5 in three throws is \\(\\frac{19}{27}\\), and conditional of getting at least 5 the two outcomes of 6 and 5 have both the same probability \\(\\frac{1}{2}\\). Thus we have\n\\[\\frac{19}{27}\\times\\frac{1}{2}\\times6 + \\frac{19}{27}\\times\\frac{1}{2}\\times5 + \\left(1 - \\frac{19}{27}\\right)\\times \\sum_{i=1}^4 \\frac{1}{4}i = \\frac{83}{18} \\approx 4.61\\]\nThis indicate that for a price of \\(5\\) the player is expected to incur a loss in the long run (since \\(4.61 \u0026lt; 5\\)).\nNote that thus far we have constrained the value of the game to be somewhere between \\(4.5\\) and \\(5\\): if \\(\\mathcal{C}=4.5\\) the player will in the long run win more than they spend; and if \\(\\mathcal{C}=5\\) they will always lose. For any value of \\(\\mathcal{C}\\) within \\(4.5\\) and \\(5\\), the optimal strategy for the player is to continue until they either obtain a \\(5\\) or complete the 3 throws. And the calculation above gave the expected value for this strategy, that is \\(\\frac{83}{18} \\approx 4.61\\).\n Thus, the house needs to set a price \\(\\mathcal{C}\\) that is at least \\(\\frac{83}{18}\\approx 4.61\\) otherwise they risk incurring losses. \n  ","date":1601078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601145479,"objectID":"4e73b1c4c5f0b23dfa363ddc6ef38fe0","permalink":"http://mlisi.xyz/post/question-interview/","publishdate":"2020-09-26T00:00:00Z","relpermalink":"/post/question-interview/","section":"post","summary":"A probability question that was posed in an interview for a major financial firm.","tags":["probability theory","interview question"],"title":"How much is this game worth?","type":"post"},{"authors":null,"categories":["practical"],"content":" In this new covid-19 world it happens more and more often that I need to record full-screen videos, for example for lectures. This is something that one can do live with Zoom, but that is not the most practical option for non-live recordings.\nIn Ubuntu there is a nice software, Kazam screencaster, that is perfect for the job, except that it does not get correctly the screen size if you have a high pixel density display (HiDPI): you end up with a video with only the top-left corner of the screen cropped.\nThere is a simple patch to fix that issue, which I describe here in case it‚Äôs useful to someone else and for the benefit of my future self.\nFirst, you need to find the files gstreamer.py and prefs.py in the Kazam installation. For me they were in /usr/lib/python3/dist-packages/kazam/backend/.\nNext, you have to fix these such that they take into account the screen scaling factor, which is obtained from the get_monitor_scale_factor function in the Gtk library\nThis cane be done by adding these lines to the file gstreamer.py.\n scale = self.video_source[\u0026#39;scale\u0026#39;] startx = startx * scale starty = starty * scale endx = endx * scale endy = endy * scale  They should be added around lines 120 or so, right after the properties endx and endy are set up (endy = starty + height - 1).\nNext, open the file prefs.py and, around line 324, change this bit\nfor i in range(self.default_screen.get_n_monitors()): rect = self.default_screen.get_monitor_geometry(i) self.logger.debug(\u0026quot; Monitor {0} - X: {1}, Y: {2}, W: {3}, H: {4}\u0026quot;.format(i, rect.x, rect.y, rect.width, rect.height)) rect.height)) self.screens.append({\u0026quot;x\u0026quot;: rect.x, \u0026quot;y\u0026quot;: rect.y, \u0026quot;width\u0026quot;: rect.width, \u0026quot;height\u0026quot;: rect.height}) into this\nfor i in range(self.default_screen.get_n_monitors()): rect = self.default_screen.get_monitor_geometry(i) scale = self.default_screen.get_monitor_scale_factor(i) self.logger.debug(\u0026quot; Monitor {0} - X: {1}, Y: {2}, W: {3}, H: {4}, scale: {5}\u0026quot;.format(i, rect.x, rect.y, rect.width, rect.height, scale)) self.screens.append({\u0026quot;x\u0026quot;: rect.x, \u0026quot;y\u0026quot;: rect.y, \u0026quot;width\u0026quot;: rect.width, \u0026quot;height\u0026quot;: rect.height, \u0026quot;scale\u0026quot;: scale})  That‚Äôs it! Restart Kazam and the next fullscreen recording should work OK.\nThanks to user sllorente for describing the patch here!\n","date":1591833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591833600,"objectID":"6cc44c5ca9218d251321d8c4c08a7cab","permalink":"http://mlisi.xyz/post/kazam/","publishdate":"2020-06-11T00:00:00Z","relpermalink":"/post/kazam/","section":"post","summary":"In this new covid-19 world it happens more and more often that I need to record full-screen videos, for example for lectures. This is something that one can do live with Zoom, but that is not the most practical option for non-live recordings.\nIn Ubuntu there is a nice software, Kazam screencaster, that is perfect for the job, except that it does not get correctly the screen size if you have a high pixel density display (HiDPI): you end up with a video with only the top-left corner of the screen cropped.","tags":["Ubuntu"],"title":"Setting Kazam to correctly record full-screen on HiDPI displays in Ubuntu","type":"post"},{"authors":["Tessa Dekker","Matteo Lisi"],"categories":null,"content":"","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588550400,"objectID":"81f9c38e11d8a848048a9183799584e3","permalink":"http://mlisi.xyz/publication/baby_integration/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/publication/baby_integration/","section":"publication","summary":" ","tags":null,"title":"Sensory Development: Integration before Calibration","type":"publication"},{"authors":["Gerrit Maus","Hannah Goh","Matteo Lisi"],"categories":null,"content":"","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"bfa8b89ef3501660996a97d40c0f5375","permalink":"http://mlisi.xyz/publication/motion-blink-preprint/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/publication/motion-blink-preprint/","section":"publication","summary":"Eye blinks cause disruption of visual input that generally goes unnoticed. It is thought that the brain uses active suppression to prevent awareness of the gaps, but it is unclear how suppression would affect the perception of dynamic events, when visual input changes across the blink. Here we addressed this question by studying the perception of moving objects around eye blinks. In Experiment 1 we observed that when motion terminates during a blink, the last perceived position is extrapolated ahead. In Experiment 2 we found that motion trajectories were perceived as more continuous when the object jumped backward during the blink, cancelling a fraction of the space it travelled. This suggests subjective underestimation of blink duration. These results reveal the strategies used by the visual system to compensate for disruptions and maintain perceptual continuity: time elapsed during eye blinks is perceptually compressed and filled with extrapolated information.","tags":null,"title":"Perceiving location of moving objects across eye blinks","type":"publication"},{"authors":["Matteo Lisi","Michael J. Morgan","Joshua A. Solomon"],"categories":null,"content":"","date":1584230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584230400,"objectID":"ba444ddb2a6a872dc453d950aced95ac","permalink":"http://mlisi.xyz/publication/serial_int/","publishdate":"2020-03-15T00:00:00Z","relpermalink":"/publication/serial_int/","section":"publication","summary":"Perceptual decisions often require the integration of noisy sensory evidence over time. This process is formalized with sequential sampling models, where evidence is accumulated up to a decision threshold before a choice is made. Although classical accounts grounded in cognitive psychology tend to consider the process of decision formation and the preparation of the motor response as occurring serially, neurophysiological studies have proposed that decision formation and response preparation occur in parallel and are inseparable (Cisek, 2007; Shadlen et al., 2008). To address this serial vs. parallel debate, we developed a behavioural, reverse correlation protocol, in which the stimuli that influence perceptual decisions can be distinguished from the stimuli that influence motor responses. We show that the temporal integration windows supporting these two processes are distinct and largely non-overlapping, suggesting that they proceed in a serial or cascaded fashion.","tags":null,"title":"Serial integration of sensory evidence for perceptual decisions and oculomotor responses","type":"publication"},{"authors":["Matteo Lisi"],"categories":null,"content":"","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"d3a6b017b161b742b505b6b7d51cbc6f","permalink":"http://mlisi.xyz/publication/uncertaintyppc/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/publication/uncertaintyppc/","section":"publication","summary":" ","tags":null,"title":"Uncertainty and spatial updating in posterior parietal cortex","type":"publication"},{"authors":null,"categories":["R","statistic"],"content":" A friend of mine is working on a paper and found himself in the situation of having to defend the null hypothesis that a particular effect is absent (or not measurable) when tested under more controlled conditions than those used in previous studies. He asked for some practical advice: ‚Äúwhat would convince you as as a reviewer of a null result?‚Äù\nMy suggestions were:\n No statistical test can ‚Äúprove‚Äù a null results (intended as the point-null hypothesis that an effect of interest is zero). You can however: (i) present evidence that the data are more likely under the null hypothesis than under the alternative; or (ii) put a cap on the size of the effect, which could enable you to argue that any effect, if present, is so small that can be considered theoretically or pragmatically irrelevant.\n  (i) is the Bayesian approach and requires calculating a Bayes factor - that is the ratio between the average (or marginal) likelihood of the data under the null and alternative hypothesis. Note that Bayes factor calculation is highly influenced by the priors (e.g.¬†the prior expectations about the effect size). Luckily, in the case of a single comparison (e.g.¬†a t-test), there is popular way of computing Bayes factors which requires minimal assumptions about the effect of interest, as it is developed using uninformative or minimally informative priors, called the JZW prior (technically correspond to assuming a Cauchy prior on the standardized effect size and a uninformative Jeffrey‚Äôs prior on the variances of your measurements). It‚Äôs been derived in a paper by Rouder et al. (Rouder et al. 2009) and there is a easy-to-use R implementation of it in the package BayesFactor, (see function ttestBF()).\n  (ii) is the frequentist alternative. In a frequentist approach you don‚Äôt express belief in an hypothesis in terms of probability; uncertainty is characterized in relation to the data-generating process (e.g.¬†how many times you would reject the null if you repeated the experiment a zillion time? - probability is interpreted as the long-run frequency in an imaginary very, very large sample). Under this approach you can estimate what is the maximum size of the effect since you did not detected it in your current experiment. Daniel Lakens has written an easy-to-use package for that, called TOSTER; see this vignette for an introduction.\n Rouder, Jeffrey N, Paul L Speckman, Dongchu Sun, Richard D Morey, and Geoffrey Iverson. 2009. ‚ÄúBayesian t tests for accepting and rejecting the null hypothesis.‚Äù Psychonomic Bulletin \u0026amp; Review 16 (2): 225‚Äì37. https://doi.org/10.3758/PBR.16.2.225.\n  ","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"630f0faf41066c27633f25bdb241fbcb","permalink":"http://mlisi.xyz/post/defending-the-null/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/post/defending-the-null/","section":"post","summary":"A friend of mine is working on a paper and found himself in the situation of having to defend the null hypothesis that a particular effect is absent (or not measurable) when tested under more controlled conditions than those used in previous studies. He asked for some practical advice: ‚Äúwhat would convince you as as a reviewer of a null result?‚Äù\nMy suggestions were:\n No statistical test can ‚Äúprove‚Äù a null results (intended as the point-null hypothesis that an effect of interest is zero).","tags":["R","null-hypothesis","bayes factor"],"title":"Defending the null hypothesis","type":"post"},{"authors":["Matteo Lisi","Joshua A. Solomon","Michael J. Morgan"],"categories":null,"content":"","date":1566518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566518400,"objectID":"d7eb207f1c783fff5d3e9085ab7820e7","permalink":"http://mlisi.xyz/publication/gainpnas/","publishdate":"2019-08-23T00:00:00Z","relpermalink":"/publication/gainpnas/","section":"publication","summary":"Saccades are rapid eye movements that orient the visual axis toward objects of interest to allow their processing by the central, high-acuity retina. Our ability to collect visual information efficiently relies on saccadic accuracy, which is limited by a combination of uncertainty in the location of the target and motor noise. It has been observed that saccades have a systematic tendency to fall short of their intended targets, and it has been suggested that this bias originates from a cost function that overly penalizes hypermetric errors. Here, we tested this hypothesis by systematically manipulating the positional uncertainty of saccadic targets. We found that increasing uncertainty produced not only a larger spread of the saccadic endpoints but also more hypometric errors and a systematic bias toward the average of target locations in a given block, revealing that prior knowledge was integrated into saccadic planning. Moreover, by examining how variability and bias covaried across conditions, we estimated the asymmetry of the cost function and found that it was related to individual differences in the additional time needed to program secondary saccades for correcting hypermetric errors, relative to hypometric ones. Taken together, these findings reveal that the saccadic system uses a probabilistic-Bayesian control strategy to compensate for uncertainty in a statistically principled way and to minimize the expected cost of saccadic errors.","tags":null,"title":"Gain control of saccadic eye movements is probabilistic","type":"publication"},{"authors":null,"categories":["R","Stan"],"content":" This took me some time to make it work, so I‚Äôll write the details here for the benefit of my future self and anyone else facing similar issues.\nTo run R in the Apocrita cluster (which runs CentOS 7) first load the modules\nmodule load R module load gcc (gcc is required to compile the packages from source.)\nBefore starting you should make sure that you don‚Äôt have any previous installation of RStan in your system. From an R terminal, type:\nremove.packages(\u0026quot;rstan\u0026quot;) remove.packages(\u0026quot;StanHeaders\u0026quot;) if (file.exists(\u0026quot;.RData\u0026quot;)) file.remove(\u0026quot;.RData\u0026quot;) One problem that I had initially was (I think) due to the fact that Rcpp and rstan had been installed with different compiler or compilation flags. Thanks to the IT support at Queen Mary University, the correct C++ toolchain configuration that made the trick for me is the following:\nCXX14 = g++ -std=c++1y CXX14FLAGS = -O3 -Wno-unused-variable -Wno-unused-function -fPIC To write the correct configuration in the ~/.R/Makevars file from an R terminal:\ndotR \u0026lt;- file.path(Sys.getenv(\u0026quot;HOME\u0026quot;), \u0026quot;.R\u0026quot;) if (!file.exists(dotR)) dir.create(dotR) M \u0026lt;- file.path(dotR, \u0026quot;Makevars\u0026quot;) if (!file.exists(M)) file.create(M) cat(\u0026quot;\\nCXX14 = g++ -std=c++1y\u0026quot;, \u0026quot;CXX14FLAGS = -O3 -Wno-unused-variable -Wno-unused-function -fPIC\u0026quot;, file = M, sep = \u0026quot;\\n\u0026quot;, append = TRUE) Finally, install RStan:\nSys.setenv(MAKEFLAGS = \u0026quot;-j4\u0026quot;) # four cores used for building install install.packages(\u0026quot;rstan\u0026quot;, type = \u0026quot;source\u0026quot;) Note that in my case it worked correctly without requiring to run the instructions specific for CentOS 7.0 indicated at rstan installation page.\nAnother thing that I did, although I am not sure it is strictly necessary, was to install RStan on a new R library, that is in a directory that contained only packages necessary to run RStan.\n","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"382bf18fdd165683675fb3d3d60f3509","permalink":"http://mlisi.xyz/post/rstan-cluster/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/rstan-cluster/","section":"post","summary":"This took me some time to make it work, so I‚Äôll write the details here for the benefit of my future self and anyone else facing similar issues.\nTo run R in the Apocrita cluster (which runs CentOS 7) first load the modules\nmodule load R module load gcc (gcc is required to compile the packages from source.)\nBefore starting you should make sure that you don‚Äôt have any previous installation of RStan in your system.","tags":["R","Stan","CentOS","cluster"],"title":"Installing RStan on HPC cluster","type":"post"},{"authors":null,"categories":["politics"],"content":" The European Election results reveal a shift towards parties that support a soft or no Brexit compared to votes in 2014. Instead, major news outlets in the UK and Europe claim that Hard Brexit has gained support. To see why this is a misguided conclusion, just look at the numbers:\nAlmost complete vote share results of UK‚Äôs EU elections 2019, put in perspective.\n Parties supporting strong UK independence have lost overwhelming numbers of voters, with UKIP losing 24.2% of the votes, and conservative losing 14.8%. This loss is only partially recovered by Farage‚Äôs Brexit Party, which gained 31.6%, suggesting that a substantial proportion of voters switched to parties favoring a soft or no Brexit.\nAmongst parties favoring closer European connections, those vocally against Brexit show major wins (Lib Dem, 13.4%, Green 4.2%), whilst the Labor party without clear Brexit stance, has lost 11.3% of its votes.\nThe numerical shift towards parties against Brexit, is obscured by Farage‚Äôs clever rebranding of UKIP. The apparent overnight success of Farage‚Äôs new Brexit party, now the largest party around, is interpreted in major news outlets such as the Guardian and the French24, as a victory for hard Brexiteers. This conclusion overlooks that the vast majority of the gained votes are funneled directly from Farrage‚Äôs own former party UKIP. This rebranding allows the Farage to claim a victory of 28 new seats over 2014, instead of the correct increase of 5.\nDespite claims on the triumph of Farage‚Äôs party on the media, the numbers actually suggest a rising scepticism toward Brexit.\n(After making this plot we realized that the Guardian had written a perspective article on the same lines.).\nTessa Dekker \u0026amp; Matteo Lisi\n","date":1558915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558915200,"objectID":"d93d0c0d2b0af55d4d399524bc52ffea","permalink":"http://mlisi.xyz/post/eu/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/post/eu/","section":"post","summary":"The European Election results reveal a shift towards parties that support a soft or no Brexit compared to votes in 2014. Instead, major news outlets in the UK and Europe claim that Hard Brexit has gained support. To see why this is a misguided conclusion, just look at the numbers:\nAlmost complete vote share results of UK‚Äôs EU elections 2019, put in perspective.\n Parties supporting strong UK independence have lost overwhelming numbers of voters, with UKIP losing 24.","tags":["politics","brexit"],"title":"Attitude shift towards Remain in European Elections obscured in press by rebranded Farage party","type":"post"},{"authors":null,"categories":["books"],"content":" From ‚ÄòThe Big Picture: On the Origins of Life, Meaning, and the Universe Itself‚Äô by Sean Carrol, www.preposterousuniverse.com/bigpicture\n ","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"e448ed2f28f3f6f6e8540bd41e3080e0","permalink":"http://mlisi.xyz/post/ephemeral/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/post/ephemeral/","section":"post","summary":" From ‚ÄòThe Big Picture: On the Origins of Life, Meaning, and the Universe Itself‚Äô by Sean Carrol, www.preposterousuniverse.com/bigpicture\n ","tags":["books","philosophy"],"title":"Ephemeral patterns of complexity","type":"post"},{"authors":null,"categories":["R","tutorial"],"content":" In experimental psychology and neuroscience the classical approach when comparing different models that make quantitative predictions about the behavior of participants is to aggregate the predictive ability of the model (e.g.¬†as quantified by Akaike Information criterion) across participants, and then see which one provide on average the best performance. Although correct, this approach neglect the possibility that different participants might use different strategies that are best described by alternative, competing models. To account for this, Stephan et al. (Stephan et al. 2009) proposed a more conservative approach where models are treated as random effects that could differ between subjects and have a fixed (unknown) distribution in the population. The relevant statistical quantity is the frequency with which any model prevails in the population. Note that this is different from the definition of random-effects in classical statistic where random effects models have multiple sources of variation, e.g.¬†within- and between- subject variance. An useful and popular way to summarize the results of this analysis is by reporting the model‚Äôs exceedance probabilities, which measures how likely it is that any given model is more frequent than all other models in the set. The following exposition is largerly based on Stephan et al‚Äôs paper (Stephan et al. 2009).\nModel evidence Let‚Äôs say we have an experiment with \\(\\left(1,\\dots,N\\right)\\) participants. Their performance is quantitatively predicted by a set \\(\\left(1,\\dots,K\\right)\\) competing models. The behaviour of any subject \\(n\\) can be fit by the model \\(k\\) by finding the value(s) of the parameter(s) \\(\\theta_k\\) that maximize the likelihood of the data \\(y_n\\) under the model. In a fully Bayesian setting each unknown parameter would have a prior probability distribution, and the quantity of choice ofr comparing the goodness of fit of the model is the marginal likelihood, that is \\[ p \\left(y_n \\mid k \\right) = \\int p\\left(y_n \\mid k, \\theta_k \\right) \\, p\\left(\\theta_k \\right) d\\theta. \\] By integrating over the prior probability of parameters the marginal likelihood provide a measure of the evidence in favour of a specific model while taking into account the complexity of the model. We might also do something simpler and approximate the model evidence using e.g.¬†the Akaike information criterion.\n Models as random effects We are interested in finding which model does better at predicting behavior, however we allow for different participants to use different strategies which can be represented by different models. To achieve that we treat the model as random effects and we assume that the frequency or probability of models in the population, \\((r_1, \\dots, r_K)\\), is described by a Dirichlet distribution with parameters \\(\\boldsymbol{\\alpha } = \\alpha_1, \\dots, \\alpha_k\\), \\[ \\begin{align} p\\left(r \\mid \\boldsymbol{\\alpha } \\right) \u0026amp; = \\text{Dir} \\left(r, \\boldsymbol{ \\alpha } \\right) \\\\ \u0026amp; = \\frac{1}{\\mathbf{B} \\left(\\boldsymbol{ \\alpha } \\right)} \\prod_{i=1}^K r_i^{\\alpha_i -1} \\nonumber \\end{align}. \\] Where the normalizing constant \\(\\mathbf{B} \\left(\\boldsymbol{ \\alpha } \\right)\\) is the multivariate Beta function. The probabilities \\(r\\) generates ‚Äòswitches‚Äô or indicator variables \\(m_n = m_1, \\dots, m_N\\) where \\(m \\in \\left \\{ 0, 1\\right \\}\\) and \\(\\sum_1^K m_{nk}=1\\). These indicator variables prescribe the model for the subjects \\(n\\), $ p(m_{nk}=1)=r_k$. Given the probabilities \\(r\\), the indicator variables have thus a multinomial distribution, that is \\[ p\\left(m_n \\mid \\mathbf{r} \\right) = \\prod_{k=1}^K r_k^{m_{nk}}. \\] The graphical model that summarizes these dependencies is shown the following graph:\n  Variational Bayesian approach The goal is to estimate the parameters \\(\\boldsymbol{\\alpha}\\) that define the posterior distribution of model frequencies given the data, $ p ( r | y)$. To do so we need an estimate of the model evidence \\(p \\left(m_{nk}=1 \\mid y_n \\right)\\), that is the belief that the model \\(k\\) generated data from subject \\(m\\). There are many possible approach that can be used to estimate the model evidence, either exactly or approximately. Importantly, these would need to be normalized so that they sum to one across models, so that is one were using the Akaike Information criterion, this should be transformed into Akaike weights (Burnham and Anderson 2002).\nGenerative model Given the graphical model illustrated above, the joint probability of parameters and data can be expressed as \\[ \\begin{align} p \\left( y, r, m \\right) \u0026amp; = p \\left( y \\mid m \\right) \\, p \\left( m \\mid r \\right) \\, p \\left( r \\mid \\boldsymbol{\\alpha} \\right) \\\\ \u0026amp; = p \\left( r \\mid \\boldsymbol{\\alpha} \\right) \\left[ \\prod_{n=1}^N p \\left( y_n \\mid m_n \\right) \\, p\\left(m_n \\mid r \\right) \\right] \\nonumber \\\\ \u0026amp; = \\frac{1}{\\mathbf{B} \\left(\\boldsymbol{ \\alpha } \\right)} \\left[ \\prod_{k=1}^K r_k^{\\alpha_k -1} \\right] \\left[ \\prod_{n=1}^N p \\left( y_n \\mid m_n\\right) \\, \\prod_{k=1}^K r_k^{m_{nk}} \\right] \\nonumber \\\\ \u0026amp; = \\frac{1}{\\mathbf{B} \\left(\\boldsymbol{ \\alpha } \\right)} \\prod_{n=1}^N \\left[ \\prod_{k=1}^K \\left[ p \\left( y_n \\mid m_{nk} \\right) \\, r_k \\right]^{m_{nk}} \\, r_k^{\\alpha_k -1} \\right]. \\nonumber \\end{align} \\] And the log probability is \\[ \\log p \\left( y, r, m \\right) = - \\log \\mathbf{B} \\left(\\boldsymbol{ \\alpha } \\right) + \\sum_{n=1}^N \\sum_{k=1}^K \\left[ \\left(\\alpha_k -1 \\right) \\log r_k + m_{nk} \\left( p \\left( \\log y_n \\mid m_{nk} \\right) + \\log r_k\\right)\\right]. \\]\n Variational approximation In order to fit this hierarchical model following the variational approach one needs to define an approximate posterior distribution over model frequencies and assignments, \\(q\\left(r,m\\right)\\), which is assumed to be adequately described by a mean-field factorisation, that is \\(q\\left(r,m\\right) = q\\left(r\\right) \\, q\\left(m\\right)\\). The two densities are proportional to the exponentiated variational energies \\(I(m), I(r)\\), which are essentially the un-normalized approximated log-posterior densities, that is \\[ \\begin{align} q\\left(r\\right) \u0026amp; \\propto e^{I(r)}, \\, q\\left(m\\right)\\propto e^{I(m)} \\\\ I(r) \u0026amp; = \\left\u0026lt; \\log p \\left( y, r, m \\right) \\right\u0026gt;_{q(r)} \\\\ I(m) \u0026amp; = \\left\u0026lt; \\log p \\left( y, r, m \\right) \\right\u0026gt;_{q(m)} \\end{align} \\] For the approximate posterior over model assignment \\(q(m)\\) we first compute \\(I(m)\\) and then an appropriate normalization constant. From the expression above of the joint log-probability, and removing all the terms that do not depend on \\(m\\) we have that the un-normalized approximate log-posterior (the variational energy) can be expressed as \\[ \\begin{align} I(m) \u0026amp; = \\int p \\left( y, r, m \\right) \\, q(r) \\, dr \\\\ \u0026amp; = \\sum_{n=1}^N \\sum_{k=1}^K m_{nk} \\left[ p \\left( \\log y_n \\mid m_{nk} \\right) + \\int q(r_k) \\log r_k \\, d r_k \\right] \\nonumber \\\\ \u0026amp; = \\sum_{n=1}^N \\sum_{k=1}^K m_{nk} \\left[ p \\left( \\log y_n \\mid m_{nk} \\right) + \\psi (\\alpha_k) -\\psi \\left( \\alpha_S \\right) \\right] \\nonumber \\end{align} \\] where \\(\\alpha_S = \\sum_{k=1}^K \\alpha_k\\) and \\(\\psi\\) is the digamma function. If you wonder (as I did when reading this the first time) where the hell does the digamma function comes from here: well it is here due to a property of the Dirichlet distribution, which says that the expected value of \\(\\log r_k\\) can be computed as \\[ \\mathbb{E} \\left[\\log r_k \\right] = \\int p(r_k) \\log r_k \\, d r_k = \\psi (\\alpha_k) -\\psi \\left( \\sum_{k=1}^K \\alpha_k \\right) \\]\nFrom this, we have that the un-normalized posterior belief that model \\(k\\) generated data from subject \\(n\\) is \\[ u_{nk} = \\exp {\\left[ p \\left( \\log y_n \\mid m_{nk} \\right) + \\psi (\\alpha_k) -\\psi \\left( \\alpha_S \\right) \\right]} \\] and the normalized belief is \\[ g_{nk} = \\frac{u_{nk}}{\\sum_{k=1}^K u_{nk}} \\]\nWe need also to compute the approximate posterior density \\(q(r)\\), and we begin as above by computing the un-normalized, approximate log-posterior or variational energy \\[ \\begin{align} I(r) \u0026amp; = \\int p \\left( y, r, m \\right) \\, q(m) \\, dm \\\\ \u0026amp; = \\sum_{k=1}^K \\left[\\log r_k \\left(\\alpha_{0k} -1 \\right) + \\sum_{n=1}^N g_{nk} \\log r_k \\right] \\end{align} \\] The logarithm of a Dirichlet density is \\(\\log \\text{Dir} (r , \\boldsymbol{\\alpha}) = \\sum_{k=1}^K \\log r_k \\left(\\alpha_{0k} -1 \\right) + \\dots\\), therefore the parameters of the approximate posterior are \\[ \\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}_0 + \\sum_{n=1}^N g_{nk} \\]\n Iterative algorithm} The algorithm (Stephan et al. 2009) proceeds by estimating iteratively the posterior belief that a given model generated the data from a certain subject, by integrating out the prior probabilities of the models (the \\(r_k\\) predicted by the Dirichlet distribution that describes the frequency of models in the population) in log-space as described above. Next the parameters of the approximate Dirichlet posterior are updated, which gives new priors to integrate out from the model evidence, and so on until convergence.Convergence is assessed by keeping track of how much the vector $ $ change from one iteration to the next, i.e.¬†is common to consider that the procedure has converged when \\(\\left\\Vert \\boldsymbol{\\alpha}_{t-1} \\cdot \\boldsymbol{\\alpha}_t \\right\\Vert \u0026lt; 10^{-4}\\) (where \\(\\cdot\\) is the dot product).\n Exceedance probabilities After having found the optimised values of \\(\\boldsymbol{\\alpha}\\), one popular way to report the results and rank the models is by their exceedance probability, which is defined as the (second order) probability that participants were more likely to choose a certain model to generate behavior rather than any other alternative model, that is \\[ \\forall j \\in \\left\\{1, \\dots, K, j \\ne k \\right\\}, \\,\\,\\, \\varphi_k = p \\left(r_k \u0026gt; r_j \\mid y, \\boldsymbol{\\alpha} \\right). \\] In the case of \\(K\u0026gt;2\\) models, the exceedance probabilities \\(\\varphi_k\\) are computed by generating random samples from univariate Gamma densities and then normalizing. Specifically, each multivariate Dirichlet sample is composed of \\(K\\) independent random samples \\((x_1, \\dots, x_K)\\) distributed according to the density \\(\\text{Gamma}\\left(\\alpha_i, 1\\right) = \\frac{x_i^{\\alpha_i-1} e^{-x_i}}{\\Gamma(\\alpha_i)}\\), and then set normalize them by taking \\(z_i = \\frac{x_i}{ \\sum_{i=1}^K x_i}\\). The exceedance probability \\(\\varphi_k\\) for each model \\(k\\) is then computed as \\[ \\varphi_k = \\frac{\\sum \\mathop{\\bf{1}}_{z_k\u0026gt;z_j, \\forall j \\in \\left\\{1, \\dots, K, j \\ne k \\right\\} }}{ \\text{n. of samples}} \\] where \\(\\mathop{\\bf{1}}_{\\dots}\\) is the indicator function (\\(\\mathop{\\bf{1}}_{x\u0026gt;0} = 1\\) if \\(x\u0026gt;0\\) and \\(0\\) otherwise), summed over the total number of multivariate samples drawn.\n  Code! All this is already implementd in Matlab code in SPM 12. However, if you don‚Äôt like Matlab, I have translated it into R, and put it into a package on Github.\n References Burnham, Kenneth P., and David R. Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. 2nd editio. New York, US: Springer New York. https://doi.org/10.1007/b97636.\n Stephan, Klaas Enno, Will D. Penny, Jean Daunizeau, Rosalyn J. Moran, and Karl J. Friston. 2009. ‚ÄúBayesian model selection for group studies.‚Äù NeuroImage 46 (4). Elsevier Inc.: 1004‚Äì17. https://doi.org/10.1016/j.neuroimage.2009.03.025.\n   ","date":1548374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548374400,"objectID":"0e3053d0f76dab633e81aea149904112","permalink":"http://mlisi.xyz/post/bms/","publishdate":"2019-01-25T00:00:00Z","relpermalink":"/post/bms/","section":"post","summary":"In experimental psychology and neuroscience the classical approach when comparing different models that make quantitative predictions about the behavior of participants is to aggregate the predictive ability of the model (e.g.¬†as quantified by Akaike Information criterion) across participants, and then see which one provide on average the best performance. Although correct, this approach neglect the possibility that different participants might use different strategies that are best described by alternative, competing models.","tags":["R","Bayesian data analysis","model-selection"],"title":"Bayesian model selection at the group level","type":"post"},{"authors":null,"categories":["R","Stan","tutorial"],"content":" Photo ¬©Roxie and Lee Carroll, www.akidsphoto.com.\n In my previous lab I was known for promoting the use of multilevel, or mixed-effects model among my colleagues. (The slides on the /misc section of this website are part of this effort.) Multilevel models should be the standard approach in fields like experimental psychology and neuroscience, where the data is naturally grouped according to ‚Äúobservational units‚Äù, i.e.¬†individual participants. I agree with Richard McElreath when he writes that ‚Äúmultilevel regression deserves to be the default form of regression‚Äù (see here, section 1.3.2) and that, at least in our fields, studies not using a multilevel approach should justify the choice of not using it.\nIn \\(\\textsf{R}\\), the easiest way to fit multilevel linear and generalized-linear models is provided by the lme4 library (Bates et al. 2014). lme4 is a great package, which allows users to test different models very easily and painlessly. However it has also some limitations: it can be used to fit only classical forms of linear and generalized linear models, and can‚Äôt, for example, use to fit psychometric functions that take attention lapses into account (see here). Also, lme4 allows to fit multilevel models from a frequentist approach, and thus do not allow to incorporate prior knowledge into the model, or to use regularizing priors to reduce the risk of overfitting. For this reason, I have recently started using Stan, through its \\(\\textsf{R}\\)Stan interface, to fit multilevel models in a Bayesian settings, and I find it great! It certainly requires more effort to define the models, however I think that the flexibility offered by a software like Stan is well worth the time spent to learning how to use it.\nFor people like me, used to work with lme4, Stan can be a bit discouraing at first. The approach to write the model is quite different, and it requires specifying explicitly all the distributional assumptions. Also, implementing models with correlated random effects requires some specific notions of algebra. So I prepared a first tutorial showing how to analyse in Stan one of the most common introductory examples to mixed-effects models, the sleepstudy dataset (contained in the lme4 package). This will be followed by another tutorial showing how to use this approach to fit dataset where the dependent variable is a binary outcome, as it is the case for most psychophysical data.\nThe sleepstudy example This dataset contains part of the data from a published study (Belenky et al. 2003) that examined the effect of sleep deprivation on reaction times. (This is a sensible topic: think for example to long-distance truck drivers.) The dataset contains the average reaction times for the 18 subjects of the sleep-deprived group, for the first 10 days of the study, up to the recovery period.\nlibrary(lme4) Loading required package: Matrix str(sleepstudy) \u0026#39;data.frame\u0026#39;: 180 obs. of 3 variables: $ Reaction: num 250 259 251 321 357 ... $ Days : num 0 1 2 3 4 5 6 7 8 9 ... $ Subject : Factor w/ 18 levels \u0026quot;308\u0026quot;,\u0026quot;309\u0026quot;,\u0026quot;310\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The model I want to fit to the data will contain both random intercepts and slopes; in addition the correlation between the random effects should also be estimated. Using lme4, this model could be estimated by using\nlmer(Reaction ~ Days + (Days | Subject), sleepstudy) The model could be formally notated as \\[ y_{ij} = \\beta_0 + u_{0j} + \\left( \\beta_1 + u_{1j} \\right) \\cdot {\\rm{Days}} + e_i \\] where \\(\\beta_0\\) and \\(\\beta_1\\) are the fixed effects parameters (intercept and slope), \\(u_{0j}\\) and \\(u_{1j}\\) are the subject specific random intercept and slope (the index \\(j\\) denotes the subject), and \\(e \\sim\\cal N \\left( 0,\\sigma_e^2 \\right)\\) is the (normally distributed) residual error. The random effects \\(u_0\\) and \\(u_1\\) have a multivariate normal distribution, with mean 0 and covariance matrix \\(\\Omega\\) \\[ \\left[ {\\begin{array}{*{20}{c}} {{u_0}}\\\\ {{u_1}} \\end{array}} \\right] \\sim\\cal N \\left( {\\left[ {\\begin{array}{*{20}{c}} 0\\\\ 0 \\end{array}} \\right],\\Omega = \\left[ {\\begin{array}{*{20}{c}} {\\sigma _0^2}\u0026amp;{{\\mathop{\\rm cov}} \\left( {{u_0},{u_1}} \\right)}\\\\ {{\\mathop{\\rm cov}} \\left( {{u_0},{u_1}} \\right)}\u0026amp;{\\sigma _1^2} \\end{array}} \\right]} \\right) \\]\nIn Stan, fitting this model requires preparing a separate text file (usually saved with the ‚Äò.stan‚Äô extension), containing several ‚Äúblocks‚Äù. The 3 main types of blocks in Stan are:\n data all the dependent and independent variables needs to be declared in this blocks parameters here one should declare the free parameters of the model; what Stan do is essentially use a MCMC algorithm to draw samples from the posterior distribution of the parameters given the dataset model here one should define the likelihood function and, if used, the priors  Additionally, we will use two other types of blocks, transformed parameters and generated quantities. The first is necessary because we are estimating also the full correlation matrix of the random effects. We will parametrize the covariance matrix as the Cholesky factor of the correlation matrix (see my post on the Cholesky factorization), and in the transformed parameters block we will multiply the random effects with the Choleki factor, to transform them so that they have the intended correlation matrix. The generated quantities block can be used to compute any additional quantities we may want to compute once for each sample; I will use it to transform the Cholesky factor into the correlation matrix (this step is not essential but makes the examination of the model easier).\nData RStan requires the data to be organized in a list object. It can be done with the following command\nd_stan \u0026lt;- list(Subject = as.numeric(factor(sleepstudy$Subject, labels = 1:length(unique(sleepstudy$Subject)))), Days = sleepstudy$Days, RT = sleepstudy$Reaction/1000, N = nrow(sleepstudy), J = length(unique(sleepstudy$Subject))) Note that I also included two scalar variables, N and J, indicating respectively the number of observation and the number of subjects. Subject was a categorical factor, but to input it in Stan I transformed it into an integer index. I also rescaled the reaction times, so that they are in seconds instead of milliseconds.\nThese variables can be declared in Stan with the following block. We need to declare the variable type (e.g.¬†real or integer, similarly to programming languages as C++) and for vectors we need to declare the length of the vectors (hence the need of the two scalar variables N and J). Note that variables can be given lower and upper bounds. See the Stan reference manual for more information of the variable types.\ndata { int\u0026lt;lower=1\u0026gt; N; //number of observations real RT[N]; //reaction times int\u0026lt;lower=0,upper=9\u0026gt; Days[N]; //predictor (days of sleep deprivation) // grouping factor int\u0026lt;lower=1\u0026gt; J; //number of subjects int\u0026lt;lower=1,upper=J\u0026gt; Subject[N]; //subject id }  Parameters Here is the parameter block. Stan will draw samples from the posterior distribution of all the parameters listed here. Note that for parameters representing standard deviations is necessary to set the lower bound to 0 (variances and standard deviations cannot be negative). This is equivalent to estimating the logarithm of the standard deviation (which can be both positive or negative) and exponentiating before computing the likelihood (because \\(e^x\u0026gt;0\\) for any \\(x\\)). Note that we have also one parameter for the standard deviation of the residual errors (which was implicit in lme4). The random effects are parametrixed by a 2 x J random effect matrix z_u, and by the Cholesky factor of the correlation matrix L_u. I have added also the transformed parameters block, where the Cholesky factor is first multipled by the diagonal matrix formed by the vector of the random effect variances sigma_u, and then is multiplied with the random effect matrix, to obtain a random effects matrix with the intended correlations, which will be used in the model block below to compute the likelihood of the data.\nparameters { vector[2] beta; // fixed-effects parameters real\u0026lt;lower=0\u0026gt; sigma_e; // residual std vector\u0026lt;lower=0\u0026gt;[2] sigma_u; // random effects standard deviations // declare L_u to be the Choleski factor of a 2x2 correlation matrix cholesky_factor_corr[2] L_u; matrix[2,J] z_u; // random effect matrix } transformed parameters { // this transform random effects so that they have the correlation // matrix specified by the correlation matrix above matrix[2,J] u; u = diag_pre_multiply(sigma_u, L_u) * z_u; }  Model Finally the model block. Here we can define priors for the parameters, and then write the likelihood of the data given the parameters. The likelihood function corresponds to the model equation we saw before.\nmodel { real mu; // conditional mean of the dependent variable //priors L_u ~ lkj_corr_cholesky(1.5); // LKJ prior for the correlation matrix to_vector(z_u) ~ normal(0,2); sigma_e ~ normal(0, 5); // prior for residual standard deviation beta[1] ~ normal(0.3, 0.5); // prior for fixed-effect intercept beta[2] ~ normal(0.2, 2); // prior for fixed-effect slope //likelihood for (i in 1:N){ mu = beta[1] + u[1,Subject[i]] + (beta[2] + u[2,Subject[i]])*Days[i]; RT[i] ~ normal(mu, sigma_e); } } For the correlation matrix, Stan manual suggest to use a LKJ prior1. This prior has one single shape parameters, \\(\\eta\\): if you set \\(\\eta=1\\) then you have effectively a uniform prior distribution over any (Cholesky factor of) 2x2 correlation matrices. For values \\(\\eta\u0026gt;1\\) instead you get a more conservative prior, with a mode in the identity matrix (where the correlations are 0). For more information about the LKJ prior see page 556 of Stan reference manual, version 2.17.0, and also this page for an intuitive demonstration.\nImportantly, I have used (weakly) informative priors for the fixed effect estimates. We know from the literature that simple reaction times are around 300ms, hence the prior for the intercept, which represents the avearage reaction times at Day 0, i.e.¬†before the sleep deprivation. We expect the reaction times to increase with sleep deprivation, so I have used for the slope a Gaussian prior centered at a small positive value (0.2 seconds), which would represents the increase in reaction times with each day of sleep deprivation, however using a very broad standard deviation (2 seconds), which could accomodate also negative or very different slope values if needed. It may be useful to visualize with a plot the priors.  Generated quantities Finally, we can add one last block to the model file, to store for each sampling iteration the correlation matrix of the random effect, which can be computed multyplying the Cholesky factor with its transpose.\ngenerated quantities { matrix[2, 2] Omega; Omega = L_u * L_u\u0026#39;; // so that it return the correlation matrix }   Estimating the model Having written all the above blocks in a separate text file (I called it ‚Äúsleep_model.stan‚Äù), we can call Stan from R with following commands. I run 4 independent chains (each chain is a stochastic process which sequentially generate random values; they are called chain because each sample depends on the previous one), each for 2000 samples. The first 1000 samples are the warmup (or sometimes called burn-in), which are intended to allow the sampling process to settle into the posterior distribution; these samples will not be used for inference. Each chain is independent from the others, therefore having multiple chains is also useful to check the convergence (i.e.¬†by looking if all chains converged to the same regions of the parameter space). Additionally, having multiple chain allows to compute a statistic which is also used to check convergence: this is called \\(\\hat R\\) and it corresponds to the ratio of the between-chain variance and the within-chain variance. If the sampling has converged then \\({\\hat R} \\approx 1 \\pm 0.01\\). When we call function stan, it will compile a C++ program which produces samples from the joint posterior of the parameter using a powerful variant of MCMC sampling, called Hamiltomian Monte Carlo (see here for an intuitive explanation of the sampling algorithm).\nlibrary(rstan) options(mc.cores = parallel::detectCores()) # indicate stan to use multiple cores if available sleep_model \u0026lt;- stan(file = \u0026quot;sleep_model.stan\u0026quot;, data = d_stan, iter = 2000, chains = 4) One way to check the convergence of the model is to plot the chain of samples. They should look like a ‚Äúfat, hairy caterpillar which does not bend‚Äù (Sorensen, Hohenstein, and Vasishth 2016), suggesting that the sampling was stable at the posterior.\ntraceplot(sleep_model, pars = c(\u0026quot;beta\u0026quot;), inc_warmup = FALSE) There is a print() method for visualising the estimates of the parameters. The values of the \\({\\hat R}\\) (Rhat) statistics also confirm that the chains converged. The method automatically report credible intervals for the parameters (computed with the percentile method from the samples of the posterior distribution).\nprint(sleep_model, pars = c(\u0026quot;beta\u0026quot;), probs = c(0.025, 0.975), digits = 3) Inference for Stan model: sleep_model_v1. 5 chains, each with iter=6000; warmup=3000; thin=1; post-warmup draws per chain=3000, total post-warmup draws=15000. mean se_mean sd 2.5% 97.5% n_eff Rhat beta[1] 0.255 0 0.006 0.243 0.268 6826 1 beta[2] 0.011 0 0.001 0.008 0.013 7830 1 Samples were drawn using NUTS(diag_e) at Sat Sep 22 17:15:42 2018. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). And we can visualze the posterior distribution as histograms (here for the fixed effects parameters and the standard deviations of the corresponding random effects).\nplot(sleep_model, plotfun = \u0026quot;hist\u0026quot;, pars = c(\u0026quot;beta\u0026quot;, \u0026quot;sigma_u\u0026quot;)) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Finally, we can also examine the correlation matrix of random-effects.\nprint(sleep_model, pars = c(\u0026quot;Omega\u0026quot;), digits = 3) Inference for Stan model: sleep_model_v1. 5 chains, each with iter=6000; warmup=3000; thin=1; post-warmup draws per chain=3000, total post-warmup draws=15000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat Omega[1,1] 1.000 NaN 0.000 1.000 1.000 1.000 1.000 1.000 NaN NaN Omega[1,2] 0.221 0.007 0.344 -0.546 0.011 0.251 0.467 0.807 2228 1.001 Omega[2,1] 0.221 0.007 0.344 -0.546 0.011 0.251 0.467 0.807 2228 1.001 Omega[2,2] 1.000 0.000 0.000 1.000 1.000 1.000 1.000 1.000 160 1.000 Samples were drawn using NUTS(diag_e) at Sat Sep 22 17:15:42 2018. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). The Rhat values for the first entry of the correlation matrix is NaN. This is expected for variables that remain constant during samples. We can check that this variable resulted in a series of identical values during sampling with the following command\nall(unlist(extract(sleep_model, pars = \u0026quot;Omega[1,1]\u0026quot;)) == 1) # all values are =1 ? [1] TRUE That‚Äôs all! You can check by yourself that the values are the sufficiently similar to what we would obtain using lmer, and eventually experiment by yourself how the estimates changes when more informative priors are used. For more examples on how to fit linear mixed-effects models using Stan I recommend the article by Sorensen (Sorensen, Hohenstein, and Vasishth 2016), which also show how to implement crossed random effects of subjects and item (words), as it is conventional in linguistics.\n References Bates, D, M Maechler, B Bolker, and S Walker. 2014. ‚Äúlme4: Linear mixed-effects models using Eigen and S4.‚Äù R package version 1.1-7. http://cran.r-project.org/package=lme4.\n Belenky, Gregory, Nancy J Wesensten, David R Thorne, Maria L Thomas, Helen C Sing, Daniel P Redmond, Michael B Russo, and J Balkin, Thomas. 2003. ‚ÄúPatterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study.‚Äù Journal of Sleep Research 12 (1): 1‚Äì12. https://doi.org/10.1046/j.1365-2869.2003.00337.x.\n Sorensen, Tanner, Sven Hohenstein, and Shravan Vasishth. 2016. ‚ÄúBayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists.‚Äù The Quantitative Methods for Psychology 12 (3): 175‚Äì200. https://doi.org/10.20982/tqmp.12.3.p175.\n    The LKJ prior is named after the authors, see: Lewandowski, D., Kurowicka, D., and Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. Journal of Multivariate Analysis, 100:1989‚Äì2001‚Ü©\n   ","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"31440bf4831077fc05cc8ba78543ccac","permalink":"http://mlisi.xyz/post/bayesian-multilevel-models-r-stan/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/post/bayesian-multilevel-models-r-stan/","section":"post","summary":"Photo ¬©Roxie and Lee Carroll, www.akidsphoto.com.\n In my previous lab I was known for promoting the use of multilevel, or mixed-effects model among my colleagues. (The slides on the /misc section of this website are part of this effort.) Multilevel models should be the standard approach in fields like experimental psychology and neuroscience, where the data is naturally grouped according to ‚Äúobservational units‚Äù, i.e.¬†individual participants. I agree with Richard McElreath when he writes that ‚Äúmultilevel regression deserves to be the default form of regression‚Äù (see here, section 1.","tags":["R","Bayesian data analysis","multilevel models","mixed-effects"],"title":"Bayesian multilevel models using R and Stan (part 1)","type":"post"},{"authors":["Mario Bonato","Matteo Lisi","Sara Pegoraro","Gilles Pourtois"],"categories":null,"content":"Mario Bonato and Matteo Lisi contributed equally to the article and are listed in alphabetical order.\n","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"26774999b660aa3e35094819e944657a","permalink":"http://mlisi.xyz/publication/validity_psyres/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/validity_psyres/","section":"publication","summary":"Voluntary orienting of spatial attention is typically investigated by visually presented directional cues, which are called predictive when they indicate where the target is more likely to appear. In this study, we investigated the nature of the potential link between cue predictivity (the proportion of valid trials) and the strength of the resulting covert orienting of attention. Participants judged the orientation of a unilateral Gabor grating preceded by a centrally presented, non-directional, color cue, arbitrarily prompting a leftwards or rightwards shift of attention. Unknown to them, cue predictivity was manipulated across blocks, whereby the cue was only predictive for either the first or the second half of the experiment. Our results show that the cueing effects were strongly influenced by the change in predictivity. This influence differently emerged in response speed and accuracy. The speed difference between valid and invalid trials was significantly larger when cues were predictive, and the amplitude of this effect was modulated at the single trial level by the recent trial history. Complementary to these findings, accuracy revealed a robust effect of block history and also a different time-course compared with speed, as if it mainly mirrored voluntary processes. These findings, obtained with a new manipulation and using arbitrary non-directional cueing, demonstrate that cue-target contingencies strongly modulate the way attention is deployed in space.","tags":null,"title":"Cue-target contingencies modulate voluntary orienting of spatial attention: dissociable effects for speed and accuracy","type":"publication"},{"authors":["Harry Haladjian","Matteo Lisi","Patrick Cavanagh"],"categories":null,"content":"","date":1517702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517702400,"objectID":"6a213e7fb5c75dd2e7eadb70d1e6a203","permalink":"http://mlisi.xyz/publication/dd_load/","publishdate":"2018-02-04T00:00:00Z","relpermalink":"/publication/dd_load/","section":"publication","summary":"The double-drift stimulus produces a strong shift in apparent motion direction that generates large errors of perceived position. In this study, we tested the effect of attentional load on the perceptual estimates of motion direction and position for double-drift stimuli. In each trial, four objects appeared, one in each quadrant of a large screen, and they moved upward or downward on an angled trajectory. The target object whose direction or position was to be judged was either cued with a small arrow prior to object motion (low attentional load condition), or cued after the objects stopped moving and disappeared (high attentional load condition). In Experiment 1, these objects appeared 10 deg. from the central fixation and participants reported the perceived direction of the target's trajectory after the stimulus disappeared by adjusting the direction of an arrow at the center of the response screen. In Experiment 2, the four double-drift objects could appear between 6-14 deg. from the central fixation and participants reported the location of the target object after its disappearance by adjusting the position of a small circle on the response screen. The errors in direction and position judgments showed little effect of the attentional manipulation-similar errors were seen in both experiments whether or not the participant knew which double-drift object would be tested. This suggests that orienting endogenous attention (i.e., by only attending to one object in the pre-cued trials) does not interact with the strength of the motion or position shifts for the double-drift stimulus.","tags":null,"title":"Motion and position shifts induced by the double-drift stimulus are unaffected by attentional load","type":"publication"},{"authors":null,"categories":["R","self-study","note"],"content":" Generating random variables with given variance-covariance matrix can be useful for many purposes. For example it is useful for generating random intercepts and slopes with given correlations when simulating a multilevel, or mixed-effects, model (e.g.¬†see here). This can be achieved efficiently with the Choleski factorization. In linear algebra the factorization or decomposition of a matrix is the factorization of a matrix into a product of matrices. More specifically, the Choleski factorization is a decomposition of a positive-defined, symmetric1 matrix into a product of a triangular matrix and its conjugate transpose; in other words is a method to find the square root of a matrix. The square root of a matrix \\(C\\) is another matrix \\(L\\) such that \\({L^T}L = C\\).2\nSuppose you want to create 2 variables, having a Gaussian distribution, and a positive correlation, say \\(0.7\\). The first step is to define the correlation matrix \\[C = \\left( {\\begin{array}{*{20}{c}} 1\u0026amp;{0.7}\\\\ {0.7}\u0026amp;1 \\end{array}} \\right)\\] Elements in the diagonal can be understood as the correlation of each variable with itself, and therefore are 1, while elements outside the diagonal indicate the desired correlation. In \\(\\textsf{R}\\)\nC \u0026lt;- matrix(c(1,0.7,0.7,1),2,2) Next one can use the chol() function to compute the Cholesky factor. (The function provides the upper triangular square root of \\(C\\)).\nL \u0026lt;- chol(C) If you multiply the matrix \\(L\\) with itself you get back the original correlation matrix (\\(\\textsf{R}\\) output below).\nt(L) %*% L [,1] [,2] [1,] 1.0 0.7 [2,] 0.7 1.0 Then we need another matrix with the desired standard deviation in the diagonal (in this example I choose 1 and 2)\ntau \u0026lt;- diag(c(1,2)) Multiply that matrix with the lower triangular square root of the correlation matrix (can be obtained by taking the transpose of \\(L\\))\nLambda \u0026lt;- tau %*% t(L) Now we can generate values for 2 independent random variables \\(z\\sim\\cal N\\left( {0,1} \\right)\\)\nZ \u0026lt;- rbind(rnorm(1e4),rnorm(1e4)) Finally, to introduce the correlations , multiply them with the Lambda obtained above\nX \u0026lt;- Lambda %*% Z Now plot the results We can verify that the correlation as estimated from the sample corresponds (or is close enough) to the generative value.\n# correlation in the generated sample cor(X[1,],X[2,]) [1] 0.7093591 Why does it work? The covariance matrix of the initial, uncorrelated sample is \\(\\mathbb{E} \\left( Z Z^T \\right) = I\\), that is the identity matrix, since they have zero mean and unit variance \\(z\\sim\\cal N\\left( {0,1} \\right)\\)3.\nLet‚Äôs suppose that the desired covariance matrix is \\(\\Sigma\\); since it is symmetric and positive defined it is possible to obtain the Cholesky factorization \\(L{L^T} = \\Sigma\\).\nIf we then compute a new random vector as \\(X=LZ\\), we have that its covariance matrix is \\[ \\begin{align} \\mathbb{E} \\left(XX^T\\right) \u0026amp;= \\mathbb{E} \\left((LZ)(LZ)^T \\right) \\\\ \u0026amp;= \\mathbb{E} \\left(LZ Z^T L^T\\right) \\\\ \u0026amp;= L \\mathbb{E} \\left(ZZ^T \\right) L^T \\\\ \u0026amp;= LIL^T = LL^T = \\Sigma \\\\ \\end{align} \\] Therefore the new random vector \\(X\\) has the covariance matrix \\(\\Sigma\\).\nThe third step is justified because the expected value is a linear operator, therefore \\(\\mathbb{E}(cX) = c\\mathbb{E}(X)\\). Also \\((AB)^T = B^T A^T\\), note that the order of the factor reverses.\n  Actually Choleski factorization can be obtained from all Hermitian matrices. Hermitian matrices are a complex extension of real symmetric matrices. A symmetric matrices is one that it is equal to its transpose, which implies that its entries are symmetric with respect to the diagonal. In a Hermitian matrix, symmetric entries with respect to the diagonal are complex conjugates, i.e.¬†they have the same real part, and an imaginary part with equal magnitude but opposite in sign. For example, the complex conjugate of \\(x+iy\\) is \\(x-iy\\) (or, equivalently, \\(re^{i\\theta}\\) and \\(re^{-i\\theta}\\)). Real symmetric matrices can be considered a special case of Hermitian matrices where the imaginary component \\(y\\) (or \\(\\theta\\)) is \\(0\\).‚Ü©\n Note that I am using the convention of \\(\\textsf{R}\\) software, where the function chol(), which compute the factorization, returns the upper triangular factor of the Choleski decomposition. I think that is more commonly assumed that the Choleski decomposition returns the lower triangular factor \\(L\\), in which case \\(L{L^T} = C\\).‚Ü©\n More generally the variance-covariance matrix is \\(\\Sigma = \\mathbb{E}\\left( {X{X^T}} \\right) - \\mathbb{E}\\left( X \\right) \\mathbb{E}\\left(X \\right)^T\\). \\(\\mathbb{E}\\) indicates the expected value.‚Ü©\n   ","date":1516492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516492800,"objectID":"bec54247054d3b9ddcf642807e1beaae","permalink":"http://mlisi.xyz/post/simulating-correlated-variables-with-the-cholesky-factorization/","publishdate":"2018-01-21T00:00:00Z","relpermalink":"/post/simulating-correlated-variables-with-the-cholesky-factorization/","section":"post","summary":"Generating random variables with given variance-covariance matrix can be useful for many purposes. For example it is useful for generating random intercepts and slopes with given correlations when simulating a multilevel, or mixed-effects, model (e.g.¬†see here). This can be achieved efficiently with the Choleski factorization. In linear algebra the factorization or decomposition of a matrix is the factorization of a matrix into a product of matrices. More specifically, the Choleski factorization is a decomposition of a positive-defined, symmetric1 matrix into a product of a triangular matrix and its conjugate transpose; in other words is a method to find the square root of a matrix.","tags":["R","Cholesky","simulation"],"title":"Simulating correlated variables with the Cholesky factorization","type":"post"},{"authors":null,"categories":["R","data-analysis","psychophysics"],"content":" In the study of human perception we often need to measure how sensitive is an observer to a stimulus variation, and how her/his sensitivity changes due to changes in the context or experimental manipulations. In many applications this can be done by estimating the slope of the psychometric function1, a parameter that relates to the precision with which the observer can make judgements about the stimulus. A psychometric function is generally characterized by 2-3 parameters: the slope, the threshold (or criterion), and an optional lapse parameter, which indicate the rate at which attention lapses (i.e. stimulus-independent errors) occur.\nAs an example, consider the situation where an observer is asked to judge whether a signal (can be anything, from the orientation angle of a line on a screen, or the pitch of a tone, to the speed of a car or the approximate number of people in a crowd, etc.) is above or below a given reference value, call it zero. The experimenter presents the observers with many signals of different intensities, and the observer is asked to respond by making a binary choice (larger/smaller than the reference), under two different contextual conditions (before/after having a pint, with different headphones, etc.). These two conditions are expected to results in different sensitivity, and the experimenter is interested in estimating as precisely as possible the difference in sensitivity2. The psychometric function for one observer in the two conditions might look like this (figure below).\nPsychometric functions. Each points is a response (0 or 1 ; some vertical jitter is added for clarity), and the lines represent the fitted psychometric model (here a cumulative Gaussian psychometric function). The two facets of the plots represent the two different conditions. It can be seen that the precision seems to be different across conditions: judgements made under condition ‚Äò2‚Äô are more variable, indicating reduced sensitivity.  Our focus is on the psychometric slope, and we are not really interested in measuring the lapse rate; however it is still important to take lapses into account: it has been shown that not accounting for lapses can have a large influence on the estimates of the slope (Wichmann and Hill 2001).\nThe problem with lapses Different observer may lapse at quite different rates, and for some of them the lapse rate is probably so small that can be considered negligible. Also, we usually don‚Äôt have hypothesis about lapses, and about whether they should or should not vary across conditions. We can base our analysis on different assumptions about when the observers may have attention lapses:\nthey may never lapse (or they do so with a small, negligible frequency); they may lapse at a fairly large rate, but the rate is assumed constant across conditions (reasonable, especially if conditions are randomly interleaved); they may lapse with variable rate across conditions.  These assumptions will lead to three different psychometric models. The number can increase if we consider also different functional forms of the relationship between stimulus and choice; here for simplicity I will consider only psychometric models based on the cumulative Gaussian function (equivalent to a probit analysis), \\(\\Phi (\\frac{x-\\mu}{\\sigma}) = \\frac{1}{2}\\left[ {1 + {\\rm{erf}}\\left( {\\frac{{x - \\mu }}{{\\sigma \\sqrt 2 }}} \\right)} \\right]\\), where the mean \\(\\mu\\) woud correspond to the threshold parameter, \\(\\sigma\\) to the slope, and \\(x\\) is the stimulus intensity. In our case the first assumption (zero lapses) would lead to the simplest psychometric model \\[ \\Psi (x, \\mu_i, \\sigma_i)= \\Phi (\\frac{x-\\mu_i}{\\sigma_i}) \\] where the subscript \\(i\\) indicates that the values of both mean \\(\\mu_i\\) and slope \\(\\sigma_i\\) are specific to the condition \\(i\\). The second assumption (fixed lapse rate) could correspond to the model \\[ \\Psi (x, \\mu_i, \\sigma_i, \\lambda)= \\lambda + (1-2\\lambda) \\Phi (\\frac{x-\\mu_i}{\\sigma_i}) \\] where the parameter \\(\\lambda\\) correspond to the probability of the observer making a random error. Note that this is assumed to be fixed with respect to the condition (no subscript). Finally the last assumption (variable lapse rate) would suggests the model \\[ \\Psi (x, \\mu_i, \\sigma_i, \\lambda_i)= \\lambda_i + (1-2\\lambda_i) \\Phi (\\frac{x-\\mu_i}{\\sigma_i}) \\] where all the parameters are allowed to vary between conditions.\nWe have thus three different models, but we haven‚Äôt any prior information to decide which model is more likely to be correct in our case. Also, we acknowledge the fact that there are individual differences and each observer in our sample may conform to one of the three assumptions with equal probability. Hence, ideally, we would like to find a way to deal with lapses - and find the best estimates of the slope values \\(\\sigma_i\\) without committing to one of the three models.\n Multi-model inference One possible solution to this problem is provided by a multi-model, or model averaging, approach (Burnham and Anderson 2002). This requires calculating the AIC (Akaike Information Criterion)3 for each model and subjects, and then combine the estimates according to the Akaike weights of each model. To compute the Akaike weights one typically proceed by first transforming them into differences with respect to the AIC of the best candidate model (i.e.¬†the one with lower AIC) \\[ {\\Delta _m} = {\\rm{AI}}{{\\rm{C}}_m} - \\min {\\rm{AIC}} \\] From the differences in AIC, we can obtain an estimate of the relative likelihood of the model \\(m\\) given the data \\[ \\mathcal{L} \\left( {m|{\\rm{data}}} \\right) \\propto \\exp \\left( { - \\frac{1}{2}{\\Delta _m}} \\right) \\] Then, to obtain the Akaike weight \\(w_m\\) of the model \\(m\\), the relative likelihoods are normalized (divided by their sum) \\[ {w_m} = \\frac{{\\exp \\left( { - \\frac{1}{2}{\\Delta _m}} \\right)}}{{\\mathop \\sum \\limits_{k = 1}^K \\exp \\left( { - \\frac{1}{2}{\\Delta _k}} \\right)}} \\] Finally, one can compute the model-averaged estimate of the parameter4, \\(\\hat {\\bar \\sigma}\\), by combining the estimate of each model according to their Akaike weight \\[ \\hat {\\bar \\sigma} = \\sum\\limits_{k = 1}^K {{w_k}\\hat \\sigma_k } \\]\n Simulation results Model averaging seems a sensitive approach to deal with the uncertainty about which form of the model is best suited to our data. To see whether it is worth doing the extra work of fitting 3 models instead of just one, I run a simulation, where I repeatedly fit and compare the estimates of the three models, with the model-averaged estimate, for different values of sample sizes. In all the simulations, each observer is generated by randomly drawing parameters from a Gaussian distribution which summarize the distribution of the parameters in the population. Hence, I know the true difference in sensitivity in the population, and by simulating and fitting the models I can test which estimating procedure is more efficient. In statistics a procedure or an estimator is said to be more efficient than another one when it provides a better estimate with the same number or fewer observations. The notion of ‚Äúbetter‚Äù clearly relies on the choice of a cost function, which for example can be the mean squared error (it is here).\nAdditionally, in my simulations each simulated observer could, with equal probability \\(\\frac{1}{3}\\), either never lapse, lapse with a constant rate across conditions, or lapse at a higher rate in the more difficult condition (condition ‚Äò2‚Äô where the judgements are less precise). The lapse rates were draw uniformly from the interval [0.01, 0.1], and could get as high as 0.15 in condition ‚Äò2‚Äô. Each simulated observer ran 250 trials per condition (similar to the figure at the top of this page). I simulated dataset from \\(n=5\\) to \\(n=50\\), using 100 iterations for each sample size (only 85 in the case of \\(n=50\\) because the simulation was taking too long and I needed my laptop for other stuff). For simplicity I assumed that the different parameters were not correlated across observers5. I also had my simulated observer using the same criterion across the two conditions, although this may not necessarily be true. The quantity of interest here is the difference in slope between the two condition, that is \\(\\sigma_2 - \\sigma_1\\).\nFirst, I examined the mean squared error of each of the models‚Äô estimates, and of the model-averaged estimate. This is the average squared difference between the estimate and the true value. The results shows (unless my color blindness fooled me) that the model-averaged estimate attains always the smaller error. Note also that the error tend to decrease exponentially with the sample size. Interestingly, the worst model seems to be the one that allow for the lapses to vary across conditions. This may be because the change in the lapse rate across condition was - when present - relatively small, but also because this model has a larger number of parameters, and thus produces more variable estimates (that is with higher standard errors) than smaller model. Indeed, given that I know the ‚Äòtrue‚Äô value of the parameres in this simulation settings, I can divide the error into the two subcomponents of variance and bias (see this page for a nice introduction to the bias-variance tradeoff). The bias is the difference between the expected estimate (averaged over many repetitions/iterations) of the same model and the true quantity that we want to estimate. The variance is simply the variability of the model estimates, i.e.¬†how much they oscillate around the expected estimate.\nHere is a plot of the variance. Indeed it can be seen that the variable-lapse model, which has more parameters, is the one that produces more variable estimates. There is however little difference between the other two models‚Äô and the multi-model estimates And here is the bias. This is very satisfactory, as it shows that while all individual models produced biased estimates, the bias of the model-averaged estimates is zero, or very close to zero. In sum, by averaging models of different levels of complexity according to their relative likelihood, I was able to simultaneously minimize the variance and decrease the bias of my estimates, and achieve a greater efficiency. Model averaging seems to be the ideal procedure in this specific settings where the observer would belong to one of the three categories (i.e., she/he would conform to one of the three assumptions) with equal probability. However I think (although I haven‚Äôt checked) that it would perform well even in cases where a single ‚Äútype‚Äù of observers is largely predominant over the other.\n Code The (clumsy written) code for the simulations is shown below:\n# This load some handy functions that are required below library(RCurl) script \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/mattelisi/miscR/master/miscFunctions.R\u0026quot;, ssl.verifypeer = FALSE) eval(parse(text = script)) set.seed(1) # sim parameters n_sim \u0026lt;- 100 sample_sizes \u0026lt;- seq(5, 100, 5) # parameters R \u0026lt;- 3 # range of signal levels (-R, R) n_trial \u0026lt;- 500 mu_par \u0026lt;- c(0, 0.25) # population (mean, std.) sigma_par \u0026lt;- c(1, 0.25) sigmaDiff_par \u0026lt;- c(1, 0.5) lapse_range \u0026lt;- c(0.01, 0.1) # start res \u0026lt;- {} for(n_subjects in sample_sizes){ for(iteration in 1:n_sim){ # make dataset d \u0026lt;- {} for(i in 1:n_subjects){ d_ \u0026lt;- data.frame(x=runif(n_trial)*2*R-R, condition=as.factor(rep(1:2,n_trial/2)), id=i, r=NA) r_i \u0026lt;- runif(1) # draw observer type (wrt lapses) if(r_i\u0026lt;1/3){ # no lapses par1 \u0026lt;- c(rnorm(1,mu_par[1],mu_par[2]), abs(rnorm(1,sigma_par[1],sigma_par[2])), 0) par2 \u0026lt;- c(par1[1], par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])), 0) }else if(r_i\u0026gt;=1/3 \u0026amp; r_i\u0026lt;2/3){ # fixed lapses l_i \u0026lt;- runif(1)*diff(lapse_range) + lapse_range[1] par1 \u0026lt;- c(rnorm(1,mu_par[1],mu_par[2]), abs(rnorm(1,sigma_par[1],sigma_par[2])), l_i) par2 \u0026lt;- c(par1[1], par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])), l_i) }else{ # varying lapses l_i_1 \u0026lt;- runif(1)*diff(lapse_range) + lapse_range[1] l_i_2 \u0026lt;- l_i_1 + (runif(1)*diff(lapse_range) + lapse_range[1])/2 par1 \u0026lt;- c(rnorm(1,mu_par[1],mu_par[2]), abs(rnorm(1,sigma_par[1],sigma_par[2])), l_i_1) par2 \u0026lt;- c(par1[1], par1[2]+abs(rnorm(1,sigmaDiff_par[1],sigmaDiff_par[2])), l_i_2) } ## simulate observer for(i in 1:sum(d_$condition==\u0026quot;1\u0026quot;)){ d_$r[d_$condition==\u0026quot;1\u0026quot;][i] \u0026lt;- rbinom(1,1, psy_3par(d_$x[d_$condition==\u0026quot;1\u0026quot;][i],par1[1],par1[2],par1[3])) } for(i in 1:sum(d_$condition==\u0026quot;2\u0026quot;)){ d_$r[d_$condition==\u0026quot;2\u0026quot;][i] \u0026lt;- rbinom(1,1, psy_3par(d_$x[d_$condition==\u0026quot;2\u0026quot;][i],par2[1],par2[2],par2[3])) } d \u0026lt;- rbind(d,d_) } ## model fitting # lapse assumed to be 0 fit0 \u0026lt;- {} for(j in unique(d$id)){ m0 \u0026lt;- glm(r~x*condition, family=binomial(probit),d[d$id==j,]) sigma_1 \u0026lt;- 1/coef(m0)[2] sigma_2 \u0026lt;- 1/(coef(m0)[2] + coef(m0)[4]) fit0 \u0026lt;- rbind(fit0, data.frame(id=j, sigma_1, sigma_2, loglik=logLik(m0), aic=AIC(m0), model=\u0026quot;zero_lapse\u0026quot;) ) } # fix lapse rate start_p \u0026lt;- c(rep(c(0,1),2), 0) l_b \u0026lt;- c(rep(c(-5, 0.05),2), 0) u_b \u0026lt;- c(rep(c(5, 20), 2), 0.5) fit1 \u0026lt;- {} for(j in unique(d$id)){ ftm \u0026lt;- optimx::optimx(par = start_p, lnorm_3par_multi , d=d[d$id==j,], method=\u0026quot;bobyqa\u0026quot;, lower =l_b, upper =u_b) negloglik \u0026lt;- ftm$value aic \u0026lt;- 2*5 + 2*negloglik # fitted parameters are the first n numbers of optimx output sigma_1\u0026lt;-unlist(ftm [1,2]) sigma_2\u0026lt;-unlist(ftm [1,4]) fit1 \u0026lt;- rbind(fit1, data.frame(id=j, sigma_1, sigma_2, loglik=-negloglik, aic, model=\u0026quot;fix_lapse\u0026quot;) ) } # varying lapse rate start_p \u0026lt;- c(0,1, 0) l_b \u0026lt;- c(-5, 0.05, 0) u_b \u0026lt;- c(5, 20, 0.5) fit2 \u0026lt;- {} for(j in unique(d$id)){ # fit condition 1 ftm \u0026lt;- optimx::optimx(par = start_p, lnorm_3par , d=d[d$id==j \u0026amp; d$condition==\u0026quot;1\u0026quot;,], method=\u0026quot;bobyqa\u0026quot;, lower =l_b, upper =u_b) negloglik_1 \u0026lt;- ftm$value; sigma_1 \u0026lt;- unlist(ftm [1,2]) # fit condition 2 ftm \u0026lt;- optimx::optimx(par = start_p, lnorm_3par , d=d[d$id==j \u0026amp; d$condition==\u0026quot;2\u0026quot;,], method=\u0026quot;bobyqa\u0026quot;, lower =l_b, upper =u_b) negloglik_2 \u0026lt;- ftm$value; sigma_2 \u0026lt;- unlist(ftm [1,2]) aic \u0026lt;- 2*6 + 2*(negloglik_1 + negloglik_2) fit2 \u0026lt;- rbind(fit2, data.frame(id=j, sigma_1, sigma_2, loglik=-negloglik_1-negloglik_2, aic, model=\u0026quot;var_lapse\u0026quot;)) } # compute estimates of the change in slope effect_0 \u0026lt;- mean((fit0$sigma_2-fit0$sigma_1)) effect_1 \u0026lt;- mean((fit1$sigma_2-fit1$sigma_1)) effect_2 \u0026lt;- mean((fit2$sigma_2-fit2$sigma_1)) effect_av \u0026lt;- {} for(j in unique(fit0$id)){ dj \u0026lt;- rbind(fit0[fit0$id==j,], fit1[fit1$id==j,], fit2[fit2$id==j,]) min_aic \u0026lt;- min(dj$aic) dj$delta \u0026lt;- dj$aic - min_aic den \u0026lt;- sum(exp(-0.5*c(dj$delta))) dj$w \u0026lt;- exp(-0.5*dj$delta) / den effect_av \u0026lt;- c(effect_av, sum((dj$sigma_2-dj$sigma_1) * dj$w)) } effect_av \u0026lt;- mean(effect_av) # store results res \u0026lt;- rbind(res, data.frame(effect_0, effect_1, effect_2, effect_av, effect_true=sigmaDiff_par[1], n_subjects, n_trial, iteration)) } } ## PLOT RESULTS library(ggplot2) library(reshape2) res$err0 \u0026lt;- (res$effect_0 -1)^2 res$err1 \u0026lt;- (res$effect_1 -1)^2 res$err2 \u0026lt;- (res$effect_2 -1)^2 res$errav \u0026lt;- (res$effect_av -1)^2 # plot MSE ares \u0026lt;- aggregate(cbind(err0,err1,err2,errav)~n_subjects, res, mean) ares \u0026lt;- melt(ares, id.vars=c(\u0026quot;n_subjects\u0026quot;)) levels(ares$variable) \u0026lt;- c(\u0026quot;no lapses\u0026quot;, \u0026quot;fixed lapse rate\u0026quot;, \u0026quot;variable lapse rate\u0026quot;, \u0026quot;model averaged\u0026quot;) ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_line(size=1)+nice_theme+scale_color_brewer(palette=\u0026quot;Dark2\u0026quot;,name=\u0026quot;model\u0026quot;)+labs(x=\u0026quot;number of subjects\u0026quot;,y=\u0026quot;mean squared error\u0026quot;)+geom_hline(yintercept=0,lty=2,size=0.2) # plot variance ares \u0026lt;- aggregate(cbind(effect_0,effect_1,effect_2,effect_av)~n_subjects, res, var) ares \u0026lt;- melt(ares, id.vars=c(\u0026quot;n_subjects\u0026quot;)) levels(ares$variable) \u0026lt;- c(\u0026quot;no lapses\u0026quot;, \u0026quot;fixed lapse rate\u0026quot;, \u0026quot;variable lapse rate\u0026quot;, \u0026quot;model averaged\u0026quot;) ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_line(size=1)+nice_theme+scale_color_brewer(palette=\u0026quot;Dark2\u0026quot;,name=\u0026quot;model\u0026quot;)+labs(x=\u0026quot;number of subjects\u0026quot;,y=\u0026quot;variance\u0026quot;) # plot bias ares \u0026lt;- aggregate(cbind(effect_0,effect_1,effect_2,effect_av)~n_subjects, res, mean) ares \u0026lt;- melt(ares, id.vars=c(\u0026quot;n_subjects\u0026quot;)) levels(ares$variable) \u0026lt;- c(\u0026quot;no lapses\u0026quot;, \u0026quot;fixed lapse rate\u0026quot;, \u0026quot;variable lapse rate\u0026quot;, \u0026quot;model averaged\u0026quot;) ares$value \u0026lt;- ares$value -1 ggplot(ares,aes(x=n_subjects, y=value, color=variable))+geom_hline(yintercept=0,lty=2,size=0.2)+geom_line(size=1)+nice_theme+scale_color_brewer(palette=\u0026quot;Dark2\u0026quot;,name=\u0026quot;model\u0026quot;)+labs(x=\u0026quot;number of subjects\u0026quot;,y=\u0026quot;bias\u0026quot;)  References Burnham, Kenneth P., and David R. Anderson. 2002. Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach. 2nd editio. New York, US: Springer New York. https://doi.org/10.1007/b97636.\n Wichmann, F a, and N J Hill. 2001. ‚ÄúThe psychometric function: I. Fitting, sampling, and goodness of fit.‚Äù Perception \u0026amp; Psychophysics 63 (8): 1293‚Äì1313. http://www.ncbi.nlm.nih.gov/pubmed/11800458.\n    The psychometric function is a statistical model that predicts the probabilities of the observer response (e.g. ‚Äústimulus A has a larger/smaller instensity than stimulus B‚Äù), conditional to the stimulus and the experimental condition.‚Ü©\n A good experimenter should do that (estimate the size of the difference). A ‚Äúbad‚Äù experimenter might just be interested in obtaining \\(p\u0026lt;.05\\). See this page, compiled by Jean-Michel Hup√©, for some references and guidelines against \\(p\\)-hacking and the misuse of statistical tools in neuroscience.‚Ü©\n The AIC of a model is computed as \\({\\rm{AIC}} = 2k - 2\\log \\left( \\mathcal{L} \\right)\\), where \\(k\\) is the number of free parameters, and \\(\\mathcal{L}\\) is the maximum value of the likelihood function of that model.‚Ü©\n Here it is a parameter common to all models. See the book of Burnham \u0026amp; Andersen for methods to methods to deal with different situations (Burnham and Anderson 2002).‚Ü©\n Such correlation when present can be modelled using a mixed-effect approach. See my tutorial on mized-effects model in the‚Äòmisc‚Äô section of this website.‚Ü©\n   ","date":1512691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512691200,"objectID":"61e0c5318b01590f79c4735b9071af34","permalink":"http://mlisi.xyz/post/model-averaging/","publishdate":"2017-12-08T00:00:00Z","relpermalink":"/post/model-averaging/","section":"post","summary":"In the study of human perception we often need to measure how sensitive is an observer to a stimulus variation, and how her/his sensitivity changes due to changes in the context or experimental manipulations. In many applications this can be done by estimating the slope of the psychometric function1, a parameter that relates to the precision with which the observer can make judgements about the stimulus. A psychometric function is generally characterized by 2-3 parameters: the slope, the threshold (or criterion), and an optional lapse parameter, which indicate the rate at which attention lapses (i.","tags":["R","psychophysics"],"title":"Multi-model estimation of psychophysical parameters","type":"post"},{"authors":null,"categories":["eye movements","tutorial"],"content":" Brief intro to the mathematical formalism used to describe rotations of the eyes in 3D (including the torsional component). The shape of the human eye is approximately a sphere with a diameter of 23 mm, and mechanically it behaves like a ball in a ball and socket joint. Because there is a functional distinguished axis - the visual axis, that is the line of gaze or more precisely the imaginary straight line passing through both the center of the pupil and the center of the fovea - the movements of the eyes are usually divided in gaze direction and cyclotorsion (or simply torsion): while gaze direction refers to the direction of the visual axis, the torsion indicates the rotation of the eyeball about the visual axis. While modern video-based eyetrackers allow to record movements of the visual axis, they do not provide data about torsion. It turns out that there is a nice mathematical relationship that constrains the torsion of the eye in every direction of the gaze. This relationship is known as Listing‚Äôs law, and was named after the german mathematician Johann Benedict Listing (1808-1882). Listing‚Äôs law can be better understood by looking at how the 3D orientation of the eye can be formally described.\nMathematics of 3D eye movements 3D eye position can be specified by characterising the 3D rotation that brings the eye to the current eye position from an arbitrary reference or primary position1, which typically is defined as the position that the eye assumes when looking straight ahead with the head in normal, upright position. This rotation can be described by the 3-by-3 rotation matrix \\(\\bf{R}\\). More specifically the matrix can be used to describe the rotation of three-dimensional coordinates by a certain angle about a certain axis. To formally define this matrix, consider the coordinate system \\(\\{ \\vec{h}_1,\\vec{h}_2,\\vec{h}_3 \\}\\) (a coordinate system is defined by a set of linearly independent vectors; e.g.¬†here \\(\\vec{h}_1 = (1,0,0)\\), corresponding to the \\(x\\) axis) as the head-centered coordinate system where the axis \\(\\vec{h}_1\\) correspond to the visual axis when the eye is in the reference position, and \\(\\{\\vec{e}_1,\\vec{e}_2,\\vec{e}_3\\}\\) is an eye-centered coordinate system where \\(\\vec{e}_1\\) always correspond to the visual axis, regardless of the orientation of the eye (see the figure on top of this page). Any orientation of the eye can be described by a matrix \\(\\bf{R}\\) such that \\[ {{\\vec{e}}_i} = {\\bf{R}} {{\\vec{h}}_i} \\] where \\(i=1,2,3\\). This rotation matrix is straightforward for 1D rotations. For example, a purely horizontal rotation of an angle \\(\\theta\\) around the axis \\(\\vec{h}_3\\) is formulated as \\[ \\bf{R}_3 \\left( \\theta \\right) = \\left( {\\begin{array}{*{20}{c}} {\\cos \\theta }\u0026amp;{ - \\sin \\theta }\u0026amp;0\\\\ {\\sin \\theta }\u0026amp;{\\cos \\theta }\u0026amp;0\\\\ 0\u0026amp;0\u0026amp;1 \\end{array}} \\right) \\] The first two columns of the matrix indicates the new coordinates of the first (i.e., \\(\\vec{h}_1\\)) and of the second basis (\\(\\vec{h}_2\\)) of the new eye-centerd coordinate system after the rotation, expressed according to the initial head-centered coordinate system. The third basis, \\(\\vec{h}_3\\) is the axis of rotation, and does not change. It becomes more complicated for 3D rotations, i.e.¬†rotations of the fixed eye-centered coordinate system to any new orientation. They can be obtained by calculating a sequence of 3 different rotations about the three fixed axis, and multiplying the corresponding matrices: \\(\\bf{R} = \\bf{R}_3 \\left( \\theta \\right) \\bf{R}_2 \\left( \\phi \\right) \\bf{R}_1 \\left( \\psi \\right)\\). Although the first two rotations are sufficient to specity the orientation of the visual axis, the third is necessary to specify the torsion component and fully specify the 3D orientation of the eye. Importantly, the order of the three rotations is relevant - rotations are not commutative, so if you put them in different order you end up with a different result - and needs to be arbitrarily specified (when it is specified in this order is referred to as Flick sequence). This representation of 3D orientations is not very efficient (9 values, while only 3 are necessary), or practical for computations; additionally one needs to define arbitrarily the order of the rotations of the sequence.\nQuaternions and rotation vectors An alternative way to describe rotations is with quaternions. Quaternions can be looked upon as four-dimensional vectors, although they are more commonly split in a real scalar part and an imaginary vector part; they are in fact an extension of the complex numbers. They have the form \\[ q_0 + q_1i + q_2j + q_3k = \\left( q_0,\\vec{q} \\cdot \\vec{I} \\right) = \\left( r,\\vec{v} \\right) \\] where \\[ \\vec{q} = \\left( \\begin{array}{*{20}{c}} {q_1}\\\\ {q_2}\\\\ {q_3} \\end{array} \\right) \\] and \\[ \\vec{I} = \\left( \\begin{array}{*{20}{c}} {i}\\\\ {j}\\\\ {k} \\end{array} \\right) \\] \\(i,j,k\\) are the quaternion units. These can be multiplied according to the following formula, discovered by Hamilton in 1843 \\[ i^2 = j^2 = k^2 = ijk = - 1 \\] This formula may seems strange but it determines all the possible products of \\(i,j,k\\), such as \\(ij=k\\) and \\(ji=-k\\). Note that the product of the basis are not commutative. There is a visual trick to remember the multiplication rules, based on the following diagram:\nMultiplying quaternions. Multiplying two elements in the clockwise direction gives the next element along the same direction (e.g. \\(jk=i\\)). The same is for counter-clockwise directions, except that the result is negative (e.g. \\(kj=-i\\)).  Quaternions can be used to represent rotations. For example, a rotation of an angle \\(\\theta\\) around the axis define by the unit vector \\(\\vec{u} = (u_1, u_2,u_3) = u_1i + u_2j + u_3k\\)2 can be described by the following quaternion \\[ \\cos \\frac{\\theta}{2} + \\sin \\frac{\\theta}{2}\\left( u_1i + u_2j + u_3k \\right) \\] The direction of the rotation is given by the right-hand rule. Successive rotations can combined using the formula for quaternion multiplication. The multiplication of quaternions can be computed by the products of their elements element as if they were two polynomials, but keeping track of the ordering of the basis, as their multiplication is not commutative. This is a desired property if we want to specify rotations, which as seen earlier are also not commutative. Quaternion multiplication can be also expressed in the modern language of vector and cross product \\[ \\left( r_1,\\vec{v_1} \\right) \\left( r_2,\\vec{v_2} \\right) = \\left( r_1 r_2 - \\vec{v_1} \\cdot \\vec{v_2},\\;\\; r_1\\vec{v_2} + r_2\\vec{v_1} +\\vec{v_1} \\times \\vec{v_2} \\right) \\] where ‚Äú\\(\\cdot\\)‚Äù is the dot product and ‚Äú\\(\\times\\)‚Äù is the cross product.\nIn sum, quaternions are pretty useful to compute transformations in 3D. One can use quaternions to combine 3D any sequence of rotations about arbitrary axis (using quaternion multiplications), as well as to rotate any 3D Euclidean vector about any arbitrary axis. A quaternion can also be transformed into a 3D rotation matrix (formula here), which then may be used in 3D graphics.\nRotation vectors are an even more succint representation of rotations. Indeed, the scalar component of the quaterion (\\(q_0\\)) does not add any information that is not alredy in the vector part, so a rotation could be effectively described by just 3 numbers. The rotation vector \\(\\vec{r}\\), which correspond to a rotation of an angle \\(\\theta\\) about an axis \\(\\vec{n}\\) is defined as \\[ \\vec{r} = \\tan \\left( \\frac{\\theta}{2} \\right) \\vec{n} \\] which can be defined also with respect to the equivalent quaternion \\(\\textbf{q}\\) \\[ \\textbf{q}=\\left( q_0, \\vec{q} \\right) = \\left( \\cos \\left(\\frac{\\theta}{2}\\right), \\sin \\left(\\frac{\\theta}{2}\\right)\\vec{n} \\right) \\] as \\[ \\vec{r} = \\frac{\\vec{q}}{q_0} \\].\n  Donder‚Äôs law and Listing‚Äôs law Donder‚Äôs law (1848) states that the eye use only two degrees of freedom while fixating, although mechanically it has three. In othere words this means that the torsion component of the eye movement is not arbitrary but it is uniquely determined by the direction of the visual axis and is independent of the previous eye movements. From the material review above it should be clear how any 3D eye orientation can be fully described as a rotation abour a given axis from a primary reference position. This allows also to formulate Donder‚Äôs law more specifically,according to what is known as Listing‚Äôs law (Helmholtz et al. 1910,@Haustein1989) ‚ÄúThere exists a certain eye position from which the eye may reach any other position of fixation by a rotation around an axis perpendicular to the visual axis. This particular position is called primary position‚Äù. This means that all possible eye positions can be reached from the primary position by a single rotation about an axis perpendicular to the visual axis. Since they are all perpendicular to the visual axis, all rotation axis that satisfy Listing‚Äôs law are on the same plane (Listing‚Äôs plane). The law can be tested with eyetracking equipments that allows measuring also the torsional components (such as scleral coils): results have shown that the standard deviation from Listing‚Äôs plane of empirically measured rotation vectors is only about 0.5-1 deg (Haslwanter 1995). Formally it can be written that for any orientation of the visual axis, defined by the rotation vector \\(\\vec{a}\\) and measured from the primary position \\(\\vec{h_1}=(1,0,0)\\), \\[ \\vec{h_1} \\cdot \\vec{a} = 0 \\] This indicates simply that the rotation about the visual axis is 0, and that as a consequence all the rotation axes lies in a frontal plane.\nGoing back to the beginning, knowing the coordinates os Listing‚Äôs plane one can compute the rotation vector that correspond to the current eye position from the recording of the 2D gaze location on a screen. In the simplest case, we assume that the primary position corresponds to when the observer fixates the center of the screen, \\((0,0)\\). What is the rotation vector that describes the 3D eye orientation when the observer fixates the location \\((s_x, s_y)\\) ? Let‚Äôs say the position on screen is defined in cm, and we know that the distance of the eye from the screen is \\(L\\) cm. The rotation angle can be computed as \\(\\theta = \\rm{atan} \\frac{\\sqrt{s_x^2+s_y^2}}{L}\\), while the angle that defines the orientation of the rotation axis within Listing‚Äôs plane is \\(\\alpha = \\rm{atan2}(s_y,s_x)\\). The complete rotation vector is then \\[ \\vec{r} = \\tan \\left( \\frac{\\theta}{2}\\right) \\cdot \\left( {\\begin{array}{*{20}{c}} 0\\\\ {\\cos \\alpha }\\\\ { - \\sin \\alpha } \\end{array}} \\right) \\] This vector describe aparticular eye position as a rotation from the reference position, and does not have a torsional component (that is a component along \\(\\vec{h_1}\\)). Indeed, Listing‚Äôs law implies that all possible eye positions can be reached from the primary reference position without a torsional component. However, vectors describing rotations from and to positions different than the primary one do not, in general, lie in Listing‚Äôs plane. For Listing law to hold such vectors must lie in a plane whose orientation depends on the current eye position, and more specifically is such that the vector perpendicular to the plane is exactly halfway between the current and the primary eye position (Tweed and Vilis 1990).\n References Haslwanter, Thomas. 1995. ‚ÄúMathematics of three-dimensional eye rotations.‚Äù Vision Research 35 (12): 1727‚Äì39. https://doi.org/10.1016/0042-6989(94)00257-M.\n Haustein, Werner. 1989. ‚ÄúConsiderations on Listing‚Äôs Law and the primary position by means of a matrix description of eye position control.‚Äù Biological Cybernetics 60 (6): 411‚Äì20. https://doi.org/10.1007/BF00204696.\n Helmholtz, Hermann von, Hermann von Helmholtz, Hermann von Helmholtz, and Hermann von Helmholtz. 1910. Handbuch der Physiologischen Optik. Hamburg: Voss.\n Tweed, Douglas, and Tutis Vilis. 1990. ‚ÄúGeometric relations of eye position and velocity vectors during saccades.‚Äù Vision Research 30 (1): 111‚Äì27. https://doi.org/10.1016/0042-6989(90)90131-4.\n    Euler‚Äôs theorem guarantee that a rigid body can always move from one orientation to any different one through a single rotation about a fixed axis.‚Ü©\n Saying that \\(\\vec(u)\\) is a unit vector indicates that it has length 1, i.e. \\(\\left| \\vec{u} \\right| = \\sqrt{u_1^2 + u_2^2 + u_3^2} = 1\\)‚Ü©\n   ","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"dfd83b341bfdcdee59396316fbda5106","permalink":"http://mlisi.xyz/post/listing-s-law-and-the-mathematics-of-the-eyes/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/post/listing-s-law-and-the-mathematics-of-the-eyes/","section":"post","summary":"Brief intro to the mathematical formalism used to describe rotations of the eyes in 3D (including the torsional component). The shape of the human eye is approximately a sphere with a diameter of 23 mm, and mechanically it behaves like a ball in a ball and socket joint. Because there is a functional distinguished axis - the visual axis, that is the line of gaze or more precisely the imaginary straight line passing through both the center of the pupil and the center of the fovea - the movements of the eyes are usually divided in gaze direction and cyclotorsion (or simply torsion): while gaze direction refers to the direction of the visual axis, the torsion indicates the rotation of the eyeball about the visual axis.","tags":["eye movements"],"title":"Listing's law, and the mathematics of the eyes","type":"post"},{"authors":["Delphine Massendari","Matteo Lisi","Th√©r√®se Collins","Patrick Cavanagh"],"categories":null,"content":"","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"973335e911ca6f4139b9e9d9433b8815","permalink":"http://mlisi.xyz/publication/memory-delphine/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/publication/memory-delphine/","section":"publication","summary":"The double-drift stimulus (a drifting Gabor with orthogonal internal motion) generates a large discrepancy between its physical and perceived path. Surprisingly, saccades directed to the double-drift stimulus land along the physical, and not perceived, path (Lisi \u0026 Cavanagh, 2015). Here we asked whether memory-guided saccades exhibited the same dissociation from perception. Participants were asked to keep their gaze centered on a fixation dot while the double-drift stimulus moved back and forth on a linear path in the periphery. The offset of the fixation was the go-signal to make a saccade to the target. In the visually-guided saccade condition, the Gabor kept moving on its trajectory after the go-signal but was removed once the saccade began. In the memory conditions, the Gabor disappeared before or at the same time as the go-signal (0 to 1000 ms delay) and participants made a saccade to its remembered location. The results showed that visually-guided saccades again targeted the physical rather than the perceived location. However, memory saccades, even with 0 ms delay, had landing positions shifted toward the perceived location. Our result shows that memory- and visually-guided saccades are based on different spatial information.","tags":null,"title":"Memory-guided saccades show effect of perceptual illusion whereas visually-guided saccades do not","type":"publication"},{"authors":["Matteo Lisi","Patrick Cavanagh"],"categories":null,"content":"","date":1506211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506211200,"objectID":"fa97b55219a60c55ef9edc5cd119bdb6","permalink":"http://mlisi.xyz/publication/double-drift-pointing/","publishdate":"2017-09-24T00:00:00Z","relpermalink":"/publication/double-drift-pointing/","section":"publication","summary":"Our visual system allows us to localize objects in the world and plan motor actions toward them. We have recently shown that the localization of moving objects differs between perception and saccadic eye movements (Lisi \u0026 Cavanagh, 2015), suggesting different localization mechanisms for perception and action. This finding, however, could reflect a unique feature of the saccade system rather than a general dissociation between perception and action. To disentangle these hypotheses, we compared object localization between saccades and hand movements. We flashed brief targets on top of double-drift stimuli (moving Gabors with the internal pattern drifting orthogonally to their displacement, inducing large distortions in perceived location and direction) and asked participants to point or make saccades to them. We found a surprising difference between the two types of movements: Although saccades targeted the physical location of the flashes, pointing movements were strongly biased toward the perceived location (about 63% of the perceptual illusion). The same bias was found when pointing movements were made in open-loop conditions (without vision of the hand). These results indicate that dissociations are present between different types of actions (not only between action and perception) and that visual processing for saccadic eye movements differs from that for other actions. Because the position bias in the double-drift stimulus depends on a persisting influence of past sensory signals, we suggest that spatial maps for saccades might reflect only recent, short-lived signals, and the spatial representations supporting conscious perception and hand movements integrate visual input over longer temporal intervals.","tags":null,"title":"Different spatial representations guide eye and hand movements","type":"publication"},{"authors":["Matteo Lisi","Gianluigi Mongillo","Andrei Gorea"],"categories":null,"content":"","date":1501200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501200000,"objectID":"7936ddf0d021f816ec6edbd566db8956","permalink":"http://mlisi.xyz/publication/discrete-confidence/","publishdate":"2017-07-28T00:00:00Z","relpermalink":"/publication/discrete-confidence/","section":"publication","summary":"Whether humans are optimal decision makers is still a debated issue in the realm of perceptual decisions. Taking advantage of the direct link between an optimal decision-making and the confidence in that decision, we offer a new dual-decisions method of inferring such confidence without asking for its explicit valuation. Our method circumvents the well-known miscalibration issue with explicit confidence reports as well as the specification of the cost-function required by 'opt-out' or post-decision wagering methods. We show that observers' inferred confidence in their first decision and its use in a subsequent decision (conditioned upon the correctness of the first) fall short of both an optimal and an under-sampling behavior and are significantly better fitted by a model positing that observers use no more than four confidence levels, at odds with the continuous confidence function of stimulus level prescribed by a normative behavior.","tags":null,"title":"Discrete confidence levels revealed by sequential decisions","type":"publication"},{"authors":["Gerrit Maus","Marianne Dyuck","Matteo Lisi","Th√©r√®se Collins","David Whitney","Patrick Cavanagh"],"categories":null,"content":"","date":1485993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485993600,"objectID":"f1ba8566cce3b9700c52d9075c22cb49","permalink":"http://mlisi.xyz/publication/blink-adapt/","publishdate":"2017-02-02T00:00:00Z","relpermalink":"/publication/blink-adapt/","section":"publication","summary":"Eye blinks cause disruptions to visual input and are accompanied by rotations of the eyeball [1]. Like every motor action, these eye movements are subject to noise and introduce instabilities in gaze direction across blinks [2]. Accumulating errors across repeated blinks would be debilitating for visual performance. Here, we show that the oculomotor system constantly recalibrates gaze direction during blinks to counteract gaze instability. Observers were instructed to fixate a visual target while gaze direction was recorded and blinks were detected in real time. With every spontaneous blink - while eyelids were closed - the target was displaced laterally by 0.5 degrees (or 1.0 degrees). Most observers reported being unaware of displacements during blinks. After adapting for approx. 35 blinks, gaze positions after blinks showed significant biases toward the new target position. Automatic eye movements accompanied each blink, and an aftereffect persisted for a few blinks after target displacements were eliminated. No adaptive gaze shift occurred when blinks were simulated with shutter glasses at random time points or actively triggered by observers, or when target displacements were masked by a distracting stimulus. Visual signals during blinks are suppressed by inhibitory mechanisms [3-6], so that small changes across blinks are generally not noticed [7, 8]. Additionally, target displacements during blinks can trigger automatic gaze recalibration, similar to the well-known saccadic adaptation effect [9-11]. This novel mechanism might be specific to the maintenance of gaze direction across blinks or might depend on a more general oculomotor recalibration mechanism adapting gaze position during intrinsically generated disruptions to visual input..","tags":null,"title":"Target Displacements during Eye Blinks Trigger Automatic Recalibration of Gaze Direction","type":"publication"},{"authors":["Matteo Lisi","Andrei Gorea"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"462ebd5574d1340577635f1949fa1e06","permalink":"http://mlisi.xyz/publication/time-constancy/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/publication/time-constancy/","section":"publication","summary":"Estimated time contracts or dilates depending on many visual-stimulation attributes (size, speed, etc.). Here we show that when such attributes are jointly modulated so as to respect the rules of perspective, their effect on the perceived duration of moving objects depends on the presence of contextual information about viewing distance. We show that perceived duration contracts and dilates with changes in the retinal input associated with increasing distance from the observer only when the moving objects are presented in the absence of information about the viewing distance. When this information (in the form of linear perspective cues) is present, the time-contraction/dilation effect is eliminated and time constancy is preserved. This is the first demonstration of a perceptual time constancy, analogous to size constancy but in the time domain. It points to a normalization of time computation operated by the visual brain when stimulated within a quasi-ecological environment.","tags":null,"title":"Time Costancy in human perception","type":"publication"},{"authors":["Mariagrazia Ranzini","Matteo Lisi","Marco Zorzi"],"categories":null,"content":"","date":1454371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454371200,"objectID":"75feea5133bc5477adaf7c4e969f3172","permalink":"http://mlisi.xyz/publication/voluntary-eye/","publishdate":"2016-02-02T00:00:00Z","relpermalink":"/publication/voluntary-eye/","section":"publication","summary":"Growing evidence suggests that orienting visual attention in space can influence the processing of numerical magnitude, with leftward orienting speeding up the processing of small numbers relative to larger ones and the converse for rightward orienting. The manipulation of eye movements is a convenient way to direct visuospatial attention, but several aspects of the complex relationship between eye movements, attention orienting and number processing remain unexplored. In a previous study, we observed that inducing involuntary, reflexive eye movements by means of optokinetic stimulation affected number processing only when numerical magnitude was task relevant (i.e., during magnitude comparison, but not during parity judgment; Ranzini et al., in J Cogn Psychol 27, 459-470, (2015). Here, we investigated whether processing of task-irrelevant numerical magnitude can be modulated by voluntary eye movements, and whether the type of eye movements (smooth pursuit vs. saccades) would influence this interaction. Participants tracked with their gaze a dot while listening to a digit. The numerical task was to indicate whether the digit was odd or even through non-spatial, verbal responses. The dot could move leftward or rightward either continuously, allowing tracking by smooth pursuit eye movements, or in discrete steps across a series of adjacent locations, triggering a sequence of saccades. Both smooth pursuit and saccadic eye movements similarly affected number processing and modulated response times for large numbers as a function of direction of motion. These findings suggest that voluntary eye movements redirect attention in mental number space and highlight that eye movements should play a key factor in the investigation of number-space interactions.","tags":null,"title":"Voluntary eye movements direct attention on the mental number space","type":"publication"},{"authors":["Matteo Lisi","Patrick Cavanagh"],"categories":null,"content":"","date":1443052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443052800,"objectID":"7848f75a6c65db9980c195245815eb3e","permalink":"http://mlisi.xyz/publication/double-drift-1/","publishdate":"2015-09-24T00:00:00Z","relpermalink":"/publication/double-drift-1/","section":"publication","summary":"Visual processing in the human brain provides the data both for perception and for guiding motor actions. It seems natural that our actions would be directed toward perceived locations of their targets, but it has been proposed that action and perception rely on different visual information [1, 2, 3, 4], and this provocative claim has triggered a long-lasting debate [5, 6, 7]. Here, in support of this claim, we report a large, robust dissociation between perception and action. We take advantage of a perceptual illusion in which visual motion signals presented within the boundaries of a peripheral moving object can make the object's apparent trajectory deviate by 45 degrees or more from its physical trajectory [8, 9, 10], a shift several times larger than the typical discrimination threshold for motion direction [11]. Despite the large perceptual distortion, we found that saccadic eye movements directed to these moving objects clearly targeted locations along their physical rather than apparent trajectories. We show that the perceived trajectory is based on the accumulation of position error determined by prior sensory history - an accumulation of error that is not found for the action toward the same target. We suggest that visual processing for perception and action might diverge in how past information is combined with new visual input, with action relying only on immediate information to track a target, whereas perception builds on previous estimates to construct a conscious representation.","tags":null,"title":"Dissociation between the Perceptual and Saccadic Localization of Moving Objects","type":"publication"},{"authors":["Mario Bonato","Chiara Spironelli","Matteo Lisi","Kostantinos Priftis","Marco Zorzi"],"categories":null,"content":"","date":1441238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441238400,"objectID":"7ad9670843230e6e62d6fb63e88c7b69","permalink":"http://mlisi.xyz/publication/load-erp/","publishdate":"2015-09-03T00:00:00Z","relpermalink":"/publication/load-erp/","section":"publication","summary":"While the role of selective attention in filtering out irrelevant information has been extensively studied, its characteristics and neural underpinnings when multiple environmental stimuli have to be processed in parallel are much less known. Building upon a dual-task paradigm that induced spatial awareness deficits for contralesional hemispace in right hemisphere-damaged patients, we investigated the electrophysiological correlates of multimodal load during spatial monitoring in healthy participants. The position of appearance of briefly presented, lateralized targets had to be reported either in isolation (single task) or together with a concurrent task, visual or auditory, which recruited additional attentional resources (dual-task). This top-down manipulation of attentional load, without any change of the sensory stimulation, modulated the amplitude of the first positive ERP response (P1) and shifted its neural generators, with a suppression of the signal in the early visual areas during both visual and auditory dual tasks. Furthermore, later N2 contralateral components elicited by left targets were particularly influenced by the concurrent visual task and were related to increased activation of the supramarginal gyrus. These results suggest that the right hemisphere is particularly affected by load manipulations, and confirm its crucial role in subtending automatic orienting of spatial attention and in monitoring both hemispaces.","tags":null,"title":"Effects of Multimodal Load on Spatial Monitoring as Revealed by ERPs","type":"publication"},{"authors":["Matteo Lisi","Mario Bonato","Marco Zorzi"],"categories":null,"content":"","date":1441238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441238400,"objectID":"6ebbccd34101c82d369f8b214f6a1b38","permalink":"http://mlisi.xyz/publication/load-pupil/","publishdate":"2015-09-03T00:00:00Z","relpermalink":"/publication/load-pupil/","section":"publication","summary":"It has long been known that the diameter of human pupil enlarges with increasing effort during the execution of a task. This has been observed not only for purely mechanical effort but also for mental effort, as for example the computation of arithmetic problems with different levels of difficulty. Here we show that pupil dilation reflects changes in visuospatial awareness induced by attentional load during multi-tasking. In the single-task condition, participants had to report the position of lateralized, briefly presented, masked visual targets ('right', 'left', or 'both' sides). In the multitasking conditions, participants also performed additional tasks, either visual or auditory, to increase the attentional load. Sensory stimulation was kept constant across all conditions to rule out the influence of low-level factors. Results show that event-related pupil dilation strikingly increased with task demands, mirroring a concurrent decrease in visuospatial awareness. Importantly, pupil dilation significantly differed between two dual-task conditions that required to process the same number of stimuli but yielded differed levels of accuracy (difficulty). In contrast, pupil dilation did not differ between two conditions which were equally challenging but differed both in the modality of the dual task (auditory vs. visual) and in the number of stimuli to be attended. We conclude that pupil dilation genuinely reflects the top-down allocation of supramodal attentional resources.","tags":null,"title":"Pupil dilation reveals top-down attentional load during spatial monitoring","type":"publication"},{"authors":["Matteo Lisi","Marco Zorzi","Patrick Cavanagh"],"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"3f20ca68e81e61c11ff1bed4823d8b0b","permalink":"http://mlisi.xyz/publication/spatial-constancy-attention/","publishdate":"2015-05-01T00:00:00Z","relpermalink":"/publication/spatial-constancy-attention/","section":"publication","summary":"Recent studies have shown that attentional facilitation lingers at the retinotopic coordinates of a previously attended position after an eye movement. These results are intriguing, because the retinotopic location becomes behaviorally irrelevant once the eyes have moved. Critically, in these studies participants were asked to maintain attention on a blank location of the screen. In the present study, we examined whether the continuing presence of a visual object at the cued location could affect the allocation of attention across eye movements. We used a trans-saccadic cueing paradigm in which the relevant positions could be defined or not by visual objects (simple square outlines). We find an attentional benefit at the spatiotopic location of the cue only when the object (the placeholder) has been continuously present at that location. We conclude that the presence of an object at the attended location is a critical factor for the maintenance of spatial constancy of attention across eye movements, a finding that helps to reconcile previous conflicting results.","tags":null,"title":"Spatial constancy of attention across eye movements is mediated by the presence of visual objects","type":"publication"},{"authors":["Mariagrazia Ranzini","Matteo Lisi","Elvio Blini","Marco Pitteri","Barbara Treccani","Konstantinos Priftis","Marco Zorzi"],"categories":null,"content":"Mariagrazia Ranzini and Matteo Lisi contributed equally to the work\n","date":1407110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407110400,"objectID":"2f40e57dcabb44650a9da05ff9373b53","permalink":"http://mlisi.xyz/publication/optokinetic/","publishdate":"2014-08-04T00:00:00Z","relpermalink":"/publication/optokinetic/","section":"publication","summary":"Previous studies have shown that number processing can induce spatial biases in perception and action and can trigger the orienting of visuospatial attention. Few studies, however, have investigated how spatial processing and visuospatial attention influences number processing. In the present study, we used the optokinetic stimulation (OKS) technique to trigger eye movements and thus overt orienting of visuospatial attention. Participants were asked to stare at OKS, while performing parity judgements (Experiment 1) or number comparison (Experiment 2), two numerical tasks that differ in terms of demands on magnitude processing. Numerical stimuli were acoustically presented, and participants responded orally. We examined the effects of OKS direction (leftward or rightward) on number processing. The results showed that rightward OKS abolished the classic number size effect (i.e., faster reaction times for small than large numbers) in the comparison task, whereas the parity task was unaffected by OKS direction. The effect of OKS highlights a link between visuospatial orienting and processing of number magnitude that is complementary to the more established link between numerical and visuospatial processing. We suggest that the bidirectional link between numbers and space is embodied in the mechanisms subserving sensorimotor transformations for the control of eye movements and spatial attention.","tags":null,"title":"Larger, smaller, odd or even? Task-specific effects of optokinetic stimulation on the mental number space","type":"publication"},{"authors":["Andrea Desantis","Pascal Mamassian","Matteo Lisi","Florian Waszak"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"634dce2d4310a81ae045eb12baaacce6","permalink":"http://mlisi.xyz/publication/desantis/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/publication/desantis/","section":"publication","summary":"The brain combines information from different senses to improve performance on perceptual tasks. For instance, auditory processing is enhanced by the mere fact that a visual input is processed simultaneously. However, the sensory processing of one modality is itself subject to diverse influences. Namely, perceptual processing depends on the degree to which a stimulus is predicted. The present study investigated the extent to which the influence of one processing pathway on another pathway depends on whether or not the stimulation in this pathway is predicted. We used an action-effect paradigm to vary the match between incoming and predicted visual stimulation. Participants triggered a bimodal stimulus composed of a Gabor and a tone. The Gabor was either congruent or incongruent compared to an action-effect association that participants learned in an acquisition phase.We tested the influence of action-effect congruency on the loudness perception of the tone. We observed that an incongruent-task-irrelevant Gabor stimulus increases participant's sensitivity to loudness discrimination. An identical result was obtained for a second condition in which the visual stimulus was predicted by a cue instead of an action. Our results suggest that prediction error is a driving factor of the crossmodal interplay between vision and audition.","tags":null,"title":"The prediction of visual stimuli influences auditory loudness discrimination","type":"publication"},{"authors":["Michele De Filippo De Grazia","Simone Cutini","Matteo Lisi","Marco Zorzi"],"categories":null,"content":"","date":1346284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1346284800,"objectID":"a5a79badb0397b22e5b40b86b0bb0ad7","permalink":"http://mlisi.xyz/publication/unsupervised-space-coding/","publishdate":"2012-08-30T00:00:00Z","relpermalink":"/publication/unsupervised-space-coding/","section":"publication","summary":"The posterior parietal cortex (PPC) is fundamental for sensorimotor transformations because it combines multiple sensory inputs and posture signals into different spatial reference frames that drive motor programming. Here, we present a computational model mimicking the sensorimotor transformations occurring in the PPC. A recurrent neural network with one layer of hidden neurons (restricted Boltzmann machine) learned a stochastic generative model of the sensory data without supervision. After the unsupervised learning phase, the activity of the hidden neurons was used to compute a motor program (a population code on a bidimensional map) through a simple linear projection and delta rule learning. The average motor error, calculated as the difference between the expected and the computed output, was less than 3 degrees. Importantly, analyses of the hidden neurons revealed gain-modulated visual receptive fields, thereby showing that space coding for sensorimotor transformations similar to that observed in the PPC can emerge through unsupervised learning. These results suggest that gain modulation is an efficient coding strategy to integrate visual and postural information toward the generation of motor commands.","tags":null,"title":"Space coding for sensorimotor transformations can emerge through unsupervised learning","type":"publication"},{"authors":["Casarotti Marco","Lisi Matteo","Umilt√† Carlo","Zorzi Marco"],"categories":null,"content":"Marco Casarotti and Matteo Lisi contributed equally to the article and are listed in alphabetical order.\n","date":1338249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338249600,"objectID":"c12ed7bbc167ad9c2bb5a85900a6553e","permalink":"http://mlisi.xyz/publication/premotor-model/","publishdate":"2012-05-29T00:00:00Z","relpermalink":"/publication/premotor-model/","section":"publication","summary":"Growing evidence indicates that planning eye movements and orienting visuospatial attention share overlapping brain mechanisms. A tight link between endogenous attention and eye movements is maintained by the premotor theory, in contrast to other accounts that postulate the existence of specific attention mechanisms that modulate the activity of information processing systems. The strong assumption of equivalence between attention and eye movements, however, is challenged by demonstrations that human observers are able to keep attention on a specific location while moving the eyes elsewhere. Here we investigate whether a recurrent model of saccadic planning can account for attentional effects without requiring additional or specific mechanisms separate from the circuits that perform sensorimotor transformations for eye movements. The model builds on the basis function approach and includes a circuit that performs spatial remapping using an internal forward model of how visual inputs are modified as a result of saccadic movements. Simulations show that the latter circuit is crucial to account for dissociations between attention and eye movements that may be invoked to disprove the premotor theory. The model provides new insights into how spatial remapping may be implemented in parietal cortex and offers a computational framework for recent proposals that link visual stability with remapping of attention pointers.","tags":null,"title":"Paying Attention through Eye Movements: A Computational Investigation of the Premotor Theory of Spatial Attention","type":"publication"}]