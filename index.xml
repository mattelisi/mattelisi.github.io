<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>juicy bits on juicy bits</title>
    <link>https://mattelisi.github.io/</link>
    <description>Recent content in juicy bits on juicy bits</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Matteo Lisi</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Multi-model estimation of psychophysical parameters</title>
      <link>https://mattelisi.github.io/post/model-averaging/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/post/model-averaging/</guid>
      <description>&lt;p&gt;In the study of human perception we often need to measure how sensitive is an observer to a stimulus variation, and how her/his sensitivity changes due to changes in the context or experimental manipulations. In many applications this can be done by estimating the slope of the psychometric function&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, a parameter that relates to the precision with which the observer can make judgements about the stimulus. A psychometric function is generally characterized by 2-3 parameters: the slope, the threshold (or criterion), and an optional lapse parameter, which indicate the rate at which attention lapses (i.e. &lt;em&gt;stimulus-independent&lt;/em&gt; errors) occur.&lt;/p&gt;
&lt;p&gt;As an example, consider the situation where an observer is asked to judge whether a signal (can be anything, from the orientation angle of a line on a screen, or the pitch of a tone, to the speed of a car or the approximate number of people in a crowd, etc.) is above or below a given reference value, call it zero. The experimenter presents the observers with many signals of different intensities, and the observer is asked to respond by making a binary choice (larger/smaller than the reference), under two different contextual conditions (before/after having a pint, with different headphones, etc.). These two conditions are expected to results in different sensitivity, and the experimenter is interested in estimating as precisely as possible the difference in sensitivity&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The psychometric function for one observer in the two conditions might look like this (figure below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://mattelisi.github.io/img/psyfun.png&#34; alt=&#34;Psychometric functions. Each points is a response (0 or 1 ; some vertical jitter is added for clarity), and the lines represent the fitted psychometric model (here a cumulative Gaussian psychometric function). The two facets of the plots represent the two different conditions. It can be seen that the precision seems to be different across conditions: judgements made under condition 2 are more variable, indicating reduced sensitivity. &#34; style=&#34;width:50.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Psychometric functions. Each points is a response (0 or 1 ; some vertical jitter is added for clarity), and the lines represent the fitted psychometric model (here a cumulative Gaussian psychometric function). The two facets of the plots represent the two different conditions. It can be seen that the precision seems to be different across conditions: judgements made under condition ‘2’ are more variable, indicating reduced sensitivity. &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Our focus is on the psychometric slope, and we are not really interested in measuring the lapse rate; however it is still important to take lapses into account: it has been shown that not accounting for lapses can have a large influence on the estimates of the slope &lt;span class=&#34;citation&#34;&gt;(Wichmann and Hill 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-problem-with-lapses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The problem with lapses&lt;/h3&gt;
&lt;p&gt;Different observer may lapse at quite different rates, and for some of them the lapse rate is probably so small that can be considered negligible. Also, we usually don’t have hypothesis about lapses, and about whether they should or should not vary across conditions. We can base our analysis on different assumptions about when the observers may have attention lapses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;they may never lapse (or they do so with a small, negligible frequency);&lt;/li&gt;
&lt;li&gt;they may lapse at a fairly large rate, but the rate is assumed constant across conditions (reasonable, especially if conditions are randomly interleaved);&lt;/li&gt;
&lt;li&gt;they may lapse with variable rate across conditions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These assumptions will lead to three different psychometric models. The number can increase if we consider also different functional forms of the relationship between stimulus and choice; here for simplicity I will consider only psychometric models based on the cumulative Gaussian function (equivalent to a &lt;em&gt;probit&lt;/em&gt; analysis), &lt;span class=&#34;math inline&#34;&gt;\(\Phi = \frac{1}{2}\left[ {1 + {\rm{erf}}\left( {\frac{{x - \mu }}{{\sigma \sqrt 2 }}} \right)} \right]\)&lt;/span&gt;, where the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; woud correspond to the threshold parameter, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to the slope, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the stimulus intensity. In our case the first assumption (&lt;em&gt;zero lapses&lt;/em&gt;) would lead to the simplest psychometric model &lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i)= \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt; where the subscript &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; indicates that the values of both mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; and slope &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; are specific to the condition &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The second assumption (&lt;em&gt;fixed lapse rate&lt;/em&gt;) could correspond to the model &lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i, \lambda)= \lambda + (1-2\lambda) \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt; where the parameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; correspond to the probability of the observer making a random error. Note that this is assumed to be fixed with respect to the condition (no subscript). Finally the last assumption (&lt;em&gt;variable lapse rate&lt;/em&gt;) would suggests the model &lt;span class=&#34;math display&#34;&gt;\[
\Psi (x, \mu_i, \sigma_i, \lambda_i)= \lambda_i + (1-2\lambda_i) \Phi (\frac{x-\mu_i}{\sigma_i})
\]&lt;/span&gt; where all the parameters are allowed to vary between conditions.&lt;/p&gt;
&lt;p&gt;We have thus three different models, but we haven’t any prior information to decide which model is more likely to be correct in our case. Also, we acknowledge the fact that there are individual differences and each observer in our sample may conform to one of the three assumptions with equal probability. Hence, ideally, we would like to find a way to deal with lapses - and find the best estimates of the slope values &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; without committing to one of the three models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-model-inference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-model inference&lt;/h1&gt;
&lt;p&gt;One possible solution to this problem is provided by a &lt;em&gt;multi-model&lt;/em&gt;, or model averaging, approach &lt;span class=&#34;citation&#34;&gt;(Burnham and Anderson 2002)&lt;/span&gt;. This requires calculating the &lt;a href=&#34;https://en.wikipedia.org/wiki/Akaike_information_criterion&#34;&gt;AIC (Akaike Information Criterion)&lt;/a&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; for each model and subjects, and then combine the estimates according to the Akaike weights of each model. To compute the Akaike weights one typically proceed by first transforming them into differences with respect to the AIC of the best candidate model (i.e. the one with lower AIC) &lt;span class=&#34;math display&#34;&gt;\[
{\Delta _m} = {\rm{AI}}{{\rm{C}}_m} - \min {\rm{AIC}}
\]&lt;/span&gt; From the differences in AIC, we can obtain an estimate of the relative likelihood of the model &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; given the data &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L} \left( {m|{\rm{data}}} \right) \propto \exp \left( { - \frac{1}{2}{\Delta _m}} \right)
\]&lt;/span&gt; Then, to obtain the Akaike weight &lt;span class=&#34;math inline&#34;&gt;\(w_m\)&lt;/span&gt; of the model &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, the relative likelihoods are normalized (divided by their sum) &lt;span class=&#34;math display&#34;&gt;\[
{w_m} = \frac{{\exp \left( { - \frac{1}{2}{\Delta _m}} \right)}}{{\mathop \sum \limits_{k = 1}^K \exp \left( { - \frac{1}{2}{\Delta _k}} \right)}}
\]&lt;/span&gt; Finally, one can compute the model-averaged estimate of the parameter&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat {\bar \sigma}\)&lt;/span&gt;, by combining the estimate of each model according to their Akaike weight &lt;span class=&#34;math display&#34;&gt;\[
\hat {\bar \sigma} = \sum\limits_{k = 1}^K {{w_k}\hat \sigma_k } 
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation results&lt;/h1&gt;
&lt;p&gt;Model averaging seems a sensitive approach to deal with the uncertainty about which form of the model works is more correct in our case. To see whether it is worth doing the extra work of fitting 3 models instead of just one, I run a simulation, where I repeatedly fit and compare the estimates of the three models, with the model averaged estimate, for different values of sample sizes. In all the simulations, each observer is generated by randomly drawing parameters from a Gaussian distribution which summarize the distribution of the parameters in the population. Hence, I know the &lt;em&gt;true&lt;/em&gt; difference in sensitivity in the population, and by simulating and fitting the models I can test which estimating procedure is more &lt;em&gt;efficient&lt;/em&gt;. In statistics a procedure or an estimator is said to be more efficient than another one when it provides a better estimate with the same number or fewer observations. The notion of “better” clearly relies on the choice of a cost function, which for example can be the mean squared error (it is here).&lt;/p&gt;
&lt;p&gt;Additionally, in my simulations each simulated observer could, &lt;em&gt;with equal probability&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{3}\)&lt;/span&gt;, either never lapse, lapse with a constant rate across conditions, or lapse at a higher rate in the more difficult condition (condition ‘2’ where the judgements are less precise). The lapse rates were draw uniformly from the interval [0.01, 0.1], and could get as high as 0.15 in condition ‘2’. Each simulated observer ran 250 trials per condition (similar to the figure at the top of this page). I simulated dataset from &lt;span class=&#34;math inline&#34;&gt;\(n=5\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(n=50\)&lt;/span&gt;, using 100 iterations for each sample size (only 85 in the case of &lt;span class=&#34;math inline&#34;&gt;\(n=50\)&lt;/span&gt; because the simulation was taking too long and I needed my laptop for other stuff). For simplicity I assumed that the different parameters were not correlated across observers&lt;a href=&#34;#fn5&#34; class=&#34;footnoteRef&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. I also had my simulated observer using the same criterion across the two conditions, although this may not necessarily be true. The quantity of interest here is the difference in slope between the two condition, that is &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2 - \sigma_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I examined the mean squared error of each of the models’ estimates, and of the model-averaged estimate. This is the average squared difference between the estimate and the true value. &lt;img src=&#34;https://mattelisi.github.io/img/mse.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt; The results shows (unless my color blindness fooled me) that the model-averaged estimate attains always the smaller error. Note also that the error tend to decrease exponentially with the sample size. Interestingly, the worst model seems to be the one that allow for the lapses to vary across conditions. This may be because the change in the lapse rate across condition was - when present - relatively small, but also because this model has a larger number of parameters, and thus produces more variable estimates (that is with higher standard errors) than smaller model. Indeed, given that I know the ‘true’ value of the parameres in this simulation settings, I can divide the error into the two subcomponents of variance and bias (see &lt;a href=&#34;http://scott.fortmann-roe.com/docs/BiasVariance.html&#34;&gt;this page&lt;/a&gt; for a nice introduction to the bias-variance tradeoff). The bias is the difference between the expected estimate (averaged over many repetitions/iterations) of the same model and the true quantity that we want to estimate. The variance is simply the variability of the model estimates, i.e. how much they oscillate around the expected estimate.&lt;/p&gt;
&lt;p&gt;Here is a plot of the variance. Indeed it can be seen that the variable-lapse model, which has more parameters, is the one that produces more variable estimates. There is however little difference between the other two models’ and the multi-model estimates &lt;img src=&#34;https://mattelisi.github.io/img/variance.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is the bias. This is very satisfactory, as it shows that while all individual models produced biased estimates, the bias of the model-averaged estimates is zero, or very close to zero. &lt;img src=&#34;https://mattelisi.github.io/img/bias.png&#34; alt=&#34;&#34; style=&#34;width:60.0%&#34; /&gt; In sum, by averaging models of different levels of complexity according to their relative likelihood, I was able to simultaneously decrease both the variance and the bias of my estimates, and achieve a greater efficiency. Model averaging seems to be the ideal procedure in this specific settings where the observer would belong to one of the three categories (i.e., she/he would conform to one of the three assumptions) with equal probability. However I think (although I haven’t checked) that it would perform at least as well as any of the other model even in cases where a single category of observers is largely predominant over the other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Burnham2002a&#34;&gt;
&lt;p&gt;Burnham, Kenneth P., and David R. Anderson. 2002. &lt;em&gt;Model Selection and Multimodel Inference&lt;/em&gt;. 2nd editio. New York, NY: Springer New York. doi:&lt;a href=&#34;https://doi.org/10.1007/b97636&#34;&gt;10.1007/b97636&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wichmann2001&#34;&gt;
&lt;p&gt;Wichmann, F a, and N J Hill. 2001. “The psychometric function: I. Fitting, sampling, and goodness of fit.” &lt;em&gt;Perception &amp;amp; Psychophysics&lt;/em&gt; 63 (8): 1293–1313. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/11800458&#34; class=&#34;uri&#34;&gt;http://www.ncbi.nlm.nih.gov/pubmed/11800458&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The psychometric function is a statistical model that predicts the probabilities of the observer response (e.g. “stimulus A has a larger/smaller instensity than stimulus B”), conditional to the stimulus and the experimental condition.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;A good experimenter should do that. A bad experimenter might just be interested in obtaining &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;.05\)&lt;/span&gt;. See &lt;a href=&#34;http://cerco.ups-tlse.fr/-Charte-statistique-?lang=fr&#34;&gt;this page&lt;/a&gt;, compiled by Jean-Michel Hupé, for some references and guidelines against &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-hacking and the misuse of statistical tools in neuroscience.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The AIC of a model is computed as &lt;span class=&#34;math inline&#34;&gt;\({\rm{AIC}} = 2k - 2\log \left( \mathcal{L} \right)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of free parameters, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}\)&lt;/span&gt; is the maximum value of the likelihood function of that model.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Here it is a parameter common to all models. See the book of Burnham &amp;amp; Andersen for methods to methods to deal with different situations &lt;span class=&#34;citation&#34;&gt;(Burnham and Anderson 2002)&lt;/span&gt;.&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Such correlation when present can be modelled using a mixed-effect approach. See my tutorial on mized-effects model in the&lt;a href=&#34;http://mattelisi.github.io/#notes&#34;&gt;‘misc’&lt;/a&gt; section of this website.&lt;a href=&#34;#fnref5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Listing&#39;s law, and the mathematics of the eyes</title>
      <link>https://mattelisi.github.io/post/listing-s-law-and-the-mathematics-of-the-eyes/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/post/listing-s-law-and-the-mathematics-of-the-eyes/</guid>
      <description>&lt;p&gt;&lt;em&gt;Brief intro to the mathematical formalism used to describe rotations of the eyes in 3D (including the torsional component).&lt;/em&gt; &lt;img src=&#34;https://mattelisi.github.io/img/3Deyecoord_Haslwanter1995.png&#34; alt=&#34;3D eye coordinate systems in the primary reference position, left panel, and after a leftward rotation, right panel (Haslwanter 1995). &#34; /&gt;&lt;/p&gt;
&lt;p&gt;The shape of the human eye is approximately a sphere with a diameter of 23 mm, and mechanically it behaves like a ball in a ball and socket joint. Because there is a functional distinguished axis - the visual axis, that is the line of gaze or more precisely the imaginary straight line passing through both the center of the pupil and the center of the fovea - the movements of the eyes are usually divided in &lt;em&gt;gaze direction&lt;/em&gt; and &lt;em&gt;cyclotorsion&lt;/em&gt; (or simply &lt;em&gt;torsion&lt;/em&gt;): while gaze direction refers to the direction of the visual axis, the torsion indicates the rotation of the eyeball about the visual axis. While modern video-based eyetrackers allow to record movements of the visual axis, they do not provide data about torsion. It turns out that there is a nice mathematical relationship that specifies the torsion of the eye in every direction of the gaze. This relationship is known as Listing’s law, and was named after the german mathematician &lt;a href=&#34;https://en.wikipedia.org/wiki/Johann_Benedict_Listing&#34;&gt;Johann Benedict Listing (1808-1882)&lt;/a&gt;. Listing’s law can be better understood by looking at how the 3D orientation of the eye can be formally described.&lt;/p&gt;
&lt;div id=&#34;mathematics-of-3d-eye-movements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mathematics of 3D eye movements&lt;/h1&gt;
&lt;p&gt;3D eye position can be specified by characterising the 3D rotation that brings the eye to the current eye position from an arbitrary reference or &lt;em&gt;primary&lt;/em&gt; position&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, which typically is defined as the position that the eye assumes when looking straight ahead with the head in normal, upright position. This rotation can be described by the 3-by-3 rotation matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt;. More specifically the matrix can be used to describe the rotation of three-dimensional coordinates by a certain angle about a certain axis. To formally define this matrix, consider the coordinate system &lt;span class=&#34;math inline&#34;&gt;\(\{ \vec{h}_1,\vec{h}_2,\vec{h}_3 \}\)&lt;/span&gt; (a coordinate system is defined by a set of linearly independent vectors; e.g. here &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1 = (1,0,0)\)&lt;/span&gt;, corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; axis) as the &lt;em&gt;head-centered&lt;/em&gt; coordinate system where the axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1\)&lt;/span&gt; correspond to the visual axis when the eye is in the reference position, and &lt;span class=&#34;math inline&#34;&gt;\(\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}\)&lt;/span&gt; is an &lt;em&gt;eye-centered&lt;/em&gt; coordinate system where &lt;span class=&#34;math inline&#34;&gt;\(\vec{e}_1\)&lt;/span&gt; always correspond to the visual axis, regardless of the orientation of the eye (see the figure on top of this page). Any orientation of the eye can be described by a matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt; such that &lt;span class=&#34;math display&#34;&gt;\[
{{\vec{e}}_i} = {\bf{R}} {{\vec{h}}_i}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(i=1,2,3\)&lt;/span&gt;. This rotation matrix is straightforward for 1D rotations. For example, a purely horizontal rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; around the axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_3\)&lt;/span&gt; is formulated as &lt;span class=&#34;math display&#34;&gt;\[
\bf{R}_3 \left( \theta  \right) = \left( {\begin{array}{*{20}{c}}
{\cos \theta }&amp;amp;{ - \sin \theta }&amp;amp;0\\
{\sin \theta }&amp;amp;{\cos \theta }&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{array}} \right)
\]&lt;/span&gt; The first two columns of the matrix indicates the new coordinates of the first (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_1\)&lt;/span&gt;) and of the second basis (&lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_2\)&lt;/span&gt;) of the new eye-centerd coordinate system after the rotation, expressed according to the initial head-centered coordinate system. The third basis, &lt;span class=&#34;math inline&#34;&gt;\(\vec{h}_3\)&lt;/span&gt; is the axis of rotation, and does not change. It becomes more complicated for 3D rotations, i.e. rotations of the fixed eye-centered coordinate system to any new orientation. They can be obtained by calculating a sequence of 3 different rotations about the three fixed axis, and multiplying the corresponding matrices: &lt;span class=&#34;math inline&#34;&gt;\(\bf{R} = \bf{R}_3 \left( \theta \right) \bf{R}_2 \left( \phi \right) \bf{R}_1 \left( \psi \right)\)&lt;/span&gt;. Although the first two rotations are sufficient to specity the orientation of the visual axis, the third is necessary to specify the torsion component and fully specify the 3D orientation of the eye. Importantly, the order of the three rotations is relevant - rotations are not commutative, so if you put them in different order you end up with a different result - and needs to be arbitrarily specified (when it is specified in this order is referred to as &lt;em&gt;Flick sequence&lt;/em&gt;). This representation of 3D orientations is not very efficient (9 values, while only 3 are necessary), or practical for computations; additionally one needs to define arbitrarily the order of the rotations of the sequence.&lt;/p&gt;
&lt;div id=&#34;quaternions-and-rotation-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quaternions and rotation vectors&lt;/h2&gt;
&lt;p&gt;An alternative way to describe rotations is with &lt;em&gt;quaternions&lt;/em&gt;. Quaternions can be looked upon as four-dimensional vectors, although they are more commonly split in a real scalar part and an imaginary vector part; they are in fact an extension of the complex numbers. They have the form &lt;span class=&#34;math display&#34;&gt;\[
q_0 + q_1i + q_2j + q_3k = \left( q_0,\vec{q} \cdot \vec{I} \right) = \left( r,\vec{v} \right)
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\vec{q} = \left( \begin{array}{*{20}{c}}
{q_1}\\
{q_2}\\
{q_3}
\end{array} \right)
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\vec{I} = \left( \begin{array}{*{20}{c}}
{i}\\
{j}\\
{k}
\end{array} \right)
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(i,j,k\)&lt;/span&gt; are the quaternion units. These can be multiplied according to the following formula, discovered by &lt;a href=&#34;https://en.wikipedia.org/wiki/William_Rowan_Hamilton&#34;&gt;Hamilton&lt;/a&gt; in 1843 &lt;span class=&#34;math display&#34;&gt;\[
i^2 = j^2 = k^2 = ijk =  - 1
\]&lt;/span&gt; This formula may seems strange but it determines all the possible products of &lt;span class=&#34;math inline&#34;&gt;\(i,j,k\)&lt;/span&gt;, such as &lt;span class=&#34;math inline&#34;&gt;\(ij=k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(ji=-k\)&lt;/span&gt;. Note that the product of the basis are not commutative. There is a visual trick to remember the multiplication rules, based on the following diagram:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://mattelisi.github.io/img/quatrule.png&#34; alt=&#34;Multiplying quaternions. Multiplying two elements in the clockwise direction gives the next element along the same direction (e.g. jk=i). The same is for counter-clockwise directions, except that the result is negative (e.g. kj=-i). &#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Multiplying quaternions. Multiplying two elements in the clockwise direction gives the next element along the same direction (e.g. &lt;span class=&#34;math inline&#34;&gt;\(jk=i\)&lt;/span&gt;). The same is for counter-clockwise directions, except that the result is negative (e.g. &lt;span class=&#34;math inline&#34;&gt;\(kj=-i\)&lt;/span&gt;). &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Quaternions can be used to represent rotations. For example, a rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; around the axis define by the unit vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{u} = (u_1, u_2,u_3) = u_1i + u_2j + u_3k\)&lt;/span&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; can be described by the following quaternion &lt;span class=&#34;math display&#34;&gt;\[
\cos \frac{\theta}{2} + \sin \frac{\theta}{2}\left( u_1i + u_2j + u_3k \right)
\]&lt;/span&gt; The direction of the rotation is given by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Right-hand_rule#A_rotating_body&#34;&gt;right-hand rule&lt;/a&gt;. Successive rotations can combined using the formula for quaternion multiplication. The multiplication of quaternions can be computed by the products of their elements element as if they were two polynomials, but keeping track of the ordering of the basis, as their multiplication is not commutative. This is a desired property if we want to specify rotations, which as seen earlier are also not commutative. Quaternion multiplication can be also expressed in the modern language of vector and cross product &lt;span class=&#34;math display&#34;&gt;\[
\left( r_1,\vec{v_1} \right) \left( r_2,\vec{v_2} \right) = 
\left( r_1 r_2 - \vec{v_1} \cdot \vec{v_2},\;\; r_1\vec{v_2} + r_2\vec{v_1} +\vec{v_1} \times \vec{v_2} \right)
\]&lt;/span&gt; where “&lt;span class=&#34;math inline&#34;&gt;\(\cdot\)&lt;/span&gt;” is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Dot_product&#34;&gt;dot product&lt;/a&gt; and “&lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt;” is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross_product&#34;&gt;cross product&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In sum, quaternions are pretty useful to compute transformations in 3D. One can use quaternions to combine 3D any sequence of rotations about arbitrary axis (using quaternion multiplications), as well as to rotate any 3D Euclidean vector about any arbitrary axis. A quaternion can also be transformed into a 3D rotation matrix (formula &lt;a href=&#34;https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Quaternion-derived_rotation_matrix&#34;&gt;here&lt;/a&gt;), which then may be used in 3D graphics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rotation vectors&lt;/strong&gt; are an even more succint representation of rotations. Indeed, the scalar component of the quaterion (&lt;span class=&#34;math inline&#34;&gt;\(q_0\)&lt;/span&gt;) does not add any information that is not alredy in the vector part, so a rotation could be effectively described by just 3 numbers. The rotation vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{r}\)&lt;/span&gt;, which correspond to a rotation of an angle &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; about an axis &lt;span class=&#34;math inline&#34;&gt;\(\vec{n}\)&lt;/span&gt; is defined as &lt;span class=&#34;math display&#34;&gt;\[
\vec{r} = \tan \left( \frac{\theta}{2} \right) \vec{n}
\]&lt;/span&gt; which can be defined also with respect to the equivalent quaternion &lt;span class=&#34;math inline&#34;&gt;\(\textbf{q}\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[
\textbf{q}=\left( q_0, \vec{q} \right) = \left( \cos \left(\frac{\theta}{2}\right), \sin \left(\frac{\theta}{2}\right)\vec{n} \right)
\]&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[ \vec{r} = \frac{\vec{q}}{q_0} \]&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;donders-law-and-listings-law&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Donder’s law and Listing’s law&lt;/h1&gt;
&lt;p&gt;Donder’s law (1848) states that the eye use only two degrees of freedom while fixating, although mechanically it has three. In othere words this means that the torsion component of the eye movement is not arbitrary but it is uniquely determined by the direction of the visual axis and is independent of the previous eye movements. From the material review above it should be clear how any 3D eye orientation can be fully described as a rotation abour a given axis from a primary reference position. This allows also to formulate Donder’s law more specifically,according to what is known as Listing’s law &lt;span class=&#34;citation&#34;&gt;(Helmholtz 1910,&lt;span class=&#34;citation&#34;&gt;Haustein (1989)&lt;/span&gt;)&lt;/span&gt; “&lt;em&gt;There exists a certain eye position from which the eye may reach any other position of fixation by a rotation around an axis perpendicular to the visual axis. This particular position is called primary position&lt;/em&gt;”. This means that &lt;em&gt;all possible eye positions&lt;/em&gt; can be reached from the primary position by a single rotation about an axis perpendicular to the visual axis. Since they are all perpendicular to the visual axis, all rotation axis that satisfy Listing’s law are on the same plane (&lt;em&gt;Listing’s plane&lt;/em&gt;). The law can be tested with eyetracking equipments that allows measuring also the torsional components (such as scleral coils): results have shown that the standard deviation from Listing’s plane of empirically measured rotation vectors is only about 0.5-1 deg &lt;span class=&#34;citation&#34;&gt;(Haslwanter 1995)&lt;/span&gt;. Formally it can be written that for any orientation of the visual axis, defined by the rotation vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{a}\)&lt;/span&gt; and measured from the primary position &lt;span class=&#34;math inline&#34;&gt;\(\vec{h_1}=(1,0,0)\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\vec{h_1} \cdot \vec{a} = 0
\]&lt;/span&gt; This indicates simply that the rotation about the visual axis is 0, and that as a consequence all the rotation axes lies in a frontal plane.&lt;/p&gt;
&lt;p&gt;Going back to the beginning, knowing the coordinates os Listing’s plane one can compute the rotation vector that correspond to the current eye position from the recording of the 2D gaze location on a screen. In the simplest case, we assume that the primary position corresponds to when the observer fixates the center of the screen, &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt;. What is the rotation vector that describes the 3D eye orientation when the observer fixates the location &lt;span class=&#34;math inline&#34;&gt;\((s_x, s_y)\)&lt;/span&gt; ? Let’s say the position on screen is defined in cm, and we know that the distance of the eye from the screen is &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; cm. The rotation angle can be computed as &lt;span class=&#34;math inline&#34;&gt;\(\theta = \rm{atan} \frac{\sqrt{s_x^2+s_y^2}}{L}\)&lt;/span&gt;, while the angle that defines the orientation of the rotation axis within Listing’s plane is &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \rm{atan2}(s_y,s_x)\)&lt;/span&gt;. The complete rotation vector is then &lt;span class=&#34;math display&#34;&gt;\[
\vec{r} = \tan \left( \frac{\theta}{2}\right) \cdot \left( {\begin{array}{*{20}{c}}
0\\
{\cos \alpha }\\
{ - \sin \alpha }
\end{array}} \right)
\]&lt;/span&gt; This vector describe aparticular eye position as a rotation from the reference position, and does not have a torsional component (that is a component along &lt;span class=&#34;math inline&#34;&gt;\(\vec{h_1}\)&lt;/span&gt;). Indeed, Listing’s law implies that all possible eye positions can be reached from the primary reference position without a torsional component. However, vectors describing rotations from and to positions different than the primary one do &lt;em&gt;not&lt;/em&gt;, in general, lie in Listing’s plane. For Listing law to hold such vectors must lie in a plane whose orientation depends on the current eye position, and more specifically is such that the vector perpendicular to the plane is exactly halfway between the current and the primary eye position &lt;span class=&#34;citation&#34;&gt;(Tweed and Vilis 1990)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Haslwanter1995&#34;&gt;
&lt;p&gt;Haslwanter, Thomas. 1995. “Mathematics of three-dimensional eye rotations.” &lt;em&gt;Vision Research&lt;/em&gt; 35 (12): 1727–39. doi:&lt;a href=&#34;https://doi.org/10.1016/0042-6989(94)00257-M&#34;&gt;10.1016/0042-6989(94)00257-M&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Haustein1989&#34;&gt;
&lt;p&gt;Haustein, Werner. 1989. “Considerations on Listing’s Law and the primary position by means of a matrix description of eye position control.” &lt;em&gt;Biological Cybernetics&lt;/em&gt; 60 (6): 411–20. doi:&lt;a href=&#34;https://doi.org/10.1007/BF00204696&#34;&gt;10.1007/BF00204696&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Helmholtz1910&#34;&gt;
&lt;p&gt;Helmholtz, Hermann von. 1910. &lt;em&gt;Handbuch der Physiologischen Optik&lt;/em&gt;. Hamburg: Voss.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Tweed1990&#34;&gt;
&lt;p&gt;Tweed, Douglas, and Tutis Vilis. 1990. “Geometric relations of eye position and velocity vectors during saccades.” &lt;em&gt;Vision Research&lt;/em&gt; 30 (1): 111–27. doi:&lt;a href=&#34;https://doi.org/10.1016/0042-6989(90)90131-4&#34;&gt;10.1016/0042-6989(90)90131-4&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Euler%27s_rotation_theorem&#34;&gt;Euler’s theorem&lt;/a&gt; guarantee that a rigid body can always move from one orientation to any different one through a single rotation about a fixed axis.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Saying that &lt;span class=&#34;math inline&#34;&gt;\(\vec(u)\)&lt;/span&gt; is a unit vector indicates that it has length 1, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\left| \vec{u} \right| = \sqrt{u_1^2 + u_2^2 + u_3^2} = 1\)&lt;/span&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Memory-guided saccades show effect of perceptual illusion whereas visually-guided saccades do not</title>
      <link>https://mattelisi.github.io/publication/memory-delphine/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/memory-delphine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Different spatial representations guide eye and hand movements</title>
      <link>https://mattelisi.github.io/publication/double-drift-pointing/</link>
      <pubDate>Sun, 24 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/double-drift-pointing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Discrete confidence levels revealed by sequential decisions</title>
      <link>https://mattelisi.github.io/publication/discrete-confidence/</link>
      <pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/discrete-confidence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Target Displacements during Eye Blinks Trigger Automatic Recalibration of Gaze Direction</title>
      <link>https://mattelisi.github.io/publication/blink-adapt/</link>
      <pubDate>Thu, 02 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/blink-adapt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Time Costancy in human perception</title>
      <link>https://mattelisi.github.io/publication/time-constancy/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/time-constancy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary eye movements direct attention on the mental number space</title>
      <link>https://mattelisi.github.io/publication/voluntary-eye/</link>
      <pubDate>Tue, 02 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/voluntary-eye/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dissociation between the Perceptual and Saccadic Localization of Moving Objects</title>
      <link>https://mattelisi.github.io/publication/double-drift-1/</link>
      <pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/double-drift-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of Multimodal Load on Spatial Monitoring as Revealed by ERPs</title>
      <link>https://mattelisi.github.io/publication/load-erp/</link>
      <pubDate>Thu, 03 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/load-erp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pupil dilation reveals top-down attentional load during spatial monitoring</title>
      <link>https://mattelisi.github.io/publication/load-pupil/</link>
      <pubDate>Thu, 03 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/load-pupil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spatial constancy of attention across eye movements is mediated by the presence of visual objects</title>
      <link>https://mattelisi.github.io/publication/spatial-constancy-attention/</link>
      <pubDate>Fri, 01 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/spatial-constancy-attention/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Larger, smaller, odd or even? Task-specific effects of optokinetic stimulation on the mental number space</title>
      <link>https://mattelisi.github.io/publication/optokinetic/</link>
      <pubDate>Mon, 04 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/optokinetic/</guid>
      <description>&lt;p&gt;Mariagrazia Ranzini and Matteo Lisi contributed equally to the work&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The prediction of visual stimuli influences auditory loudness discrimination</title>
      <link>https://mattelisi.github.io/publication/desantis/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/desantis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Space coding for sensorimotor transformations can emerge through unsupervised learning</title>
      <link>https://mattelisi.github.io/publication/unsupervised-space-coding/</link>
      <pubDate>Thu, 30 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mattelisi.github.io/publication/unsupervised-space-coding/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
